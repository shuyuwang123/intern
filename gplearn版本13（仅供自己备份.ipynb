{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-0FwpEirOd4"
      },
      "source": [
        "# 限价订单簿（LOB）特征字典（共 110 维）\n",
        "\n",
        "## v₁ 基础集合 (20 维)\n",
        "\n",
        "| 特征名 | 数学符号 | 含义 |\n",
        "|--------|----------|------|\n",
        "| AskPrice_L1 … L5 | \\(P^{ask}_i\\) | 卖一 ~ 卖五 **报价** |\n",
        "| AskVol_L1 … L5  | \\(V^{ask}_i\\) | 卖一 ~ 卖五 **挂单量** |\n",
        "| BidPrice_L1 … L5 | \\(P^{bid}_i\\) | 买一 ~ 买五 报价 |\n",
        "| BidVol_L1 … L5  | \\(V^{bid}_i\\) | 买一 ~ 买五 挂单量 |\n",
        "\n",
        "---\n",
        "\n",
        "## v₂ 时间无关集合 Ⅰ (10 维)\n",
        "\n",
        "| 特征名 | 公式 | 说明 |\n",
        "|--------|------|------|\n",
        "| Spread_L1 … L5 | \\(P^{ask}_i - P^{bid}_i\\) | 各档 **买卖价差** |\n",
        "| MidPrice_L1 … L5 | \\(\\tfrac{P^{ask}_i + P^{bid}_i}{2}\\) | 各档 **中间价** |\n",
        "\n",
        "---\n",
        "\n",
        "## v₃ 时间无关集合 Ⅱ (12 维)\n",
        "\n",
        "| 特征名 | 公式 | 说明 |\n",
        "|--------|------|------|\n",
        "| AskStackRange | \\(P^{ask}_{L5} - P^{ask}_{L1}\\) | 卖盘价格跨度 |\n",
        "| BidStackRange | \\(P^{bid}_{L1} - P^{bid}_{L5}\\) | 买盘价格跨度 |\n",
        "| AskGrad_L1_L2 … L4_L5 | \\(|P^{ask}_i - P^{ask}_{i+1}|\\) | 相邻 **卖档梯度** |\n",
        "| BidGrad_L1_L2 … L4_L5 | \\(|P^{bid}_i - P^{bid}_{i+1}|\\) | 相邻 **买档梯度** |\n",
        "\n",
        "---\n",
        "\n",
        "## v₄ 均值价格与数量 (4 维)\n",
        "\n",
        "| 特征名 | 公式 | 说明 |\n",
        "|--------|------|------|\n",
        "| MeanAskPrice | \\(\\frac{1}{5}\\sum_i P^{ask}_i\\) | 卖五档 **均价** |\n",
        "| MeanBidPrice | \\(\\frac{1}{5}\\sum_i P^{bid}_i\\) | 买五档 均价 |\n",
        "| MeanAskVol   | \\(\\frac{1}{5}\\sum_i V^{ask}_i\\) | 卖五档 **均量** |\n",
        "| MeanBidVol   | \\(\\frac{1}{5}\\sum_i V^{bid}_i\\) | 买五档 均量 |\n",
        "\n",
        "---\n",
        "\n",
        "## v₅ 累计不平衡 (2 维)\n",
        "\n",
        "| 特征名 | 公式 | 说明 |\n",
        "|--------|------|------|\n",
        "| CumPriceImb | \\(\\sum_i (P^{ask}_i - P^{bid}_i)\\) | 总体 **价格不平衡** |\n",
        "| CumVolImb   | \\(\\sum_i (V^{ask}_i - V^{bid}_i)\\) | 总体 **数量不平衡** |\n",
        "\n",
        "---\n",
        "\n",
        "## v₆ 一阶导数（首尾差） (20 维)\n",
        "\n",
        "> 以窗口首尾快照计算\n",
        "\n",
        "| 特征名 | 说明 |\n",
        "|--------|------|\n",
        "| dAskPrice_L1 … L5 | 卖档 **价格斜率** |\n",
        "| dBidPrice_L1 … L5 | 买档 价格斜率 |\n",
        "| dAskVol_L1 … L5   | 卖档 **挂量斜率** |\n",
        "| dBidVol_L1 … L5   | 买档 挂量斜率 |\n",
        "\n",
        "---\n",
        "\n",
        "## v₇ 平均 \\|一阶导\\| (20 维)\n",
        "\n",
        "> 对窗口内每时刻的一阶导取绝对值，再对时间求均值。\n",
        "\n",
        "| 特征名 | 说明 |\n",
        "|--------|------|\n",
        "| Mean_dAskPrice_L1 … L5 | 卖档价格平均变动速率 |\n",
        "| Mean_dBidPrice_L1 … L5 | 买档价格平均变动速率 |\n",
        "| Mean_dAskVol_L1 … L5   | 卖档挂量平均变动速率 |\n",
        "| Mean_dBidVol_L1 … L5   | 买档挂量平均变动速率 |\n",
        "\n",
        "---\n",
        "\n",
        "## v₈ 相对强度指示符 (4 维, 0/1)\n",
        "\n",
        "| 特征名 | 判别条件 | 意义 |\n",
        "|--------|----------|------|\n",
        "| RI_Price  | mean(dP<sub>ask</sub>) > mean(dP<sub>bid</sub>) | 价格趋势偏向卖方？ |\n",
        "| RI_Vol    | MeanAskVol > MeanBidVol | 挂量偏向卖方？ |\n",
        "| RI_dPrice | avg\\|dP<sub>ask</sub>\\| > avg\\|dP<sub>bid</sub>\\| | 价格波动更偏向卖方 |\n",
        "| RI_dVol   | avg\\|dV<sub>ask</sub>\\| > avg\\|dV<sub>bid</sub>\\| | 挂量波动更偏向卖方 |\n",
        "\n",
        "---\n",
        "\n",
        "## v₉ 平均二阶导数 (20 维)\n",
        "\n",
        "> 二阶导：\n",
        "\n",
        "| 特征名 | 说明 |\n",
        "|--------|------|\n",
        "| Mean_ddAskPrice_L1 … L5 | 卖档价格 **加速度** |\n",
        "| Mean_ddBidPrice_L1 … L5 | 买档价格 加速度 |\n",
        "| Mean_ddAskVol_L1 … L5   | 卖档挂量 加速度 |\n",
        "| Mean_ddBidVol_L1 … L5   | 买档挂量 加速度 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d-zEZaArZC4",
        "outputId": "a54e1863-b377-4760-8193-353e5bc0c161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gplearn==0.4.2\n",
            "  Downloading gplearn-0.4.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from gplearn==0.4.2) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gplearn==0.4.2) (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->gplearn==0.4.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->gplearn==0.4.2) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->gplearn==0.4.2) (3.6.0)\n",
            "Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: gplearn\n",
            "Successfully installed gplearn-0.4.2\n"
          ]
        }
      ],
      "source": [
        "pip install gplearn==0.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjN7H6fmscc1"
      },
      "source": [
        "\n",
        "参数：\n",
        "   - `SEED`：随机种子  \n",
        "   - `STRIDE`：滑动步长  \n",
        "   - `TRAIN_FILE` / `VAL_FILE`：训练集与验证集路径  \n",
        "   - `MODEL_DIR`：模型保存目录\n",
        "\n",
        "我们定义的函数：\n",
        "\n",
        "   - `ensure_ts_datetime`：确保时间列为日期时间格式\n",
        "   - `purge`：去除空值与非有限值\n",
        "   - `build_matrix`：构建价格-数量矩阵\n",
        "   - `compute_v_features`：从窗口计算 v₁…v₉ 特征\n",
        "   - `pretty_expr`：将程序树转化为可读表达式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QGzYgc2q5vR"
      },
      "outputs": [],
      "source": [
        "# # =============================================================\n",
        "# 有一些版本兼容问题\n",
        "# =============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import sklearn\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "def _no_tags(self): return {}\n",
        "BaseEstimator.__sklearn_tags__ = _no_tags\n",
        "BaseEstimator._get_tags        = _no_tags\n",
        "\n",
        "\n",
        "import gplearn.genetic as _gp\n",
        "for _cls_name in (\"SymbolicRegressor\",\"SymbolicClassifier\",\"SymbolicTransformer\"):\n",
        "    cls = getattr(_gp, _cls_name)\n",
        "    cls.__sklearn_tags__ = _no_tags\n",
        "    cls._get_tags        = _no_tags\n",
        "\n",
        "# ---------------- imports & I/O ----------------\n",
        "import os, random, warnings, re, numpy as np, polars as pl, cloudpickle\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.utils import check_random_state\n",
        "from gplearn.genetic import SymbolicTransformer as _ST_orig\n",
        "\n",
        "\n",
        "SEED         = 123          # 固定随机种子\n",
        "STRIDE       = 50           # stride设置成50，不然内存会崩\n",
        "TRAIN_FILE   = \"/content/drive/My Drive/train_unstandlized.parquet\"\n",
        "VAL_FILE     = \"/content/drive/My Drive/val_unstandlized.parquet\"\n",
        "MODEL_DIR    = \"/content/drive/My Drive/gp_models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "################################################################\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "# ---------- 兼容 gplearn 与 sklearn 1.3+ ----------------------\n",
        "class SymbolicTransformer(_ST_orig):\n",
        "    def __sklearn_tags__(self): return {}\n",
        "    def _get_tags(self):        return {}\n",
        "\n",
        "# ---------------- 固定超参/列名/工具 ----------------\n",
        "STEP_MS, WINDOW_SIZE, HORIZON_STP = 500, 20, 20\n",
        "GENERATIONS, POP_SIZE, TOURNAMENT_SZ, MAX_DEPTH = 1, 2500, 40, 5\n",
        "N_LEVELS = 5\n",
        "PRICE_COLS = [f\"Buy{i}Price\"  for i in range(1, N_LEVELS+1)] + \\\n",
        "             [f\"Sell{i}Price\" for i in range(1, N_LEVELS+1)]\n",
        "QTY_COLS   = [f\"Buy{i}OrderQty\"  for i in range(1, N_LEVELS+1)] + \\\n",
        "             [f\"Sell{i}OrderQty\" for i in range(1, N_LEVELS+1)]\n",
        "RAW_COLS   = PRICE_COLS + QTY_COLS\n",
        "\n",
        "FEATURE_NAMES = [\n",
        "    # v1–v6（66）\n",
        "    \"AskPrice_L1\",\"AskPrice_L2\",\"AskPrice_L3\",\"AskPrice_L4\",\"AskPrice_L5\",\n",
        "    \"AskVol_L1\",\"AskVol_L2\",\"AskVol_L3\",\"AskVol_L4\",\"AskVol_L5\",\n",
        "    \"BidPrice_L1\",\"BidPrice_L2\",\"BidPrice_L3\",\"BidPrice_L4\",\"BidPrice_L5\",\n",
        "    \"BidVol_L1\",\"BidVol_L2\",\"BidVol_L3\",\"BidVol_L4\",\"BidVol_L5\",\n",
        "    \"Spread_L1\",\"Spread_L2\",\"Spread_L3\",\"Spread_L4\",\"Spread_L5\",\n",
        "    \"MidPrice_L1\",\"MidPrice_L2\",\"MidPrice_L3\",\"MidPrice_L4\",\"MidPrice_L5\",\n",
        "    \"AskStackRange\",\"BidStackRange\",\n",
        "    \"AskGrad_L1_L2\",\"AskGrad_L2_L3\",\"AskGrad_L3_L4\",\"AskGrad_L4_L5\",\n",
        "    \"BidGrad_L1_L2\",\"BidGrad_L2_L3\",\"BidGrad_L3_L4\",\"BidGrad_L4_L5\",\n",
        "    \"MeanAskPrice\",\"MeanBidPrice\",\"MeanAskVol\",\"MeanBidVol\",\n",
        "    \"CumPriceImb\",\"CumVolImb\",\n",
        "    \"dAskPrice_L1\",\"dAskPrice_L2\",\"dAskPrice_L3\",\"dAskPrice_L4\",\"dAskPrice_L5\",\n",
        "    \"dBidPrice_L1\",\"dBidPrice_L2\",\"dBidPrice_L3\",\"dBidPrice_L4\",\"dBidPrice_L5\",\n",
        "    \"dAskVol_L1\",\"dAskVol_L2\",\"dAskVol_L3\",\"dAskVol_L4\",\"dAskVol_L5\",\n",
        "    \"dBidVol_L1\",\"dBidVol_L2\",\"dBidVol_L3\",\"dBidVol_L4\",\"dBidVol_L5\",\n",
        "    # v7（20）\n",
        "    \"Mean_dAskPrice_L1\",\"Mean_dAskPrice_L2\",\"Mean_dAskPrice_L3\",\"Mean_dAskPrice_L4\",\"Mean_dAskPrice_L5\",\n",
        "    \"Mean_dBidPrice_L1\",\"Mean_dBidPrice_L2\",\"Mean_dBidPrice_L3\",\"Mean_dBidPrice_L4\",\"Mean_dBidPrice_L5\",\n",
        "    \"Mean_dAskVol_L1\",\"Mean_dAskVol_L2\",\"Mean_dAskVol_L3\",\"Mean_dAskVol_L4\",\"Mean_dAskVol_L5\",\n",
        "    \"Mean_dBidVol_L1\",\"Mean_dBidVol_L2\",\"Mean_dBidVol_L3\",\"Mean_dBidVol_L4\",\"Mean_dBidVol_L5\",\n",
        "    # v8（4）\n",
        "    \"RI_Price\", \"RI_Vol\", \"RI_dPrice\", \"RI_dVol\",\n",
        "    # v9（20）\n",
        "    \"Mean_ddAskPrice_L1\",\"Mean_ddAskPrice_L2\",\"Mean_ddAskPrice_L3\",\"Mean_ddAskPrice_L4\",\"Mean_ddAskPrice_L5\",\n",
        "    \"Mean_ddBidPrice_L1\",\"Mean_ddBidPrice_L2\",\"Mean_ddBidPrice_L3\",\"Mean_ddBidPrice_L4\",\"Mean_ddBidPrice_L5\",\n",
        "    \"Mean_ddAskVol_L1\",\"Mean_ddAskVol_L2\",\"Mean_ddAskVol_L3\",\"Mean_ddAskVol_L4\",\"Mean_ddAskVol_L5\",\n",
        "    \"Mean_ddBidVol_L1\",\"Mean_ddBidVol_L2\",\"Mean_ddBidVol_L3\",\"Mean_ddBidVol_L4\",\"Mean_ddBidVol_L5\",\n",
        "]\n",
        "\n",
        "def ensure_ts_datetime(df: pl.DataFrame, col=\"ts\") -> pl.DataFrame:\n",
        "    if col not in df.columns:\n",
        "        raise KeyError(f\"列 '{col}' 不存在\")\n",
        "    if df[col].dtype in (pl.Datetime, pl.Date):\n",
        "        return df\n",
        "    # 字符串解析\n",
        "    try:\n",
        "        return df.with_columns(pl.col(col).str.strptime(pl.Datetime, strict=False, exact=False))\n",
        "    except Exception:\n",
        "        pass\n",
        "    # epoch 毫秒\n",
        "    try:\n",
        "        return df.with_columns(pl.from_epoch(pl.col(col).cast(pl.Int64), unit=\"ms\").alias(col))\n",
        "    except Exception:\n",
        "        pass\n",
        "    # epoch 秒\n",
        "    try:\n",
        "        return df.with_columns(pl.from_epoch(pl.col(col).cast(pl.Int64), unit=\"s\").alias(col))\n",
        "    except Exception:\n",
        "        pass\n",
        "    warnings.warn(f\"列 '{col}' 不是时间类型，后续 dt 操作可能失败。\")\n",
        "    return df\n",
        "\n",
        "def purge(lf: pl.LazyFrame) -> pl.DataFrame:\n",
        "    conds = [pl.col(c).is_finite() for c in RAW_COLS]\n",
        "    return lf.drop_nulls(RAW_COLS).filter(pl.all_horizontal(conds)).collect(streaming=True)\n",
        "\n",
        "\n",
        "# Construct an order book feature matrix from prices and quantities.\n",
        "def build_matrix(df: pl.DataFrame) -> np.ndarray:\n",
        "    sp = df.select([c for c in PRICE_COLS if \"Sell\" in c]).to_numpy()\n",
        "    bp = df.select([c for c in PRICE_COLS if \"Buy\"  in c]).to_numpy()\n",
        "    sq = df.select([c for c in QTY_COLS   if \"Sell\" in c]).to_numpy()\n",
        "    bq = df.select([c for c in QTY_COLS   if \"Buy\"  in c]).to_numpy()\n",
        "    return np.concatenate([sp, bp, sq, bq], axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "# 手搓因子\n",
        "def compute_v_features(window: np.ndarray, step_ms:int=STEP_MS) -> np.ndarray:\n",
        "    n = N_LEVELS\n",
        "    cur, prev = window[-1], window[0]\n",
        "    P_ask, P_bid = cur[0:n],        cur[n:2*n]\n",
        "    V_ask, V_bid = cur[2*n:3*n],    cur[3*n:4*n]\n",
        "    P_ask0, P_bid0 = prev[0:n],     prev[n:2*n]\n",
        "    V_ask0, V_bid0 = prev[2*n:3*n], prev[3*n:4*n]\n",
        "\n",
        "    # v1–v6\n",
        "    v1 = np.concatenate([P_ask, V_ask, P_bid, V_bid])\n",
        "    spread, mid = P_ask - P_bid, 0.5*(P_ask + P_bid)\n",
        "    v2 = np.concatenate([spread, mid])\n",
        "    range_ask, range_bid = P_ask[-1]-P_ask[0], P_bid[0]-P_bid[-1]\n",
        "    adj_diff = np.concatenate([np.abs(np.diff(P_ask)), np.abs(np.diff(P_bid))])\n",
        "    v3 = np.concatenate([[range_ask, range_bid], adj_diff])\n",
        "    v4 = np.array([P_ask.mean(), P_bid.mean(), V_ask.mean(), V_bid.mean()])\n",
        "    v5 = np.array([(P_ask-P_bid).sum(), (V_ask-V_bid).sum()])\n",
        "    delta_t = (window.shape[0]-1) * step_ms / 1000.0\n",
        "    dP_ask, dP_bid = (P_ask-P_ask0)/delta_t, (P_bid-P_bid0)/delta_t\n",
        "    dV_ask, dV_bid = (V_ask-V_ask0)/delta_t, (V_bid-V_bid0)/delta_t\n",
        "    v6 = np.concatenate([dP_ask, dP_bid, dV_ask, dV_bid])\n",
        "\n",
        "    # v7–v9（用完整窗口）\n",
        "    dt = step_ms / 1000.0\n",
        "    Pask_seq = window[:, 0:n];      Pbid_seq = window[:, n:2*n]\n",
        "    Vask_seq = window[:, 2*n:3*n];  Vbid_seq = window[:, 3*n:4*n]\n",
        "    dPask = np.diff(Pask_seq, axis=0) / dt\n",
        "    dPbid = np.diff(Pbid_seq, axis=0) / dt\n",
        "    dVask = np.diff(Vask_seq, axis=0) / dt\n",
        "    dVbid = np.diff(Vbid_seq, axis=0) / dt\n",
        "    v7 = np.concatenate([\n",
        "        np.mean(np.abs(dPask), axis=0), np.mean(np.abs(dPbid), axis=0),\n",
        "        np.mean(np.abs(dVask), axis=0), np.mean(np.abs(dVbid), axis=0),\n",
        "    ])\n",
        "    v8 = np.array([\n",
        "        float(dPask.mean() > dPbid.mean()),\n",
        "        float(V_ask.mean()  > V_bid.mean()),\n",
        "        float(np.abs(dPask).mean() > np.abs(dPbid).mean()),\n",
        "        float(np.abs(dVask).mean() > np.abs(dVbid).mean()),\n",
        "    ], dtype=np.float32)\n",
        "    ddPask = np.diff(dPask, axis=0) / dt\n",
        "    ddPbid = np.diff(dPbid, axis=0) / dt\n",
        "    ddVask = np.diff(dVask, axis=0) / dt\n",
        "    ddVbid = np.diff(dVbid, axis=0) / dt\n",
        "    v9 = np.concatenate([\n",
        "        np.mean(ddPask, axis=0), np.mean(ddPbid, axis=0),\n",
        "        np.mean(ddVask, axis=0), np.mean(ddVbid, axis=0),\n",
        "    ])\n",
        "    return np.concatenate([v1, v2, v3, v4, v5, v6, v7, v8, v9])\n",
        "\n",
        "def pretty_expr(program, names):\n",
        "    if hasattr(program, \"export\"):\n",
        "        return program.export(feature_names=names)\n",
        "    expr = str(program)\n",
        "    def repl(m): return names[int(m.group(1))]\n",
        "    return re.sub(r'\\bX(\\d+)\\b', repl, expr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgweEUt49adG"
      },
      "source": [
        "\n",
        "## daily_ic_stats\n",
        "- 按日期计算逐日 Spearman IC  \n",
        "- 支持 1D   输入\n",
        "- 返回：日均IC、日均|IC|、有效天数、每日IC映射\n",
        "\n",
        "## select_best_component_daily\n",
        "- 在多个组件中选择 **|日均IC| 最大** 的作为最优  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NBMNFqCoh4U"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===================== 逐日 IC 计算 =====================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "#  逐日 Spearman IC 统计:\n",
        "#       - 支持 1D\n",
        "#       - 自动清理 NaN/Inf\n",
        "#       - 返回: (日均IC, 日均|IC|, 有效天数, 每日IC字典)\n",
        "# 例如：\n",
        "# values = [0.12, 0.05, 0.33,    # 8月1日\n",
        "#           0.15, 0.08, 0.30,    # 8月2日\n",
        "#           0.10, 0.07, 0.28]    # 8月3日\n",
        "\n",
        "# y =      [0.02, 0.01, 0.05,    # 8月1日\n",
        "#           0.03, 0.02, 0.04,    # 8月2日\n",
        "#           0.01, 0.00, 0.03]    # 8月3日\n",
        "\n",
        "# dates =  ['2024-08-01'] * 3 + ['2024-08-02'] * 3 + ['2024-08-03'] * 3\n",
        "\n",
        "#     \"\"\"\n",
        "def daily_ic_stats(values, y, dates):\n",
        "    vals  = np.asarray(values)\n",
        "    y     = np.asarray(y)\n",
        "    dates = np.asarray(dates)\n",
        "\n",
        "    day_map = {}\n",
        "    for d in np.unique(dates):\n",
        "        idx = (dates == d)\n",
        "        ic = spearmanr(vals[idx], y[idx]).correlation\n",
        "        if np.isfinite(ic):         #  避免全 NaN 把均值拖没\n",
        "            day_map[d] = float(ic)\n",
        "\n",
        "    if not day_map:\n",
        "        return np.nan, np.nan, 0, {}\n",
        "\n",
        "    ics = np.fromiter(day_map.values(), dtype=float)\n",
        "    return float(ics.mean()), float(np.abs(ics).mean()), int(ics.size), day_map\n",
        "\n",
        "\n",
        "\n",
        "def select_best_component_daily(comps: np.ndarray, y: np.ndarray, dates: np.ndarray):\n",
        "    \"\"\"\n",
        "    用“|日均IC|”挑最优组件；若全 NaN，回退到 “日均|IC|”\n",
        "    组件是 gplearn 生成的候选因子（数学表达式）的输出结果，用于评估和选择最优因子\n",
        "    返回 (best_idx, summary_list)\n",
        "    \"\"\"\n",
        "    k = comps.shape[1]\n",
        "    stats = []\n",
        "    for j in range(k):\n",
        "        m_ic, m_abs, n_days, _ = daily_ic_stats(comps[:, j], y, dates)\n",
        "        stats.append((j, m_ic, m_abs, n_days))\n",
        "    scores = np.array([abs(s[1]) for s in stats], dtype=float)\n",
        "    if not np.isfinite(scores).any():\n",
        "        scores = np.array([s[2] for s in stats], dtype=float)  # 回退\n",
        "    best_idx = int(np.nanargmax(scores))\n",
        "    return best_idx, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUSGtz6T9veM"
      },
      "source": [
        "# gplearn 因子训练与评估流程\n",
        "\n",
        "## 1) 构造训练样本\n",
        "- 读取并排序 Parquet 数据，标准化时间列  \n",
        "- 计算未来 `HORIZON_STP` 步的中间价收益作为标签  \n",
        "- 按 `STRIDE` 步长与 `WINDOW_SIZE` 窗口生成 v-特征矩阵  \n",
        "-\n",
        "\n",
        "## 2) 训练 SymbolicTransformer\n",
        "- 遗传编程搜索候选因子（函数集含 `add`、`sub`、`mul`、`div`、`sin`、`cos`、`log`、`sqrt`）  \n",
        "- 评估指标设为 `spearman`，生成 4 个候选因子  \n",
        "- 固定随机种子保证可复现\n",
        "\n",
        "## 3) 评估与选择最佳组件\n",
        "- 逐日计算各组件 IC，选择 **|日均IC| 最大** 的作为最佳  \n",
        "- 输出各组件统计并记录最佳表达式\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHGVKOt4IIbe"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1) 读取训练集 & 构造样本\n",
        "#   - 读取 Parquet，时间列标准化\n",
        "#   - 构造标签：未来 HORIZON_STP 步的中间价收益\n",
        "#   - 以 STRIDE 抽样，滑窗长度 WINDOW_SIZE，计算 v-特征\n",
        "# ============================================================\n",
        "import os, re, glob, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\" 读取训练集\")\n",
        "df_tr = purge(pl.scan_parquet(TRAIN_FILE).sort(\"ts\"))\n",
        "df_tr = ensure_ts_datetime(df_tr, \"ts\").sort(\"ts\")\n",
        "print(\"完成，行数 =\", len(df_tr))\n",
        "\n",
        "# 原始矩阵 / 标签\n",
        "X_raw_tr = build_matrix(df_tr)\n",
        "mid_now_tr = (df_tr[\"Buy1Price\"] + df_tr[\"Sell1Price\"]) / 2\n",
        "mid_fut_tr = mid_now_tr.shift(-HORIZON_STP)\n",
        "ret_tr = ((mid_fut_tr - mid_now_tr) / mid_now_tr).fill_nan(0).to_numpy()\n",
        "\n",
        "idx_tr = np.arange(WINDOW_SIZE-1, len(df_tr)-HORIZON_STP, STRIDE)\n",
        "y_ic_tr = ret_tr[idx_tr]\n",
        "X_tr = np.stack([compute_v_features(X_raw_tr[t-WINDOW_SIZE+1:t+1])\n",
        "                 for t in idx_tr], axis=0).astype(np.float32)\n",
        "dates_tr = df_tr[\"ts\"].dt.date().to_numpy()[idx_tr]\n",
        "\n",
        "print(\"训练特征矩阵:\", X_tr.shape, \"| 有效样本日期数(去重):\", len(np.unique(dates_tr)))\n",
        "\n",
        "# ============================================================\n",
        "#  训练 gplearn SymbolicTransformer\n",
        "#   - n_components=4：同时产出 4 个候选因子\n",
        "# ============================================================\n",
        "gp = SymbolicTransformer(\n",
        "    generations=GENERATIONS, population_size=POP_SIZE,\n",
        "    tournament_size=TOURNAMENT_SZ, init_depth=(2, MAX_DEPTH),\n",
        "    function_set=['add','sub','mul','div','sin','cos','log','sqrt'],\n",
        "    metric='spearman', const_range=(-1,1),\n",
        "    parsimony_coefficient=5e-4, n_components=4,\n",
        "    verbose=1, n_jobs=-1, random_state=SEED,  # 固定种子便于复现\n",
        ")\n",
        "gp.fit(X_tr, y_ic_tr)\n",
        "gp.feature_names_ = FEATURE_NAMES\n",
        "\n",
        "# ============================================================\n",
        "# 训练集逐日IC评估，选择最佳组件（按“|日均IC|”）\n",
        "#   - comps_tr: (N, n_components)\n",
        "# 每天单独计算该日所有样本的 Spearman IC（因子值 vs 对应未来收益）。\n",
        "# 得到一列 “每日 IC” 时间序列。\n",
        "# 再对这些日度 IC 取均值（以及均值绝对值、标准差等），作为因子长期表现的统计指标。\n",
        "#\n",
        "# ============================================================\n",
        "comps_tr = gp.transform(X_tr)   # (N, n_components)\n",
        "best_idx, tr_stats = select_best_component_daily(comps_tr, y_ic_tr, dates_tr)\n",
        "best_prog = gp._best_programs[best_idx]\n",
        "expr_str  = pretty_expr(best_prog, FEATURE_NAMES)\n",
        "\n",
        "# 打印训练集评估详情\n",
        "best_mean_ic  = [s[1] for s in tr_stats][best_idx]   # 日均IC\n",
        "best_mean_abs = [s[2] for s in tr_stats][best_idx]   # 日均|IC|\n",
        "best_days     = [s[3] for s in tr_stats][best_idx]\n",
        "print(\"\\n====== 训练集（逐日IC）======\")\n",
        "for j, m_ic, m_abs, n_days in tr_stats:\n",
        "    print(f\"组件#{j}: 日均IC={m_ic:.4f} | 日均|IC|={m_abs:.4f} | 有效天数={n_days}\")\n",
        "print(f\"\\n▶ 选择组件 #{best_idx} 作为最佳（按“|日均IC|”）\")\n",
        "print(\"表达式：\")\n",
        "print(expr_str)\n",
        "\n",
        "# 供后续写 Word/日志使用，避免 NameError\n",
        "best_ic_tr      = float(best_mean_ic)         # 训练集 日均IC\n",
        "best_abs_ic_tr  = float(abs(best_mean_ic))    # 训练集 |日均IC|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLbCylRR9_H7"
      },
      "source": [
        "# 模型保存与验证流程\n",
        "\n",
        "## 2c) 保存模型与元数据\n",
        "- **新建运行目录**：若 `MODEL_DIR` 未定义，则以时间戳生成 `run_YYYYMMDD_HHMMSS` 目录  \n",
        "- **分配模型编号**：`model_{idx}.pkl`，避免与现有文件冲突  \n",
        "- **保存内容**：\n",
        "  - `gp` 对象（gplearn 模型）\n",
        "  - 特征名 `feature_names`\n",
        "  - 最优组件索引 `best_idx`\n",
        "  - 元数据 `meta`（训练区间、步长、窗口、预测周期、随机种子、函数集等）\n",
        "- **维护 manifest.json**：\n",
        "  - 每次保存更新清单，记录文件名、编号、最优组件、时间戳、特征数、元数据\n",
        "\n",
        "## 3) 验证集评估\n",
        "- **读取验证集**：\n",
        "  - Parquet → 排序 → 时间列标准化\n",
        "  - 调用 `purge` 清洗数据\n",
        "- **防泄漏断言**：\n",
        "  - 确保训练结束时间早于验证开始时间\n",
        "- **构造验证样本**：\n",
        "  - 与训练集相同的滑窗与步长逻辑\n",
        "  - 计算 v-特征与未来收益标签\n",
        "- **评估训练选出的最佳组件**：\n",
        "  - 调用 `daily_ic_stats` 计算验证集日均 IC\n",
        "  - 输出：\n",
        "    - `|日均IC|`（主口径）\n",
        "\n",
        "    - 有效天数\n",
        "  - 打印最差/最好 5 天的 IC 以查看稳定性\n",
        "- **回填 manifest**：\n",
        "  - 将验证区间 `val_range` 写入对应模型记录，便于追溯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skj1k17toqmA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 2c) 统一保存：model_{idx}.pkl（含 best_idx / feature_names / meta）\n",
        "#   - 保存到 <BASE_DIR>/models/run_YYYYMMDD_HHMMSS 或现有 MODEL_DIR\n",
        "\n",
        "if \"new_run_dir\" not in globals():\n",
        "    def new_run_dir(base_dir: str, subdir: str = \"models\") -> str:\n",
        "        ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        run_dir = os.path.join(base_dir, subdir, f\"run_{ts}\")\n",
        "        os.makedirs(run_dir, exist_ok=True)\n",
        "        globals()[\"MODEL_DIR\"] = run_dir\n",
        "        print(f\"[MODEL_DIR] {run_dir}\")\n",
        "        return run_dir\n",
        "\n",
        "if \"save_gp_model_bundle\" not in globals():\n",
        "    import cloudpickle\n",
        "    def _atomic_pickle_dump(obj, path):\n",
        "        tmp = f\"{path}.tmp\"\n",
        "        with open(tmp, \"wb\") as f:\n",
        "            cloudpickle.dump(obj, f)\n",
        "        os.replace(tmp, path)\n",
        "    def _append_manifest(out_dir: str, rec: dict):\n",
        "        path = os.path.join(out_dir, \"manifest.json\")\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data = json.load(f)\n",
        "            except Exception:\n",
        "                data = {\"models\": []}\n",
        "        else:\n",
        "            data = {\"models\": []}\n",
        "        data.setdefault(\"models\", []).append(rec)\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"[MANIFEST] 已更新: {path}\")\n",
        "    def save_gp_model_bundle(gp_obj, idx, out_dir, feature_names, *, best_idx=None, meta=None, overwrite=False):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        save_path = os.path.join(out_dir, f\"model_{int(idx)}.pkl\")\n",
        "        if os.path.exists(save_path) and not overwrite:\n",
        "            raise FileExistsError(f\"{save_path} 已存在；如需覆盖请传 overwrite=True\")\n",
        "        bundle = {\n",
        "            \"version\": 1,\n",
        "            \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"gp\": gp_obj,\n",
        "            \"feature_names\": list(feature_names) if feature_names is not None else None,\n",
        "            \"best_idx\": int(best_idx) if best_idx is not None else None,\n",
        "            \"meta\": meta or {}\n",
        "        }\n",
        "        _atomic_pickle_dump(bundle, save_path)\n",
        "        manifest_rec = {\n",
        "            \"file\": os.path.basename(save_path),\n",
        "            \"idx\": int(idx),\n",
        "            \"best_idx\": bundle[\"best_idx\"],\n",
        "            \"created_at\": bundle[\"created_at\"],\n",
        "            \"n_features\": None if feature_names is None else len(feature_names),\n",
        "            \"meta\": bundle[\"meta\"]\n",
        "        }\n",
        "        _append_manifest(out_dir, manifest_rec)\n",
        "        print(f\"[SAVE] 模型已保存: {save_path}\")\n",
        "        return save_path\n",
        "\n",
        "# —— 避免与已有 model_*.pkl 冲突\n",
        "def _existing_model_indices(out_dir: str):\n",
        "    files = glob.glob(os.path.join(out_dir, \"model_*.pkl\"))\n",
        "    idxs = []\n",
        "    for p in files:\n",
        "        m = re.search(r\"model_(\\d+)\\.pkl$\", os.path.basename(p))\n",
        "        if m:\n",
        "            idxs.append(int(m.group(1)))\n",
        "    return sorted(set(idxs))\n",
        "\n",
        "def _allocate_model_idx(out_dir: str, prefer_idx=None):\n",
        "    used = set(_existing_model_indices(out_dir))\n",
        "    if prefer_idx is not None and int(prefer_idx) not in used:\n",
        "        print(f\"[INFO] 使用显式指定的 MODEL_IDX={int(prefer_idx)}\")\n",
        "        return int(prefer_idx)\n",
        "    # 从 0 开始找第一个空位\n",
        "    i = 0\n",
        "    while i in used:\n",
        "        i += 1\n",
        "    if prefer_idx is not None:\n",
        "        print(f\"[INFO] 指定的 MODEL_IDX={int(prefer_idx)} 已存在，自动改用 {i}\")\n",
        "    else:\n",
        "        print(f\"[INFO] 自动分配 MODEL_IDX={i}\")\n",
        "    return i\n",
        "\n",
        "# —— 确定输出目录（优先使用已有 MODEL_DIR；否则创建新 run 目录）\n",
        "RUN_DIR = globals().get(\"MODEL_DIR\", None)\n",
        "if not RUN_DIR:\n",
        "    RUN_DIR = new_run_dir(BASE_DIR)  # 会把全局 MODEL_DIR 指向该目录\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# —— 分配编号；允许通过全局 MODEL_IDX 强制指定\n",
        "MODEL_IDX = _allocate_model_idx(RUN_DIR, prefer_idx=globals().get(\"MODEL_IDX\", None))\n",
        "\n",
        "# —— 记录元数据（train/val 时间会在后面回填 val_range）\n",
        "meta = {\n",
        "    \"train_range\": [str(np.min(dates_tr)), str(np.max(dates_tr))],\n",
        "    \"val_range\": None,  # 待回填\n",
        "    \"stride\": int(STRIDE) if \"STRIDE\" in globals() else None,\n",
        "    \"window\": int(WINDOW_SIZE) if \"WINDOW_SIZE\" in globals() else None,\n",
        "    \"horizon\": int(HORIZON_STP) if \"HORIZON_STP\" in globals() else None,\n",
        "    \"random_state\": int(SEED) if \"SEED\" in globals() else None,\n",
        "    \"function_set\": list(getattr(gp, \"function_set\", []))\n",
        "}\n",
        "\n",
        "# —— 保存为 model_{idx}.pkl（携带 best_idx/feature_names/meta）\n",
        "model_path = save_gp_model_bundle(\n",
        "    gp_obj=gp,\n",
        "    idx=MODEL_IDX,\n",
        "    out_dir=RUN_DIR,\n",
        "    feature_names=FEATURE_NAMES,\n",
        "    best_idx=best_idx,\n",
        "    meta=meta,\n",
        "    overwrite=False\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 3) 读取验证集 & 评估同一组件（与训练完全解耦的数据）\n",
        "#   - 再次构造 v-特征\n",
        "#   - 用训练时选出的 best_idx 在验证段做逐日IC\n",
        "#\n",
        "# ============================================================\n",
        "print(\"\\n 读取验证集 …\")\n",
        "df_val = purge(pl.scan_parquet(VAL_FILE).sort(\"ts\"))\n",
        "df_val = ensure_ts_datetime(df_val, \"ts\").sort(\"ts\")\n",
        "print(\"完成，行数 =\", len(df_val))\n",
        "\n",
        "# 训练区间应完全早于验证区间（防止泄漏）\n",
        "tr_end = pd.to_datetime(df_tr[\"ts\"].to_pandas()).max()\n",
        "val_start = pd.to_datetime(df_val[\"ts\"].to_pandas()).min()\n",
        "assert tr_end < val_start, f\"时间泄漏：train 结束 {tr_end} 不早于 val 开始 {val_start}\"\n",
        "\n",
        "# 验证集特征 / 标签\n",
        "X_raw_val = build_matrix(df_val)\n",
        "mid_now_val = (df_val[\"Buy1Price\"] + df_val[\"Sell1Price\"]) / 2\n",
        "mid_fut_val = mid_now_val.shift(-HORIZON_STP)\n",
        "ret_val = ((mid_fut_val - mid_now_val) / mid_now_val).fill_nan(0).to_numpy()\n",
        "\n",
        "idx_val = np.arange(WINDOW_SIZE-1, len(df_val)-HORIZON_STP, STRIDE)\n",
        "y_ic_val = ret_val[idx_val]\n",
        "X_val = np.stack([compute_v_features(X_raw_val[t-WINDOW_SIZE+1:t+1])\n",
        "                  for t in idx_val], axis=0).astype(np.float32)\n",
        "dates_val = df_val[\"ts\"].dt.date().to_numpy()[idx_val]\n",
        "\n",
        "print(\"验证特征矩阵:\", X_val.shape, \"| 有效样本日期数(去重):\", len(np.unique(dates_val)))\n",
        "\n",
        "# 仅评估训练阶段选出的最佳组件\n",
        "comps_val = gp.transform(X_val)\n",
        "val_mean_ic, val_mean_abs, val_days, val_day_map = daily_ic_stats(\n",
        "    comps_val[:, best_idx], y_ic_val, dates_val\n",
        ")\n",
        "# 兼容下游写报告/日志用的变量名\n",
        "val_ic     = float(val_mean_ic)\n",
        "val_abs_ic = float(abs(val_mean_ic))\n",
        "\n",
        "print(\"\\n====== 验证集（逐日IC）======\")\n",
        "print(f\"验证集 |日均IC| = {abs(val_mean_ic):.4f}\")   # 主口径：先均值，再取绝对值\n",
        "print(f\"验证集 日均|IC| = {val_mean_abs:.4f}\")     # 参考口径\n",
        "print(f\"验证集 有效天数 = {val_days}\")\n",
        "\n",
        "# 打印前后各5天的逐日IC，便于快速查看稳定性\n",
        "if val_day_map:\n",
        "    kv = sorted(val_day_map.items(), key=lambda x: x[1])\n",
        "    worst5 = kv[:5]\n",
        "    best5  = kv[-5:]\n",
        "    print(\"\\n最差 5 天：\")\n",
        "    for d, ic in worst5: print(d, f\"{ic:.4f}\")\n",
        "    print(\"\\n最好 5 天：\")\n",
        "    for d, ic in best5:  print(d, f\"{ic:.4f}\")\n",
        "\n",
        "# —— 回填验证区间到 manifest\n",
        "try:\n",
        "    manifest_path = os.path.join(RUN_DIR, \"manifest.json\")\n",
        "    if os.path.exists(manifest_path):\n",
        "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            mani = json.load(f)\n",
        "        for rec in mani.get(\"models\", []):\n",
        "            if rec.get(\"file\") == os.path.basename(model_path):\n",
        "                rec.setdefault(\"meta\", {})\n",
        "                rec[\"meta\"][\"val_range\"] = [str(np.min(dates_val)), str(np.max(dates_val))]\n",
        "        with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(mani, f, ensure_ascii=False, indent=2)\n",
        "        print(\"[MANIFEST] 已回填验证区间 val_range\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] 回填 manifest 失败：\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-MlaAa84aSM"
      },
      "outputs": [],
      "source": [
        "pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvl-cHd13vJc"
      },
      "outputs": [],
      "source": [
        "import os, re, pathlib, cloudpickle, json, docx\n",
        "\n",
        "# ========= 1 基本路径 =========\n",
        "BASE_DIR = \"/content/drive/My Drive/gplearn\"\n",
        "pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)  # 保证目录存在\n",
        "\n",
        "# ========= 2 计算下一个编号 =========\n",
        "def next_index(folder):\n",
        "    nums = [int(m.group(1))\n",
        "            for name in os.listdir(folder)\n",
        "            if (m := re.match(r\"model_(\\d+)\\.pkl$\", name))]\n",
        "    return max(nums) + 1 if nums else 1\n",
        "\n",
        "idx = next_index(BASE_DIR)\n",
        "print(f\"即将保存为 model_{idx}.pkl\")\n",
        "\n",
        "# ========= 3 保存模型 =========\n",
        "model_path = os.path.join(BASE_DIR, f\"model_{idx}.pkl\")\n",
        "with open(model_path, \"wb\") as f:\n",
        "    cloudpickle.dump(gp, f)\n",
        "print(\" 模型已保存:\", model_path)\n",
        "\n",
        "# ========= 4 追加 / 创建 Word 文档 =========\n",
        "docx_path = os.path.join(BASE_DIR, \"因子总表.docx\")\n",
        "doc = docx.Document(docx_path) if os.path.exists(docx_path) else docx.Document()\n",
        "\n",
        "doc.add_heading(f'模型 {idx}', level=2)\n",
        "doc.add_paragraph(expr_str)\n",
        "doc.add_paragraph(f\"训练集 IC = {best_ic_tr:.4f}\")\n",
        "doc.add_paragraph(f\"验证集 IC = {val_ic:.4f}\")\n",
        "doc.add_paragraph(\"\")   # 空行分隔\n",
        "\n",
        "doc.save(docx_path)\n",
        "print(\" Word 已更新:\", docx_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnuAXbr32wqu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "BASE_DIR = \"/content/drive/My Drive/gplearn\"\n",
        "print(\"当前目录文件：\", os.listdir(BASE_DIR))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "批量提取 gplearn 模型生成的因子，计算它们的Spearman 相关性矩阵，并找出高相关的因子对，同时将结果导出为 CSV 文件"
      ],
      "metadata": {
        "id": "ri4xI4kh-1x3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udgOEkeCuQba"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "try:\n",
        "    X_v_tr\n",
        "except NameError:\n",
        "    X_v_tr = X_tr\n",
        "try:\n",
        "    X_v_val\n",
        "except NameError:\n",
        "    X_v_val = X_val\n",
        "\n",
        "# —— 模型目录：把当前 BASE_DIR 以及上面训练时的 MODEL_DIR 都纳入 ——\n",
        "GP_DIRS = []\n",
        "for d in {BASE_DIR, MODEL_DIR}:\n",
        "    if isinstance(d, str) and os.path.isdir(d):\n",
        "        GP_DIRS.append(d)\n",
        "print(\" 搜索模型目录：\", GP_DIRS)\n",
        "\n",
        "# —— 只取“最佳组件”，若未保存 best_idx 就默认取 #0 ——\n",
        "# 因为我上周做的时候没有保存 best_idx，所以这里默认取 #0\n",
        "\n",
        "MODE = \"best\"          # 可改成 \"all\" 抽出每个模型的全部组件\n",
        "CORR_TH = 0.90         # 打印高相关对阈值\n",
        "INCLUDE_ORIG = False   # True 时原始特征也一起进相关矩阵（矩阵会更大）\n",
        "\n",
        "def load_gp_or_bundle(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        obj = cloudpickle.load(f)\n",
        "    if isinstance(obj, dict) and \"gp\" in obj:\n",
        "        return {\n",
        "            \"gp\": obj[\"gp\"],\n",
        "            \"best_idx\": obj.get(\"best_idx\"),\n",
        "            \"expr_str\": obj.get(\"expr_str\"),\n",
        "            \"feature_names\": obj.get(\"feature_names\"),\n",
        "        }\n",
        "    return {\"gp\": obj, \"best_idx\": None, \"expr_str\": None, \"feature_names\": None}\n",
        "\n",
        "# 1) 收集所有模型文件\n",
        "gp_paths = []\n",
        "for d in GP_DIRS:\n",
        "    gp_paths += glob.glob(os.path.join(d, \"model_*.pkl\"))\n",
        "gp_paths = sorted(set(gp_paths))\n",
        "print(f\" 共发现 {len(gp_paths)} 个模型文件\")\n",
        "\n",
        "# 2) 抽取组件\n",
        "# 因为之前有版本不兼容问题，所以这里进行详细的分类和跳过\n",
        "features_tr_list, features_val_list, colnames_gp = [], [], []\n",
        "for p in gp_paths:\n",
        "    name = os.path.basename(p)\n",
        "    try:\n",
        "        bundle = load_gp_or_bundle(p)\n",
        "        gp_obj  = bundle[\"gp\"]\n",
        "        # 是否训练过\n",
        "        if not hasattr(gp_obj, \"_best_programs\") or not gp_obj._best_programs:\n",
        "            print(f\"  • {name:>14s} → 尚未训练（NotFitted），跳过\")\n",
        "            continue\n",
        "        comps_tr  = gp_obj.transform(X_v_tr)  # (N_tr, k)\n",
        "        comps_val = gp_obj.transform(X_v_val) # (N_val, k)\n",
        "        k = comps_tr.shape[1]\n",
        "    except Exception as e:\n",
        "        print(f\"  • {name:>14s} → transform 失败：{e.__class__.__name__}，跳过\")\n",
        "        continue\n",
        "\n",
        "    if MODE == \"best\":\n",
        "        bidx = 0 if bundle[\"best_idx\"] is None else int(bundle[\"best_idx\"])\n",
        "        if bundle[\"best_idx\"] is None:\n",
        "            print(f\"  • {name:>14s} → 未存 best_idx，默认用组件 #0\")\n",
        "        features_tr_list.append(comps_tr[:, bidx])\n",
        "        features_val_list.append(comps_val[:, bidx])\n",
        "        colnames_gp.append(f\"{name.replace('.pkl','')}_best{bidx}\")\n",
        "        print(f\"  • {name:>14s} → 用组件 #{bidx}\")\n",
        "    else:\n",
        "        for j in range(k):\n",
        "            features_tr_list.append(comps_tr[:, j])\n",
        "            features_val_list.append(comps_val[:, j])\n",
        "            colnames_gp.append(f\"{name.replace('.pkl','')}_c{j}\")\n",
        "        print(f\"  • {name:>14s} → 抽取全部 {k} 个组件\")\n",
        "\n",
        "if not features_tr_list:\n",
        "    print(\"没有成功抽取到任何 gplearn 因子，结束。\")\n",
        "else:\n",
        "    # 3) 组装 DataFrame\n",
        "    df_gp_tr  = pd.DataFrame(np.column_stack(features_tr_list), columns=colnames_gp)\n",
        "    df_gp_val = pd.DataFrame(np.column_stack(features_val_list), columns=colnames_gp)\n",
        "    print(f\" gplearn 因子（训练/验证）形状：{df_gp_tr.shape} / {df_gp_val.shape}\")\n",
        "\n",
        "    # 4) 相关性（训练集）\n",
        "    if INCLUDE_ORIG:\n",
        "        df_tr_all = pd.concat([pd.DataFrame(X_tr), df_gp_tr], axis=1)\n",
        "        corr_df = df_tr_all.corr(method=\"spearman\")\n",
        "        out_csv = os.path.join(GP_DIRS[0], \"all_features_spearman_corr_train.csv\")\n",
        "        print(\" 已计算：原始+gplearn 全特征 Spearman 相关矩阵（训练集）\")\n",
        "    else:\n",
        "        corr_df = df_gp_tr.corr(method=\"spearman\")\n",
        "        out_csv = os.path.join(GP_DIRS[0], \"gp_factor_spearman_corr_train.csv\")\n",
        "        print(\" 已计算：gplearn 因子 Spearman 相关矩阵（训练集）\")\n",
        "\n",
        "    # 5) 打印高相关对\n",
        "    print(f\"\\n 高相关对（|corr| ≥ {CORR_TH}，不含对角）：\")\n",
        "    cols = corr_df.columns.tolist()\n",
        "    high_pairs = []\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i+1, len(cols)):\n",
        "            c = corr_df.iat[i, j]\n",
        "            if np.isfinite(c) and abs(c) >= CORR_TH:\n",
        "                high_pairs.append((cols[i], cols[j], float(c)))\n",
        "    if high_pairs:\n",
        "        for a, b, c in sorted(high_pairs, key=lambda x: -abs(x[2]))[:50]:\n",
        "            print(f\"  {a:>28s} ~ {b:<28s} : {c:+.4f}\")\n",
        "    else:\n",
        "        print(\"  无。\")\n",
        "\n",
        "    # 6) 导出\n",
        "    corr_df.to_csv(out_csv, index=True)\n",
        "    print(\" 相关矩阵已导出：\", out_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在评估前对 gplearn 因子数据做预处理，包括去除重复列名、清洗因子列表、统一评估参数配置，并准备好输出目录。"
      ],
      "metadata": {
        "id": "6-nZlCOj-8Az"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNQXc7i2xq3O"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json, time, re, glob, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cloudpickle\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "# tqdm 进度条（无则自动安装）\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except Exception:\n",
        "    !pip -q install tqdm\n",
        "    from tqdm import tqdm\n",
        "\n",
        "# 降噪：静默某些历史包的弃用警告（不影响结果）\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "def banner(txt: str):\n",
        "    \"\"\"阶段标题打印\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(txt)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "def save_csv(df: pd.DataFrame, path: str, index=False, head_n: int = 5, name: str = \"表格\"):\n",
        "    \"\"\"保存 CSV + 控制台预览（前 n 行）\"\"\"\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    df.to_csv(path, index=index, encoding=\"utf-8-sig\")\n",
        "    print(f\" {name} 已保存：{path}\")\n",
        "    with pd.option_context('display.max_rows', 5, 'display.max_columns', 12):\n",
        "        print(f\" {name} 预览（前{head_n}行）:\")\n",
        "        print(df.head(head_n))\n",
        "\n",
        "def _dedup_columns_inplace(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    对 DataFrame 执行“列名去重”，仅保留每个重名的**首列**。\n",
        "    返回一个新的 DataFrame；若无重复则原样返回。\n",
        "    \"\"\"\n",
        "    dup_mask = pd.Index(df.columns).duplicated(keep='first')\n",
        "    if dup_mask.any():\n",
        "        dup_names = list(pd.Index(df.columns)[dup_mask])\n",
        "        print(f\"[WARN] {name} 存在重复列名 {dup_mask.sum()} 个，仅保留首列。示例: {dup_names[:10]}\")\n",
        "        return df.loc[:, ~dup_mask]\n",
        "    return df\n",
        "\n",
        "# ========== 目录/全局配置 ==========\n",
        "EVAL_DIR = os.path.join(BASE_DIR, \"eval_artifacts\")\n",
        "os.makedirs(EVAL_DIR, exist_ok=True)\n",
        "print(f\"评估产物目录：{EVAL_DIR}\")\n",
        "\n",
        "# 【统一入口】相关性阈值（所有步骤仅认这一处）\n",
        "CORR_TH = 0.95\n",
        "print(f\"[CONF] 统一 CORR_TH = {CORR_TH}\")\n",
        "\n",
        "# 【统一分组数】\n",
        "Q = 5\n",
        "\n",
        "# ========== 预处理：去重列名，清理候选列表 ==========\n",
        "# 1) 对训练段、验证段的因子矩阵做列名去重（避免 DataFrame['col'] 返回二维）\n",
        "df_gp_tr  = _dedup_columns_inplace(df_gp_tr,  \"df_gp_tr\")\n",
        "df_gp_val = _dedup_columns_inplace(df_gp_val, \"df_gp_val\")\n",
        "\n",
        "# 2) 清洗因子候选列表：仅保留验证段里存在的列，并去重保持顺序\n",
        "_col_before = len(colnames_gp)\n",
        "colnames_gp = [c for c in colnames_gp if c in df_gp_val.columns]\n",
        "colnames_gp = list(dict.fromkeys(colnames_gp))\n",
        "if len(colnames_gp) != _col_before:\n",
        "    print(f\"[INFO] 因子名列表已清洗：原 { _col_before } → 现 { len(colnames_gp) }（去除重复/缺失）\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "单因子 IC（信息系数）评估模块，分别在验证集和训练集上逐个计算因子的日均 IC 指标"
      ],
      "metadata": {
        "id": "5sf2rEz-_Hx5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmLr_4Hno0v3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 步骤 1/6：单因子评估（Validation）\n",
        "# -----------------------------\n",
        "banner(\"步骤 1/6：单因子评估（Validation）\")\n",
        "t0 = time.time()\n",
        "val_rows = []\n",
        "for f in tqdm(colnames_gp, desc=\"逐因子计算 Validation IC\"):\n",
        "    m_ic, m_abs, n_days, _ = daily_ic_stats(df_gp_val[f].values, y_ic_val, dates_val)\n",
        "    val_rows.append({\n",
        "        \"factor\": f,\n",
        "        \"val_mean_ic\": m_ic,           # 日均IC\n",
        "        \"val_abs_mean_ic\": abs(m_ic),  # |日均IC|（排序指标）\n",
        "        \"val_mean_abs_ic\": m_abs,      # 日均|IC|（仅仅作为参考）\n",
        "        \"val_days\": n_days\n",
        "    })\n",
        "val_ic_df = pd.DataFrame(val_rows).sort_values(\"val_abs_mean_ic\", ascending=False)\n",
        "val_ic_path = os.path.join(EVAL_DIR, \"single_factor_ic_val.csv\")\n",
        "save_csv(val_ic_df, val_ic_path, name=\"单因子 IC（Validation）\")\n",
        "print(f\" 用时：{time.time()-t0:.2f}s\")\n",
        "\n",
        "# —— 方向性映射：按验证期 IC 的符号（后续分组收益/表达式/入库都用它）\n",
        "VAL_IC_SIGN = dict(zip(val_ic_df[\"factor\"].astype(str), np.sign(val_ic_df[\"val_mean_ic\"].astype(float)).astype(int)))\n",
        "print(f\"[INFO] 方向映射（验证期 IC 符号）条数: {len(VAL_IC_SIGN)}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 步骤 1b：单因子评估（Train）\n",
        "# -----------------------------\n",
        "banner(\"（可选）步骤 1b：单因子评估（Train）\")\n",
        "t0 = time.time()\n",
        "tr_rows = []\n",
        "for f in tqdm(colnames_gp, desc=\"逐因子计算 Train IC\"):\n",
        "    m_ic, m_abs, n_days, _ = daily_ic_stats(df_gp_tr[f].values, y_ic_tr, dates_tr)\n",
        "    tr_rows.append({\n",
        "        \"factor\": f,\n",
        "        \"tr_mean_ic\": m_ic,\n",
        "        \"tr_abs_mean_ic\": abs(m_ic),\n",
        "        \"tr_mean_abs_ic\": m_abs,\n",
        "        \"tr_days\": n_days\n",
        "    })\n",
        "tr_ic_df = pd.DataFrame(tr_rows).sort_values(\"tr_abs_mean_ic\", ascending=False)\n",
        "tr_ic_path = os.path.join(EVAL_DIR, \"single_factor_ic_train.csv\")\n",
        "save_csv(tr_ic_df, tr_ic_path, name=\"单因子 IC（Train）\")\n",
        "print(f\" 用时：{time.time()-t0:.2f}s\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算 gplearn 提取的因子之间的 Spearman 相关性矩阵，并找出高相关因子对供人工检查。"
      ],
      "metadata": {
        "id": "YhYse7cx_grp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWbJ9I7do4nT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 步骤 2/6：因子相关性矩阵（Validation）\n",
        "#   * 这里再次确保相关矩阵只基于“唯一列名”的矩阵来计算，避免 loc 返回 Series\n",
        "# -----------------------------\n",
        "banner(\"步骤 2/6：因子相关性矩阵（Validation）\")\n",
        "t0 = time.time()\n",
        "\n",
        "# 相关矩阵基于“去重后的验证集因子矩阵”\n",
        "corr_df = df_gp_val.corr(method=\"spearman\")\n",
        "corr_path = os.path.join(EVAL_DIR, \"gp_factor_spearman_corr_val.csv\")\n",
        "save_csv(corr_df, corr_path, index=True, name=\"相关性矩阵（Validation）\")\n",
        "\n",
        "# 打印 |ρ| >= CORR_TH 的 Top 20 高相关对，便于人工审阅\n",
        "pairs = []\n",
        "cols = corr_df.columns.tolist()\n",
        "for i in range(len(cols)):\n",
        "    for j in range(i+1, len(cols)):\n",
        "        c = corr_df.iat[i, j]\n",
        "        if np.isfinite(c) and abs(c) >= CORR_TH:\n",
        "            pairs.append((cols[i], cols[j], float(c)))\n",
        "print(f\" |ρ| ≥ {CORR_TH} 的高相关对数量：{len(pairs)}\")\n",
        "for a, b, c in sorted(pairs, key=lambda x: -abs(x[2]))[:20]:\n",
        "    print(f\"  • {a}  ~  {b} : {c:+.4f}\")\n",
        "print(f\" 用时：{time.time()-t0:.2f}s\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分组收益（Quantile Returns）评估模块，在验证集上根据因子值进行分位数分组，计算每组的平均收益，并结合验证期 IC 的方向来得到方向性调整后的多空收益。"
      ],
      "metadata": {
        "id": "RngJic3L_P-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0ulhQDUo8Rj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 步骤 3/6：分组收益（Validation，含“方向性”）\n",
        "# -----------------------------\n",
        "banner(\"步骤 3/6：分组收益（Validation，含方向性）\")\n",
        "t0 = time.time()\n",
        "\n",
        "def quantile_returns(factor: np.ndarray, y: np.ndarray, dates: np.ndarray, q: int = Q):\n",
        "    \"\"\"\n",
        "    按日分位分组收益（稳健版）：\n",
        "      - 接受 factor/y 为 1D 或 2D；若为 2D，仅取第 1 列\n",
        "      - 每日切片做 NaN/Inf 清理\n",
        "      - 返回 (各分位收益均值列表, spread = Qq - Q1)，若存在缺失则 spread=None\n",
        "    \"\"\"\n",
        "    # ---- 统一成 1D ----\n",
        "    x_all = np.asarray(factor)\n",
        "    y_all = np.asarray(y)\n",
        "\n",
        "    if x_all.ndim == 2:\n",
        "        x_all = x_all[:, 0]\n",
        "    elif x_all.ndim != 1:\n",
        "        raise ValueError(f\"不支持的 factor 维度: {x_all.ndim}\")\n",
        "\n",
        "    if y_all.ndim == 2:\n",
        "        y_all = y_all[:, 0]\n",
        "    elif y_all.ndim != 1:\n",
        "        raise ValueError(f\"不支持的 y 维度: {y_all.ndim}\")\n",
        "\n",
        "    if len(x_all) != len(y_all) or len(x_all) != len(dates):\n",
        "        raise ValueError(f\"长度不一致：len(x)={len(x_all)}, len(y)={len(y_all)}, len(dates)={len(dates)}\")\n",
        "\n",
        "    uniq = np.unique(dates)\n",
        "    buckets = [[] for _ in range(q)]\n",
        "\n",
        "    for d in uniq:\n",
        "        mask = (dates == d)\n",
        "        if mask.sum() < q:\n",
        "            continue\n",
        "\n",
        "        x = x_all[mask].astype(float, copy=False)\n",
        "        r = y_all[mask].astype(float, copy=False)\n",
        "\n",
        "        ok = np.isfinite(x) & np.isfinite(r)\n",
        "        if ok.sum() < q:\n",
        "            continue\n",
        "        x = x[ok]; r = r[ok]\n",
        "\n",
        "        try:\n",
        "            qs = np.quantile(x, np.linspace(0, 1, q + 1))\n",
        "        except Exception:\n",
        "            continue\n",
        "        # 包含两端\n",
        "        qs[0], qs[-1] = -np.inf, np.inf\n",
        "\n",
        "        for k in range(q):\n",
        "            sel = (x > qs[k]) & (x <= qs[k+1])  # 一维布尔\n",
        "            if sel.sum() >= 3:\n",
        "                buckets[k].append(float(np.nanmean(r[sel])))\n",
        "\n",
        "    avg = [np.nan if len(b) == 0 else float(np.nanmean(b)) for b in buckets]\n",
        "    spread = None if any(np.isnan(avg)) else float(avg[-1] - avg[0])\n",
        "    return avg, spread\n",
        "\n",
        "qr_rows = []\n",
        "for f in tqdm(val_ic_df[\"factor\"].tolist(), desc=f\"按 {Q} 分位分组计算\"):\n",
        "    avg, spread = quantile_returns(df_gp_val[f].values, y_ic_val, dates_val, q=Q)\n",
        "    # 方向：按验证期 val_mean_ic 的符号自动决定多空方向\n",
        "    direction = VAL_IC_SIGN.get(str(f), 1)  # 找不到时默认 +1\n",
        "    signed = None if spread is None else float(direction * spread)\n",
        "    row = {\n",
        "        \"factor\": f,\n",
        "        **{f\"Q{k+1}_mean\": avg[k] for k in range(Q)},\n",
        "        f\"LongShort_Q{Q}_minus_Q1\": spread,          # 原始 Q5-Q1\n",
        "        \"direction\": int(direction),                 # +1 多（Q5-Q1），-1 空（Q1-Q5）\n",
        "        \"signed_spread\": signed                      # 按方向后的收益\n",
        "    }\n",
        "    qr_rows.append(row)\n",
        "qr_df = pd.DataFrame(qr_rows)\n",
        "qr_path = os.path.join(EVAL_DIR, f\"quantile_returns_Q{Q}_val.csv\")\n",
        "save_csv(qr_df, qr_path, name=f\"分组收益（Q={Q}, 含方向性, Validation）\")\n",
        "print(f\" 用时：{time.time()-t0:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "基于验证集的因子剪枝，目的是在保留预测力较强因子的同时，去掉与已保留因子高度相关的冗余因子"
      ],
      "metadata": {
        "id": "nLOmjvwm_oKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2QKApOapARt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 步骤 4/6：剪枝（基于 Validation 的 |日均IC| 与相关性）\n",
        "#   * 使用“安全取值”避免重复名/缺失导致的异常\n",
        "# -----------------------------\n",
        "banner(\"步骤 4/6：剪枝（基于 Validation 的 |日均IC| 与相关性）\")\n",
        "t0 = time.time()\n",
        "\n",
        "def _get_corr_safe(cdf: pd.DataFrame, a: str, b: str) -> float:\n",
        "    \"\"\"从相关矩阵安全取标量，异常时返回 NaN。\"\"\"\n",
        "    try:\n",
        "        v = cdf.loc[a, b]\n",
        "    except KeyError:\n",
        "        return np.nan\n",
        "    if isinstance(v, (pd.Series, np.ndarray)):  # 理论上不会发生，做兜底\n",
        "        arr = np.asarray(v).reshape(-1)\n",
        "        return float(arr[0]) if arr.size else np.nan\n",
        "    try:\n",
        "        return float(v)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# 延续你的排序逻辑：按 |日均IC|（val_abs_mean_ic）降序遍历\n",
        "ranked = val_ic_df[\"factor\"].tolist()\n",
        "KEEP, DROP = [], []\n",
        "present = set(corr_df.columns)  # 相关矩阵里真正可用的因子集合\n",
        "\n",
        "for f in ranked:\n",
        "    # 若该因子名不在相关矩阵（极少见），先“乐观保留”并提示\n",
        "    if f not in present:\n",
        "        print(f\"[WARN] 剪枝：相关矩阵中找不到 {f}，暂时保留。\")\n",
        "        KEEP.append(f)\n",
        "        continue\n",
        "\n",
        "    keep_flag = True\n",
        "    for g in KEEP:\n",
        "        if g not in present:\n",
        "            continue\n",
        "        c = _get_corr_safe(corr_df, f, g)\n",
        "        if np.isfinite(c) and abs(c) >= CORR_TH:\n",
        "            # 若与已保留因子高度相关，则丢弃（贪心：越早进入 KEEP 的通常分数更高）\n",
        "            keep_flag = False\n",
        "            break\n",
        "    if keep_flag:\n",
        "        KEEP.append(f)\n",
        "    else:\n",
        "        DROP.append(f)\n",
        "\n",
        "print(f\" 剪枝前：{len(ranked)}  |  剪枝后保留：{len(KEEP)}  |  丢弃：{len(DROP)}（阈值 {CORR_TH}）\")\n",
        "print(\" 保留前 10 个：\", KEEP[:10])\n",
        "print(\" 丢弃前 10 个：\", DROP[:10])\n",
        "\n",
        "pruned_path = os.path.join(EVAL_DIR, f\"selected_factors_after_pruning_corr{CORR_TH:.2f}.json\")\n",
        "with open(pruned_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"keep\": KEEP, \"drop\": DROP, \"corr_threshold\": CORR_TH}, f, ensure_ascii=False, indent=2)\n",
        "print(\" 剪枝清单已保存：\", pruned_path)\n",
        "print(f\"用时：{time.time()-t0:.2f}s\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "从已保留的 gplearn 因子中提取原始表达式、使用 Sympy 化简，并带上方向性信息，以便后续落地或入库。"
      ],
      "metadata": {
        "id": "RwnEO0rO_xvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiFJlLNMpCZz"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 步骤 5/6：表达式提取与化简（用于入库）\n",
        "#   - 兼容模型文件命名（model_* 与 best_ic_gp_model_*）\n",
        "#   - 化简表达式（Sympy），受保护算子：div/sqrt/log 等\n",
        "#   - 输出 direction（用于后续落地）\n",
        "# -----------------------------\n",
        "banner(\"步骤 5/6：表达式提取与化简（用于入库）\")\n",
        "t0 = time.time()\n",
        "\n",
        "# === 模型文件收集器（命名兼容 + 冲突取最新） ===\n",
        "def collect_gp_model_paths(base_dir, model_dir=None):\n",
        "    scan_dirs = {d for d in [base_dir, model_dir or base_dir] if isinstance(d, str) and os.path.isdir(d)}\n",
        "    patterns = [\n",
        "        \"model_*.pkl\",                   # 统一命名（新）\n",
        "        \"best_ic_gp_model_*.pkl\",        # 历史命名（通用）\n",
        "        \"best_ic_gp_model_stride*.pkl\",  # 历史命名（含 stride）\n",
        "        \"gp_model_*.pkl\",                # 其他可能\n",
        "    ]\n",
        "    gp_paths = []\n",
        "    for d in scan_dirs:\n",
        "        for pat in patterns:\n",
        "            gp_paths.extend(glob.glob(os.path.join(d, pat)))\n",
        "    gp_paths = sorted(set(gp_paths))\n",
        "    # 同名取最新（mtime）\n",
        "    gp_map = {}\n",
        "    for p in gp_paths:\n",
        "        key = os.path.basename(p)\n",
        "        if key not in gp_map or os.path.getmtime(p) > os.path.getmtime(gp_map[key]):\n",
        "            gp_map[key] = p\n",
        "\n",
        "    def latest_best_fallback():\n",
        "        cands = [p for p in gp_paths if os.path.basename(p).startswith(\"best_ic_gp_model\")]\n",
        "        return max(cands, key=os.path.getmtime) if cands else None\n",
        "\n",
        "    return gp_paths, gp_map, latest_best_fallback\n",
        "\n",
        "GP_PATHS, GP_MAP, LATEST_BEST_FALLBACK = collect_gp_model_paths(BASE_DIR, globals().get(\"MODEL_DIR\", BASE_DIR))\n",
        "print(f\"[INFO] 扫描到候选模型文件 {len(GP_PATHS)} 个\")\n",
        "\n",
        "# === 反序列化工具 ===\n",
        "def load_bundle(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        obj = cloudpickle.load(f)\n",
        "    # 兼容“直接存 gp 对象”的老格式\n",
        "    return obj if (isinstance(obj, dict) and \"gp\" in obj) else {\"gp\": obj}\n",
        "\n",
        "# === gplearn Program → 可读表达式（优先 export） ===\n",
        "def pretty_expr(program, names):\n",
        "    if hasattr(program, \"export\"):\n",
        "        try:\n",
        "            return program.export(feature_names=names)\n",
        "        except Exception:\n",
        "            pass\n",
        "    expr = str(program)\n",
        "    def repl(m):\n",
        "        i = int(m.group(1))\n",
        "        return names[i] if (0 <= i < len(names)) else f\"X{i}\"\n",
        "    return re.sub(r'\\bX(\\d+)\\b', repl, expr)\n",
        "\n",
        "def gp_program_expr_str(gp_obj, comp_idx, feature_names):\n",
        "    progs = getattr(gp_obj, \"_best_programs\", None)\n",
        "    if not progs:\n",
        "        return None\n",
        "    if comp_idx >= len(progs):\n",
        "        comp_idx = len(progs) - 1\n",
        "    return pretty_expr(progs[comp_idx], feature_names)\n",
        "\n",
        "# === 前缀表达式 → Sympy（含受保护算子） ===\n",
        "import sympy as sp\n",
        "EPS = sp.Float('1e-9')\n",
        "\n",
        "_TOKEN = re.compile(r\"\\s*([A-Za-z_]\\w*|[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][-+]?\\d+)?|\\(|\\)|,)\")\n",
        "\n",
        "def _tokenize(s: str):\n",
        "    out = []; i, n = 0, len(s)\n",
        "    while i < n:\n",
        "        m = _TOKEN.match(s, i)\n",
        "        if not m:\n",
        "            raise ValueError(f\"无法解析: '{s[i:i+32]}'\")\n",
        "        out.append(m.group(1)); i = m.end()\n",
        "    return out\n",
        "\n",
        "def _parse(tokens, i=0):\n",
        "    if i >= len(tokens): raise ValueError(\"意外结束\")\n",
        "    tok = tokens[i]\n",
        "    # 函数调用 name '(' args ')'\n",
        "    if re.match(r\"[A-Za-z_]\\w*\", tok) and i + 1 < len(tokens) and tokens[i+1] == '(':\n",
        "        name = tok; i += 2\n",
        "        args = []\n",
        "        if tokens[i] == ')':\n",
        "            i += 1\n",
        "        else:\n",
        "            while True:\n",
        "                node, i = _parse(tokens, i)\n",
        "                args.append(node)\n",
        "                if tokens[i] == ',': i += 1; continue\n",
        "                if tokens[i] == ')': i += 1; break\n",
        "        return ('call', name, args), i\n",
        "    # 数字\n",
        "    if re.match(r\"^[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][-+]?\\d+)?$\", tok):\n",
        "        return ('num', sp.Float(tok)), i + 1\n",
        "    # 变量\n",
        "    if re.match(r\"^[A-Za-z_]\\w*$\", tok):\n",
        "        return ('var', tok), i + 1\n",
        "    raise ValueError(f\"意外 token: {tok}\")\n",
        "\n",
        "def _var_symbol(var_token: str, feature_names=None):\n",
        "    m = re.match(r\"^[Xx](\\d+)$\", var_token)\n",
        "    if m:\n",
        "        idx = int(m.group(1))\n",
        "        name = feature_names[idx] if (feature_names is not None and 0 <= idx < len(feature_names)) else f\"X{idx}\"\n",
        "        return sp.Symbol(str(name), real=True)\n",
        "    m = re.match(r\"^[vV]_?(\\d+)$\", var_token)\n",
        "    if m:\n",
        "        return sp.Symbol(f\"v{int(m.group(1))}\", real=True)\n",
        "    return sp.Symbol(var_token, real=True)\n",
        "\n",
        "def _fold_bin(args, f2):\n",
        "    if len(args) < 2: return args[0]\n",
        "    acc = f2(args[0], args[1])\n",
        "    for x in args[2:]:\n",
        "        acc = f2(acc, x)\n",
        "    return acc\n",
        "\n",
        "def _ast_to_sympy(ast, feature_names=None):\n",
        "    kind = ast[0]\n",
        "    if kind == 'num': return ast[1]\n",
        "    if kind == 'var': return _var_symbol(ast[1], feature_names)\n",
        "    _, name, args_ast = ast\n",
        "    args = [_ast_to_sympy(a, feature_names) for a in args_ast]\n",
        "    lname = name.lower()\n",
        "\n",
        "    if lname == 'add':  return _fold_bin(args, lambda a,b: a + b)\n",
        "    if lname == 'sub':  return _fold_bin(args, lambda a,b: a - b)\n",
        "    if lname == 'mul':  return _fold_bin(args, lambda a,b: a * b)\n",
        "    if lname in ('div','pdiv','protected_division','safe_div'):\n",
        "        return _fold_bin(args, lambda a,b: a / (b + EPS))  # 受保护除法\n",
        "\n",
        "    if lname == 'neg':\n",
        "        if len(args)!=1: raise ValueError(\"neg 需要 1 个参数\")\n",
        "        return -args[0]\n",
        "    if lname in ('abs',):\n",
        "        if len(args)!=1: raise ValueError(\"abs 需要 1 个参数\")\n",
        "        return sp.Abs(args[0])\n",
        "    if lname in ('sqrt','psqrt','protected_sqrt'):\n",
        "        if len(args)!=1: raise ValueError(\"sqrt 需要 1 个参数\")\n",
        "        return sp.sqrt(sp.Abs(args[0]))\n",
        "    if lname in ('log','plog','protected_log'):\n",
        "        if len(args)!=1: raise ValueError(\"log 需要 1 个参数\")\n",
        "        return sp.log(sp.Abs(args[0]) + EPS)\n",
        "    if lname in ('exp',):\n",
        "        if len(args)!=1: raise ValueError(\"exp 需要 1 个参数\")\n",
        "        return sp.exp(args[0])\n",
        "    if lname in ('sin','cos','tan','sinh','cosh','tanh'):\n",
        "        if len(args)!=1: raise ValueError(f\"{name} 需要 1 个参数\")\n",
        "        return getattr(sp, lname)(args[0])\n",
        "    if lname in ('inv','reciprocal'):\n",
        "        if len(args)!=1: raise ValueError(\"inv 需要 1 个参数\")\n",
        "        return 1/(args[0] + EPS)\n",
        "    if lname == 'pow':\n",
        "        if len(args)!=2: raise ValueError(\"pow 需要 2 个参数\")\n",
        "        return args[0]**args[1]\n",
        "    if lname == 'max':  return sp.Max(*args)\n",
        "    if lname == 'min':  return sp.Min(*args)\n",
        "    if lname == 'clip':\n",
        "        if len(args)!=3: raise ValueError(\"clip 需要 3 个参数\")\n",
        "        x, lo, hi = args\n",
        "        return sp.Min(sp.Max(x, lo), hi)\n",
        "    if lname in ('sign','sgn'):\n",
        "        if len(args)!=1: raise ValueError(\"sign 需要 1 个参数\")\n",
        "        return sp.sign(args[0])\n",
        "    # 未知函数：保留为 Sympy 函数\n",
        "    return sp.Function(name)(*args)\n",
        "\n",
        "def expr_to_sympy(expr_str: str, feature_names=None, simplify_level: str = \"fast\"):\n",
        "    tokens = _tokenize(expr_str)\n",
        "    ast, j = _parse(tokens, 0)\n",
        "    if j != len(tokens):\n",
        "        raise ValueError(\"解析未到末尾，请检查括号/逗号配对\")\n",
        "    expr = _ast_to_sympy(ast, feature_names=feature_names)\n",
        "    if simplify_level == \"none\":\n",
        "        return sp.sstr(expr)\n",
        "    expr = sp.simplify(expr)\n",
        "    if simplify_level == \"aggressive\":\n",
        "        expr = sp.simplify(sp.together(sp.cancel(expr)))\n",
        "    return sp.sstr(expr)\n",
        "\n",
        "# === 表达式提取主流程 ===\n",
        "print(f\"[INFO] 统一相关性阈值 CORR_TH = {CORR_TH}\")\n",
        "\n",
        "def _parse_factor_token(fac: str):\n",
        "    \"\"\"\n",
        "    解析因子名，兼容 'model_12_best0' / 'model_7_c2' 等：\n",
        "      返回 (model_key, tag, comp_idx)\n",
        "      model_key: 'model_12.pkl' 之类；不存在时 None\n",
        "      tag: 'best' 或 'c'\n",
        "      comp_idx: 组件索引\n",
        "    \"\"\"\n",
        "    m = re.search(r\"(model_\\d+)\\s*_(best|c)(\\d+)$\", fac)\n",
        "    if m:\n",
        "        return m.group(1) + \".pkl\", m.group(2), int(m.group(3))\n",
        "    m2 = re.search(r\"(best|c)(\\d+)$\", fac)\n",
        "    comp_idx = int(m2.group(2)) if m2 else 0\n",
        "    tag = m2.group(1) if m2 else \"best\"\n",
        "    return None, tag, comp_idx\n",
        "\n",
        "if \"KEEP\" not in globals():\n",
        "    raise RuntimeError(\"未发现 KEEP（入库候选因子列表）。请先完成剪枝/筛选步骤。\")\n",
        "\n",
        "factor_expr_rows = []\n",
        "ok_expr, ok_simpl = 0, 0\n",
        "\n",
        "for fac in tqdm(KEEP, desc=\"提取/化简表达式\"):\n",
        "    model_key, tag, comp_idx = _parse_factor_token(str(fac))\n",
        "\n",
        "    # 优先 'model_*.pkl'，找不到回退到最新 best_*（命名兼容）\n",
        "    pkl_path = GP_MAP.get(model_key) if model_key else None\n",
        "    if not pkl_path:\n",
        "        pkl_path = LATEST_BEST_FALLBACK()\n",
        "\n",
        "    if not pkl_path:\n",
        "        print(f\"[WARN] {fac}: 未找到对应的 .pkl（model_key={model_key}），跳过\")\n",
        "        factor_expr_rows.append({\"factor\": fac, \"expr\": None, \"expr_simplified\": None, \"direction\": 1})\n",
        "        continue\n",
        "\n",
        "    bundle = load_bundle(pkl_path)\n",
        "    gp_obj = bundle.get(\"gp\", None)\n",
        "    if gp_obj is None:\n",
        "        print(f\"[WARN] {fac}: {os.path.basename(pkl_path)} 不含 'gp'，跳过\")\n",
        "        factor_expr_rows.append({\"factor\": fac, \"expr\": None, \"expr_simplified\": None, \"direction\": 1})\n",
        "        continue\n",
        "\n",
        "    # 列名来源：bundle.feature_names > 全局 FEATURE_NAMES > ORIGINAL_FEATURE_NAMES > X<i>\n",
        "    f_names = (bundle.get(\"feature_names\") or\n",
        "               globals().get(\"FEATURE_NAMES\") or\n",
        "               globals().get(\"ORIGINAL_FEATURE_NAMES\") or\n",
        "               [f\"X{i}\" for i in range(256)])\n",
        "\n",
        "    # 若是 best，且包里提供了 best_idx，以包内为准\n",
        "    if tag == \"best\" and bundle.get(\"best_idx\") is not None:\n",
        "        comp_idx = int(bundle[\"best_idx\"])\n",
        "\n",
        "    # 提取 + 化简\n",
        "    raw_expr = gp_program_expr_str(gp_obj, comp_idx, f_names)\n",
        "    try:\n",
        "        simp_str = expr_to_sympy(raw_expr, feature_names=f_names, simplify_level=\"fast\") if raw_expr else None\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {fac}: 化简失败：{e}\")\n",
        "        simp_str = None\n",
        "\n",
        "    if raw_expr: ok_expr += 1\n",
        "    if simp_str is not None: ok_simpl += 1\n",
        "\n",
        "    direction = VAL_IC_SIGN.get(str(fac), 1)  # 将方向带出用于入库/回测\n",
        "    factor_expr_rows.append({\n",
        "        \"factor\": str(fac),\n",
        "        \"expr\": raw_expr,\n",
        "        \"expr_simplified\": simp_str,\n",
        "        \"direction\": int(direction),\n",
        "        \"corr_threshold_used\": CORR_TH\n",
        "    })\n",
        "\n",
        "# —— 输出结果（以及完整打印不省略）——\n",
        "import difflib\n",
        "\n",
        "def _norm_expr(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    # 统一比较用：去空白\n",
        "    return re.sub(r\"\\s+\", \"\", str(s))\n",
        "\n",
        "expr_df = pd.DataFrame(factor_expr_rows)\n",
        "\n",
        "# 标记是否变化 & 基本长度指标\n",
        "expr_df[\"changed\"] = (_norm_expr(expr_df[\"expr\"]) != _norm_expr(expr_df[\"expr_simplified\"]))\n",
        "expr_df[\"len_raw\"] = expr_df[\"expr\"].map(lambda x: len(x) if isinstance(x, str) else pd.NA)\n",
        "expr_df[\"len_simpl\"] = expr_df[\"expr_simplified\"].map(lambda x: len(x) if isinstance(x, str) else pd.NA)\n",
        "expr_df[\"delta_len\"] = expr_df[\"len_simpl\"] - expr_df[\"len_raw\"]\n",
        "\n",
        "# 粗略操作符计数（可辅助判断复杂度是否降低）\n",
        "_op_pat = re.compile(\n",
        "    r\"\\b(add|sub|mul|div|pdiv|protected_division|safe_div|sqrt|psqrt|protected_sqrt|log|plog|protected_log|\"\n",
        "    r\"exp|sin|cos|tan|sinh|cosh|tanh|min|max|clip|pow|abs|neg|sign|inv|reciprocal)\\b\",\n",
        "    re.I\n",
        ")\n",
        "expr_df[\"ops_raw\"] = expr_df[\"expr\"].map(lambda s: len(_op_pat.findall(s)) if isinstance(s, str) else pd.NA)\n",
        "expr_df[\"ops_simpl\"] = expr_df[\"expr_simplified\"].map(lambda s: len(_op_pat.findall(s)) if isinstance(s, str) else pd.NA)\n",
        "expr_df[\"delta_ops\"] = expr_df[\"ops_simpl\"] - expr_df[\"ops_raw\"]\n",
        "\n",
        "expr_csv = os.path.join(EVAL_DIR, \"selected_factor_expressions.csv\")\n",
        "save_csv(expr_df, expr_csv, name=\"入库候选表达式（含原始/化简/方向/复杂度）\")\n",
        "\n",
        "print(f\" 表达式提取成功 {ok_expr}/{len(KEEP)}；化简成功 {ok_simpl}/{len(KEEP)}\")\n",
        "print(f\" 化简后更短的个数：{int((expr_df['delta_len']<0).fillna(False).sum())} / {len(expr_df)}\")\n",
        "print(f\" 用时：{time.time()-t0:.2f}s\")\n",
        "\n",
        "# 完整打印：展示 原始 vs 化简后（不截断）\n",
        "display_cols = [\n",
        "    \"factor\", \"direction\", \"changed\",\n",
        "    \"len_raw\", \"len_simpl\", \"delta_len\",\n",
        "    \"ops_raw\", \"ops_simpl\", \"delta_ops\",\n",
        "    \"expr\", \"expr_simplified\"\n",
        "]\n",
        "with pd.option_context('display.max_colwidth', None, 'display.width', None, 'display.max_columns', None):\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(expr_df[display_cols])\n",
        "    except Exception:\n",
        "        print(expr_df[display_cols].to_string(index=False))\n",
        "\n",
        "# 另存一份纯文本，附带 unified diff（控制长度，避免过大）\n",
        "expr_txt = os.path.join(EVAL_DIR, \"selected_factor_expressions.txt\")\n",
        "with open(expr_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, row in expr_df.iterrows():\n",
        "        fac = row[\"factor\"]; direc = row[\"direction\"]\n",
        "        raw = str(row[\"expr\"]) if row[\"expr\"] is not None else \"\"\n",
        "        simp = str(row[\"expr_simplified\"]) if row[\"expr_simplified\"] is not None else \"\"\n",
        "        f.write(\n",
        "            f\"[{i}] factor={fac}  direction={direc}  changed={bool(row['changed'])}  \"\n",
        "            f\"len_raw={row['len_raw']}  len_simpl={row['len_simpl']}  delta_len={row['delta_len']}  \"\n",
        "            f\"ops_raw={row['ops_raw']}  ops_simpl={row['ops_simpl']}  delta_ops={row['delta_ops']}\\n\"\n",
        "        )\n",
        "        f.write(\"—— RAW ——\\n\"); f.write(raw + \"\\n\")\n",
        "        f.write(\"—— SIMPLIFIED ——\\n\"); f.write(simp + \"\\n\")\n",
        "\n",
        "        diff_lines = list(difflib.unified_diff(\n",
        "            raw.splitlines(keepends=False),\n",
        "            simp.splitlines(keepends=False),\n",
        "            fromfile=\"raw\", tofile=\"simplified\", lineterm=\"\"\n",
        "        ))\n",
        "        if diff_lines:\n",
        "            f.write(\"—— DIFF ——\\n\")\n",
        "            # 最多写入 200 行 diff，避免文件过大\n",
        "            for line in diff_lines[:200]:\n",
        "                f.write(line + \"\\n\")\n",
        "        f.write(\"-\" * 120 + \"\\n\")\n",
        "\n",
        "print(f\"[SAVE] 纯文本（含原始/化简/小diff）已写入: {expr_txt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "对每个因子分别做简单线性回归，评估它与目标变量（训练集收益）之间的拟合程度"
      ],
      "metadata": {
        "id": "LhP84-0T_338"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOkXc7I2zIma"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 步骤 6/6：单因子线性回归筛选\n",
        "\n",
        "# -----------------------------\n",
        "banner(\"预处理步骤：单因子线性回归筛选\")\n",
        "t0 = time.time()\n",
        "\n",
        "# 导入所需库\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# 存储线性回归结果\n",
        "linear_reg_results = []\n",
        "\n",
        "# 手动实现简单线性回归（避免依赖sklearn.LinearRegression）\n",
        "def simple_linear_regression(X, y):\n",
        "    \"\"\"简单实现线性回归，返回系数和截距\"\"\"\n",
        "    X = np.asarray(X).reshape(-1)\n",
        "    y = np.asarray(y).reshape(-1)\n",
        "\n",
        "    # 确保数据有效\n",
        "    valid = np.isfinite(X) & np.isfinite(y)\n",
        "    X, y = X[valid], y[valid]\n",
        "\n",
        "    if len(X) < 5:  # 至少需要一些样本\n",
        "        return 0, 0\n",
        "\n",
        "    # 计算相关系数\n",
        "    mean_x, mean_y = np.mean(X), np.mean(y)\n",
        "    cov_xy = np.sum((X - mean_x) * (y - mean_y))\n",
        "    var_x = np.sum((X - mean_x) ** 2)\n",
        "\n",
        "    if var_x == 0:\n",
        "        return 0, mean_y  # 如果X无方差，返回y的均值\n",
        "\n",
        "    # 计算斜率和截距\n",
        "    slope = cov_xy / var_x\n",
        "    intercept = mean_y - slope * mean_x\n",
        "\n",
        "    return slope, intercept\n",
        "\n",
        "# 遍历所有因子进行单因子线性回归\n",
        "for factor in tqdm(colnames_gp, desc=\"单因子线性回归\"):\n",
        "    try:\n",
        "        # 提取单个因子作为特征\n",
        "        X = df_gp_tr[factor].values\n",
        "        y = y_ic_tr\n",
        "\n",
        "        # 跳过含有NaN/Inf的因子\n",
        "        if not np.all(np.isfinite(X)):\n",
        "            print(f\"跳过 {factor}：包含NaN或Inf值\")\n",
        "            continue\n",
        "\n",
        "        # 使用自定义的简单线性回归\n",
        "        coef, intercept = simple_linear_regression(X, y)\n",
        "\n",
        "        # 计算预测值\n",
        "        y_pred = coef * X + intercept\n",
        "\n",
        "        # 计算评估指标\n",
        "        r2 = r2_score(y, y_pred)\n",
        "        mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "        # 保存结果\n",
        "        linear_reg_results.append({\n",
        "            \"factor\": factor,\n",
        "            \"r2\": r2,\n",
        "            \"mse\": mse,\n",
        "            \"coef\": coef,\n",
        "            \"abs_coef\": abs(coef)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"处理因子 {factor} 时出错: {e}\")\n",
        "\n",
        "# 转换为DataFrame并排序\n",
        "if linear_reg_results:\n",
        "    linear_reg_df = pd.DataFrame(linear_reg_results)\n",
        "\n",
        "    # 按R²降序排序（更高表示更好的拟合）\n",
        "    linear_reg_df = linear_reg_df.sort_values(\"r2\", ascending=False)\n",
        "\n",
        "    # 保存结果\n",
        "    linear_reg_path = os.path.join(EVAL_DIR, \"single_factor_linear_regression.csv\")\n",
        "    save_csv(linear_reg_df, linear_reg_path, name=\"单因子线性回归结果\")\n",
        "\n",
        "    # 可选：基于线性回归结果选择Top N个因子\n",
        "    TOP_N = min(50, len(linear_reg_df))  # 可调整数量\n",
        "    top_factors = linear_reg_df.head(TOP_N)[\"factor\"].tolist()\n",
        "\n",
        "    print(f\"\\n线性回归Top {TOP_N} 因子:\")\n",
        "    for i, (_, row) in enumerate(linear_reg_df.head(TOP_N).iterrows()):\n",
        "        print(f\"{i+1}. {row['factor']} - R²: {row['r2']:.4f}, MSE: {row['mse']:.6f}, 系数: {row['coef']:.6f}\")\n",
        "\n",
        "    # 可选：创建一个新的变量来存储线性回归筛选的因子\n",
        "    LINEAR_REG_SELECTED = top_factors\n",
        "\n",
        "    # 保存筛选的因子列表\n",
        "    linear_selected_path = os.path.join(EVAL_DIR, \"linear_regression_selected_factors.json\")\n",
        "    with open(linear_selected_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"selected_factors\": LINEAR_REG_SELECTED}, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"线性回归筛选的因子已保存到: {linear_selected_path}\")\n",
        "else:\n",
        "    print(\"没有得到有效的线性回归结果\")\n",
        "    LINEAR_REG_SELECTED = []\n",
        "\n",
        "print(f\"单因子线性回归筛选完成，用时: {time.time()-t0:.2f}s\")\n",
        "\n",
        "\n",
        "# colnames_gp = LINEAR_REG_SELECTED\n",
        "print(f\"继续使用原始的 {len(colnames_gp)} 个因子进行后续分析\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPsyolp6pFDW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# -----------------------------\n",
        "banner(\"入库清单与摘要报告\")\n",
        "t0 = time.time()\n",
        "\n",
        "registry_path = os.path.join(BASE_DIR, \"factor_registry.json\")\n",
        "registry = {\"version\": 1, \"criteria\": {\n",
        "    \"selection_set\": \"validation\",\n",
        "    \"metric\": \"|mean_daily_IC|\",     # 说明：排序采用 val_abs_mean_ic\n",
        "    \"corr_threshold\": CORR_TH,\n",
        "    \"quantiles\": Q,\n",
        "}}\n",
        "\n",
        "# 便于合并：转成 dict\n",
        "val_ic_map = val_ic_df.set_index(\"factor\").to_dict(orient=\"index\")\n",
        "qr_map     = qr_df.set_index(\"factor\").to_dict(orient=\"index\")\n",
        "\n",
        "registry[\"factors\"] = []\n",
        "for f in KEEP:\n",
        "    item = {\"name\": f}\n",
        "    # 评估指标\n",
        "    item[\"metrics\"]  = val_ic_map.get(f, {})\n",
        "    # 分组收益（含 signed_spread 与 direction）\n",
        "    item[\"quantile\"] = qr_map.get(f, {})\n",
        "    # 表达式（若有）\n",
        "    rec = expr_df[expr_df[\"factor\"] == f]\n",
        "    if not rec.empty:\n",
        "        item[\"expr\"] = rec.iloc[0][\"expr\"]\n",
        "        item[\"expr_simplified\"] = rec.iloc[0][\"expr_simplified\"]\n",
        "        item[\"direction\"] = int(rec.iloc[0][\"direction\"])\n",
        "    # 统一附上当前阈值\n",
        "    item[\"corr_threshold_used\"] = CORR_TH\n",
        "    registry[\"factors\"].append(item)\n",
        "\n",
        "with open(registry_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(registry, f, ensure_ascii=False, indent=2)\n",
        "print(\" 入库清单已更新：\", registry_path)\n",
        "\n",
        "md_path = os.path.join(EVAL_DIR, \"evaluation_summary.md\")\n",
        "with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# 因子评估与入库摘要（Validation 驱动）\\n\\n\")\n",
        "    f.write(f\"- 候选因子数：{len(colnames_gp)}\\n\")\n",
        "    f.write(f\"- 剪枝后保留：{len(KEEP)}（相关性阈值 {CORR_TH}）\\n\")\n",
        "    f.write(f\"- 单因子IC（Val）：{os.path.basename(os.path.join(EVAL_DIR, 'single_factor_ic_val.csv'))}\\n\")\n",
        "    f.write(f\"- 单因子IC（Train）：{os.path.basename(os.path.join(EVAL_DIR, 'single_factor_ic_train.csv'))}\\n\")\n",
        "    f.write(f\"- 分组收益（含方向）：{os.path.basename(qr_path)}\\n\")\n",
        "    f.write(f\"- 相关矩阵：{os.path.basename(corr_path)}\\n\")\n",
        "    f.write(f\"- 表达式表：{os.path.basename(expr_csv)}\\n\")\n",
        "    f.write(f\"- 入库清单：{os.path.basename(registry_path)}\\n\")\n",
        "print(\" 摘要报告已生成：\", md_path)\n",
        "print(f\" 用时：{time.time()-t0:.2f}s\")\n",
        "\n",
        "print(\"\\n 全流程完成。\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用剪枝后的因子（KEEP）作为特征，构建 LightGBM 三分类模型，对上涨/下跌/不变进行回测评估。流程包括标签构建、模型训练、验证集评估、特征重要性分析等。\n",
        "\n"
      ],
      "metadata": {
        "id": "pDFVp9o8_9Pz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaHSRAEtzIma"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 步骤 7/6：LightGBM 回测（三分类）- 修复版\n",
        "# -----------------------------\n",
        "banner(\"步骤 7/6：LightGBM 回测（三分类）- 修复版\")\n",
        "t0 = time.time()\n",
        "\n",
        "# 设置 alpha 阈值用于构建三分类标签\n",
        "ALPHA_THRESHOLD = 5e-5\n",
        "\n",
        "# 1. 准备特征（使用保留的因子）\n",
        "print(f\"使用 {len(KEEP)} 个保留因子作为特征\")\n",
        "\n",
        "# 创建训练特征矩阵，只使用保留的因子\n",
        "X_train_filtered = df_gp_tr[KEEP].copy()\n",
        "X_val_filtered = df_gp_val[KEEP].copy()\n",
        "\n",
        "# 2. 准备标签（三分类：上涨、下跌、不变）\n",
        "def create_labels(returns, alpha=ALPHA_THRESHOLD):\n",
        "    \"\"\"根据阈值创建三分类标签：-1（下跌）、0（不变）、1（上涨）\"\"\"\n",
        "    labels = np.zeros_like(returns)\n",
        "    labels[returns > alpha] = 1    # 上涨\n",
        "    labels[returns < -alpha] = -1  # 下跌\n",
        "    return labels\n",
        "\n",
        "# 创建训练和验证标签\n",
        "y_train = create_labels(y_ic_tr)\n",
        "y_val = create_labels(y_ic_val)\n",
        "\n",
        "# 统计标签分布\n",
        "train_label_dist = pd.Series(y_train).value_counts(normalize=True)\n",
        "val_label_dist = pd.Series(y_val).value_counts(normalize=True)\n",
        "print(\"训练集标签分布:\")\n",
        "print(train_label_dist)\n",
        "print(\"\\n验证集标签分布:\")\n",
        "print(val_label_dist)\n",
        "\n",
        "# 3. 训练 LightGBM 模型\n",
        "print(\"\\n开始训练 LightGBM 模型...\")\n",
        "\n",
        "# 将标签转换为非负整数（LightGBM 要求）\n",
        "# 映射: -1→0(下跌), 0→1(不变), 1→2(上涨)\n",
        "y_train_lgb = y_train + 1\n",
        "y_val_lgb = y_val + 1\n",
        "\n",
        "# 转换为 LightGBM 数据集格式\n",
        "train_data = lgb.Dataset(X_train_filtered, label=y_train_lgb)\n",
        "val_data = lgb.Dataset(X_val_filtered, label=y_val_lgb, reference=train_data)\n",
        "\n",
        "# 设置参数（三分类）\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 3,  # 对应类别: 0(下跌), 1(不变), 2(上涨)\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# 修复训练部分，使用兼容的 callbacks 参数\n",
        "try:\n",
        "    # 较新版本的 LightGBM\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(20, verbose=True),\n",
        "        lgb.log_evaluation(20)\n",
        "    ]\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "except (TypeError, AttributeError):\n",
        "    # 旧版本的 LightGBM\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        verbose_eval=20\n",
        "    )\n",
        "\n",
        "# 4. 评估模型（验证集）\n",
        "print(\"\\n在验证集上评估模型...\")\n",
        "\n",
        "# 预测\n",
        "probs = model.predict(X_val_filtered, num_iteration=model.best_iteration)\n",
        "y_pred_lgb = np.argmax(probs, axis=1)\n",
        "y_pred = y_pred_lgb - 1  # 将 [0,1,2] 映射回 [-1,0,1]\n",
        "\n",
        "# 计算准确率\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"验证集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 自定义分类报告函数（避免使用有问题的 sklearn.metrics.classification_report）\n",
        "def custom_classification_report(y_true, y_pred, target_names=None):\n",
        "    \"\"\"创建类似 sklearn 的分类报告，但不依赖其实现\"\"\"\n",
        "    from collections import Counter\n",
        "    import numpy as np\n",
        "\n",
        "    # 获取唯一标签\n",
        "    labels = sorted(set(np.concatenate([np.unique(y_true), np.unique(y_pred)])))\n",
        "    if target_names is None:\n",
        "        target_names = [str(l) for l in labels]\n",
        "\n",
        "    # 计算每个类别的指标\n",
        "    report_dict = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        # 真阳性、假阳性、假阴性\n",
        "        tp = np.sum((y_true == label) & (y_pred == label))\n",
        "        fp = np.sum((y_true != label) & (y_pred == label))\n",
        "        fn = np.sum((y_true == label) & (y_pred != label))\n",
        "        support = np.sum(y_true == label)\n",
        "\n",
        "        # 计算精确率、召回率、F1分数\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        report_dict[i] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1-score': f1,\n",
        "            'support': support\n",
        "        }\n",
        "\n",
        "    # 计算加权平均\n",
        "    total = len(y_true)\n",
        "    weighted_avg = {\n",
        "        'precision': sum(report_dict[i]['precision'] * report_dict[i]['support'] for i in range(len(labels))) / total,\n",
        "        'recall': sum(report_dict[i]['recall'] * report_dict[i]['support'] for i in range(len(labels))) / total,\n",
        "        'f1-score': sum(report_dict[i]['f1-score'] * report_dict[i]['support'] for i in range(len(labels))) / total,\n",
        "        'support': total\n",
        "    }\n",
        "    report_dict['weighted avg'] = weighted_avg\n",
        "\n",
        "    # 格式化输出\n",
        "    result = \"              precision    recall  f1-score   support\\n\\n\"\n",
        "    for i, name in enumerate(target_names):\n",
        "        metrics = report_dict[i]\n",
        "        result += f\"{name:14} {metrics['precision']:.2f}      {metrics['recall']:.2f}      {metrics['f1-score']:.2f}     {metrics['support']}\\n\"\n",
        "\n",
        "    result += \"\\n\"\n",
        "    metrics = report_dict['weighted avg']\n",
        "    result += f\"{'weighted avg':14} {metrics['precision']:.2f}      {metrics['recall']:.2f}      {metrics['f1-score']:.2f}     {metrics['support']}\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# 使用自定义函数打印分类报告\n",
        "print(\"\\n分类报告:\")\n",
        "print(custom_classification_report(y_val, y_pred, target_names=['下跌', '不变', '上涨']))\n",
        "\n",
        "# 混淆矩阵\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "print(\"\\n混淆矩阵:\")\n",
        "print(cm)\n",
        "\n",
        "# 5. 特征重要性分析\n",
        "importance = model.feature_importance(importance_type='gain')\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': KEEP,\n",
        "    'Importance': importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\n特征重要性 Top 10:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# 保存特征重要性\n",
        "importance_path = os.path.join(EVAL_DIR, \"feature_importance_lightgbm.csv\")\n",
        "feature_importance.to_csv(importance_path, index=False)\n",
        "print(f\"特征重要性已保存到: {importance_path}\")\n",
        "print(f\"LightGBM回测完成，用时: {time.time()-t0:.2f}s\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}