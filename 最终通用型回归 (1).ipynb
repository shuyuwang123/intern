{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# === 一次性：下载中文字体 → 注册 → 配置 Matplotlib → 测试中文 ===\n",
        "import os, sys, time, urllib.request\n",
        "import matplotlib as mpl\n",
        "from matplotlib import font_manager\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  备选字体下载地址（按优先级尝试）\n",
        "CANDIDATES = [\n",
        "    # Noto Sans CJK SC\n",
        "    (\"NotoSansCJKsc-Regular.otf\",\n",
        "     \"https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf\"),\n",
        "    (\"NotoSansCJKsc-Regular.otf\",\n",
        "     \"https://raw.githubusercontent.com/googlefonts/noto-cjk/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf\"),\n",
        "    # Source Han Sans SC (思源黑体)\n",
        "    (\"SourceHanSansSC-Regular.otf\",\n",
        "     \"https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Regular.otf\"),\n",
        "]\n",
        "\n",
        "def try_download(dest, url, timeout=25):\n",
        "    try:\n",
        "        print(f\"↘︎ 下载: {url}\")\n",
        "        urllib.request.urlretrieve(url, dest)\n",
        "        # 简单验收：大小>100KB\n",
        "        if os.path.getsize(dest) < 100 * 1024:\n",
        "            raise RuntimeError(\"下载文件异常（过小）\")\n",
        "        print(f\"已保存: {dest}  ({os.path.getsize(dest)/1024:.0f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"下载失败: {e}\")\n",
        "        if os.path.exists(dest):\n",
        "            try: os.remove(dest)\n",
        "            except: pass\n",
        "        return False\n",
        "\n",
        "# 2) 若本地已存在可用字体，直接用；否则逐个下载尝试\n",
        "font_path = None\n",
        "for fname, url in CANDIDATES:\n",
        "    if os.path.exists(fname):\n",
        "        font_path = fname\n",
        "        print(f\"已有字体文件: {fname}\")\n",
        "        break\n",
        "else:\n",
        "    for fname, url in CANDIDATES:\n",
        "        if try_download(fname, url):\n",
        "            font_path = fname\n",
        "            break\n",
        "\n",
        "if not font_path:\n",
        "    raise RuntimeError(\" 无法下载中文字体。请检查网络/代理，或手动上传任一 OTF/TTF 后重试。\")\n",
        "\n",
        "# 3) 注册字体并设为默认 sans-serif\n",
        "font_manager.fontManager.addfont(font_path)\n",
        "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
        "\n",
        "mpl.rcParams[\"font.sans-serif\"] = [font_name]\n",
        "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
        "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
        "mpl.rcParams[\"ps.fonttype\"]  = 42\n",
        "mpl.rcParams[\"svg.fonttype\"] = \"none\"\n",
        "\n",
        "print(\" 已启用中文字体：\", font_name)\n",
        "\n",
        "# 4) 画一张测试图\n",
        "plt.figure()\n",
        "plt.title(\"中文标题：收益率α 与 β\")\n",
        "plt.xlabel(\"时间（日）\")\n",
        "plt.ylabel(\"收益(%)\")\n",
        "plt.plot([0,1,2,3],[0,1,-1,2], marker=\"o\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "99mxFbtA69i4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "96f2c85b-df80-43f8-9f8d-ebf9f16da56a"
      },
      "id": "99mxFbtA69i4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↘︎ 下载: https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf\n",
            "已保存: NotoSansCJKsc-Regular.otf  (16052 KB)\n",
            " 已启用中文字体： Noto Sans CJK SC\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bfaf2a04-1c8d-4e78-a2b2-a8d91fb3c5e1",
      "metadata": {
        "id": "bfaf2a04-1c8d-4e78-a2b2-a8d91fb3c5e1",
        "outputId": "6a412ff2-8bd9-4bb4-8899-8015561d7388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "中文字体文件 'NotoSansSC-SemiBold.ttf' 未找到，将使用默认字体。\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "from typing import List, Union, Tuple\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# 添加中文字体支持\n",
        "try:\n",
        "    fm.fontManager.addfont('NotoSansSC-SemiBold.ttf')\n",
        "    font_name = fm.FontProperties(fname='NotoSansSC-SemiBold.ttf').get_name()\n",
        "    mpl.rc('font', family=font_name)\n",
        "    print(\"font family: \", plt.rcParams['font.family'])\n",
        "except FileNotFoundError:\n",
        "    print(\"中文字体文件 'NotoSansSC-SemiBold.ttf' 未找到，将使用默认字体。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "621312f1-eb21-4403-8b03-9f0dffe4c789",
      "metadata": {
        "id": "621312f1-eb21-4403-8b03-9f0dffe4c789"
      },
      "outputs": [],
      "source": [
        "# 全局参数设置\n",
        "# 数据日期范围参数\n",
        "START_DATE = '2024-10-01'  # 数据开始日期\n",
        "END_DATE = '2024-11-29'    # 数据结束日期\n",
        "\n",
        "# 期货品种设置\n",
        "SYMBOL_TYPES = ['寒武纪']  # 可以添加其他合约类型如 'IC', 'IH'\n",
        "\n",
        "# --- 回归任务参数 ---\n",
        "# 数据处理参数\n",
        "LOOKBACK_WINDOW = 5        # 用于计算标准化参数的历史天数\n",
        "TIMESTAMP_PER_SAMPLE = 8 # 每个样本的时间步长\n",
        "K = 4                      # 用于计算未来价格走势的时间步长 (y[t] = (Bid1[t+k] - Ask1[t]) / Ask1[t])\n",
        "RAW_N = 20                 # 前20列为原始、未标准化的订单簿特征 (5 levels * 2 sides * 2 (price+volume))\n",
        "\n",
        "freq: str = \"3000ms\"   # 改这里即可：支持 \"100ms\" / \"300ms\" / \"500ms\" / \"1s\" 等\n",
        "# 数据缓存路径\n",
        "DATA_CACHE_DIR = \"./data_cache\"\n",
        "os.makedirs(DATA_CACHE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049c275f-1933-4c10-a322-807c4d4a9a02",
      "metadata": {
        "id": "049c275f-1933-4c10-a322-807c4d4a9a02"
      },
      "outputs": [],
      "source": [
        "# --- 挂载 & 导入 ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from typing import Union, Tuple, List\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def get_processed_data(\n",
        "    start_date: Union[str, np.datetime64, pd.Timestamp],\n",
        "    end_date:   Union[str, np.datetime64, pd.Timestamp] = None,\n",
        "    symbol_types: List[str] = SYMBOL_TYPES,\n",
        "    time_range: Tuple[str, str] = None,\n",
        "    parquet_path: str = '/content/drive/MyDrive/2024_256.parquet',\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    从本地 Parquet 读取 -> 按日期/标的/交易时段筛选\n",
        "    -> 对每个交易日/交易时段按**全局 freq**强制对齐（重采样，backward 填充）。\n",
        "    \"\"\"\n",
        "    # 1) 读文件\n",
        "    try:\n",
        "        print(f\"正在从 '{parquet_path}' 加载数据...\")\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "        print(f\"数据加载成功，共 {len(df)} 条记录。\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：未找到文件 '{parquet_path}'。请确保文件路径正确。\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2) 处理日期参数\n",
        "    if end_date is None:\n",
        "        end_date = start_date\n",
        "    start_date_for_filter = pd.to_datetime(start_date).strftime('%Y%m%d')\n",
        "    end_date_for_filter   = pd.to_datetime(end_date).strftime('%Y%m%d')\n",
        "    print(f\"将使用筛选日期范围: {start_date_for_filter} 到 {end_date_for_filter}\")\n",
        "\n",
        "    # 3) 基础字段规范和时间戳转换\n",
        "    df['MDDate'] = df['MDDate'].astype(str)\n",
        "    df['MDTime'] = (\n",
        "        df['MDTime'].astype(str)\n",
        "        .str.replace(r'\\D', '', regex=True)\n",
        "        .str.zfill(9)        # HHMMSSfff\n",
        "        .str[:9]\n",
        "    )\n",
        "    df['Symbol'] = df['Symbol'].astype(str)\n",
        "\n",
        "    print(\"正在高效转换时间戳...\")\n",
        "    df['Timestamp'] = pd.to_datetime(\n",
        "        df['MDDate'] + ' ' + df['MDTime'],\n",
        "        format='%Y%m%d %H%M%S%f',\n",
        "        errors='coerce'\n",
        "    )\n",
        "    df.dropna(subset=['Timestamp'], inplace=True)\n",
        "    df.sort_values('Timestamp', inplace=True)\n",
        "    print(\"时间戳转换完成。\")\n",
        "\n",
        "    # 4) 日期与标的筛选\n",
        "    date_mask   = df['MDDate'].between(start_date_for_filter, end_date_for_filter)\n",
        "    symbol_mask = df['Symbol'].str.strip().str.startswith(tuple(symbol_types))\n",
        "    result_df   = df[date_mask & symbol_mask].copy()\n",
        "    if result_df.empty:\n",
        "        print(f\"在 {start_date_for_filter} 到 {end_date_for_filter} 范围内未找到合约类型为 {symbol_types} 的数据。\")\n",
        "        return pd.DataFrame()\n",
        "    print(f\"初步筛选后，找到 {len(result_df)} 条记录。\")\n",
        "\n",
        "    # 5) 交易时段（可被 time_range 覆盖）\n",
        "    sessions = [\n",
        "        (pd.to_datetime(\"09:30:00\").time(), pd.to_datetime(\"11:30:00\").time()),\n",
        "        (pd.to_datetime(\"13:00:00\").time(), pd.to_datetime(\"15:00:00\").time()),\n",
        "    ]\n",
        "    if time_range is not None:\n",
        "        sessions = [(pd.to_datetime(time_range[0]).time(),\n",
        "                     pd.to_datetime(time_range[1]).time())]\n",
        "\n",
        "    # 6) 必做重采样：按全局 freq 对齐 + backward 填充\n",
        "    out_parts = []\n",
        "    for day, df_day in result_df.groupby(result_df['Timestamp'].dt.date):\n",
        "        for (t_start, t_end) in sessions:\n",
        "            mask = ((df_day['Timestamp'].dt.time >= t_start) &\n",
        "                    (df_day['Timestamp'].dt.time <= t_end))\n",
        "            seg = (\n",
        "                df_day.loc[mask]\n",
        "                .drop_duplicates(subset=['Timestamp'], keep='last')\n",
        "                .sort_values('Timestamp')\n",
        "            )\n",
        "            if seg.empty:\n",
        "                continue\n",
        "\n",
        "            g_start, g_end = seg['Timestamp'].iloc[0], seg['Timestamp'].iloc[-1]\n",
        "            grid = pd.DataFrame({'Timestamp': pd.date_range(start=g_start, end=g_end, freq=freq)})\n",
        "\n",
        "            aligned = pd.merge_asof(\n",
        "                grid, seg, on='Timestamp',\n",
        "                direction='backward', allow_exact_matches=True\n",
        "            )\n",
        "            out_parts.append(aligned)\n",
        "\n",
        "    if not out_parts:\n",
        "        print(\"所有所选时段内均无可对齐数据，返回空表。\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    result_df = (\n",
        "        pd.concat(out_parts, ignore_index=True)\n",
        "          .sort_values('Timestamp')\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    print(f\"[resample] 已按全局 freq='{freq}' 重采样完成，输出 {len(result_df)} 行。\")\n",
        "\n",
        "    # 7) 重命名与列选择\n",
        "    result_df['Trade_Date'] = result_df['Timestamp'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    rename_map = {\n",
        "        'Buy1Price': 'BidPrice1',     'Buy1OrderQty': 'BidVolume1',\n",
        "        'Sell1Price': 'AskPrice1',    'Sell1OrderQty': 'AskVolume1',\n",
        "        'Buy2Price': 'BidPrice2',     'Buy2OrderQty': 'BidVolume2',\n",
        "        'Sell2Price': 'AskPrice2',    'Sell2OrderQty': 'AskVolume2',\n",
        "        'Buy3Price': 'BidPrice3',     'Buy3OrderQty': 'BidVolume3',\n",
        "        'Sell3Price': 'AskPrice3',    'Sell3OrderQty': 'AskVolume3',\n",
        "        'Buy4Price': 'BidPrice4',     'Buy4OrderQty': 'BidVolume4',\n",
        "        'Sell4Price': 'AskPrice4',    'Sell4OrderQty': 'AskVolume4',\n",
        "        'Buy5Price': 'BidPrice5',     'Buy5OrderQty': 'BidVolume5',\n",
        "        'Sell5Price': 'AskPrice5',    'Sell5OrderQty': 'AskVolume5',\n",
        "        'LastPrice': 'LastPrice'\n",
        "    }\n",
        "\n",
        "    # # 若没有 LastPrice，则尝试用一档中价代替\n",
        "    # if 'LastPrice' not in result_df.columns:\n",
        "    #     if {'Buy1Price', 'Sell1Price'}.issubset(result_df.columns):\n",
        "    #         result_df['LastPrice'] = (result_df['Buy1Price'] + result_df['Sell1Price']) / 2\n",
        "    #     elif {'BidPrice1', 'AskPrice1'}.issubset(result_df.columns):\n",
        "    #         result_df['LastPrice'] = (result_df['BidPrice1'] + result_df['AskPrice1']) / 2\n",
        "    #     else:\n",
        "    #         print(\"警告: 缺少一档买卖价，无法构造 LastPrice。\")\n",
        "\n",
        "    # 仅重命名存在的列\n",
        "    rename_submap = {k: v for k, v in rename_map.items() if k in result_df.columns}\n",
        "    result_df.rename(columns=rename_submap, inplace=True)\n",
        "\n",
        "    required_columns = list(rename_map.values()) + ['Symbol', 'Timestamp', 'Trade_Date']\n",
        "    final_cols = [col for col in required_columns if col in result_df.columns]\n",
        "\n",
        "    return result_df[final_cols]\n",
        "\n",
        "\n",
        "# --- 2. 标签生成 ---\n",
        "def generate_multitask_labels(k: int, daily_data_dict: dict):\n",
        "    \"\"\"\n",
        "    为多任务学习生成一组标签。\n",
        "    【最终版】移除了不稳定的 y_lambda 任务，并全面提升了计算的稳健性。\n",
        "    \"\"\"\n",
        "    assert k > 0, \"k 必须为正整数\"\n",
        "    daily_y_dict = {}\n",
        "    # 从标签名称列表中移除 y_lambda\n",
        "    label_names = ['y_reg', 'y_rv', 'y_fillprob', 'y_roll', 'y_vpin', 'y_direction', 'y_upper_bound', 'y_lower_bound']\n",
        "\n",
        "    for date, df in daily_data_dict.items():\n",
        "        n = len(df)\n",
        "        if n <= k:\n",
        "            daily_y_dict[date] = pd.DataFrame(columns=label_names, dtype=np.float32)\n",
        "            continue\n",
        "\n",
        "        labels = pd.DataFrame(index=df.index, dtype=np.float32)\n",
        "\n",
        "        mid_price = (df['AskPrice1'] + df['BidPrice1']) / 2\n",
        "        log_mid_price_ret = np.log(mid_price).diff().fillna(0)\n",
        "\n",
        "        entry_price = df['AskPrice1'].values\n",
        "        exit_price = df['BidPrice1'].shift(-k).values\n",
        "        y_reg = (exit_price - entry_price) / (entry_price + 1e-12)\n",
        "        labels['y_reg'] = y_reg * 10000.0\n",
        "\n",
        "        rv_raw = log_mid_price_ret.pow(2).rolling(window=k).sum().shift(-k)\n",
        "        labels['y_rv'] = np.sqrt(np.maximum(rv_raw, 0)) * 10000.0\n",
        "\n",
        "        future_min_ask = df['AskPrice1'].rolling(window=k).min().shift(-k)\n",
        "        labels['y_fillprob'] = (future_min_ask <= df['BidPrice1']).astype(float)\n",
        "\n",
        "\n",
        "        cov = log_mid_price_ret.rolling(window=k).cov(log_mid_price_ret.shift(1)).shift(-k)\n",
        "        roll_spread_raw = 2 * np.sqrt(np.maximum(-cov, 0))\n",
        "        labels['y_roll'] = roll_spread_raw * 10000.0\n",
        "\n",
        "        buy_vol_diff = df['BidVolume1'].diff().fillna(0).clip(lower=0)\n",
        "        sell_vol_diff = df['AskVolume1'].diff().fillna(0).clip(lower=0)\n",
        "        total_volume = (buy_vol_diff + sell_vol_diff).sum()\n",
        "        bucket_size = total_volume / 50 if total_volume > 0 else 1000\n",
        "\n",
        "        vpin_values = []\n",
        "        cum_vol = 0; buy_vol_bucket = 0; sell_vol_bucket = 0\n",
        "        start_idx = 0\n",
        "        for i in range(n):\n",
        "            b_vol, s_vol = buy_vol_diff.iloc[i], sell_vol_diff.iloc[i]\n",
        "            cum_vol += b_vol + s_vol\n",
        "            buy_vol_bucket += b_vol\n",
        "            sell_vol_bucket += s_vol\n",
        "            if cum_vol >= bucket_size and cum_vol > 0:\n",
        "                vpin = np.abs(buy_vol_bucket - sell_vol_bucket) / cum_vol\n",
        "                vpin_values.extend([vpin] * (i - start_idx + 1))\n",
        "                start_idx = i + 1\n",
        "                cum_vol = 0; buy_vol_bucket = 0; sell_vol_bucket = 0\n",
        "\n",
        "        if start_idx < n:\n",
        "            last_vpin = vpin_values[-1] if vpin_values else 0\n",
        "            vpin_values.extend([last_vpin] * (n - start_idx))\n",
        "\n",
        "        labels['y_vpin'] = pd.Series(vpin_values, index=df.index, dtype=np.float32).shift(-k)\n",
        "\n",
        "        # --- 计算新的辅助标签 ---\n",
        "        # 1. y_direction (分类任务: 0=下跌, 1=不变, 2=上涨)\n",
        "        future_mid_price = mid_price.shift(-k)\n",
        "        price_change = future_mid_price - mid_price\n",
        "        labels['y_direction'] = 1 # 默认为不变\n",
        "        labels.loc[price_change > 1e-6, 'y_direction'] = 2 # 上涨\n",
        "        labels.loc[price_change < -1e-6, 'y_direction'] = 0 # 下跌\n",
        "\n",
        "        # 2. y_upper_bound 和 y_lower_bound (回归任务)\n",
        "        future_max_mid = mid_price.rolling(window=k).max().shift(-k)\n",
        "        future_min_mid = mid_price.rolling(window=k).min().shift(-k)\n",
        "\n",
        "        # 将其表示为相对于当前价格的变化率 (BPS)\n",
        "        labels['y_upper_bound'] = (future_max_mid - mid_price) / (mid_price + 1e-12) * 10000.0\n",
        "        labels['y_lower_bound'] = (future_min_mid - mid_price) / (mid_price + 1e-12) * 10000.0\n",
        "\n",
        "        labels.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        labels.ffill(inplace=True)\n",
        "        labels.bfill(inplace=True)\n",
        "        labels.fillna(0, inplace=True)\n",
        "\n",
        "        for col in labels.columns:\n",
        "            if col == 'y_fillprob': continue\n",
        "            mean, std = labels[col].mean(), labels[col].std()\n",
        "            if std > 0:\n",
        "                lower_bound, upper_bound = mean - 5 * std, mean + 5 * std\n",
        "                labels[col] = labels[col].clip(lower_bound, upper_bound)\n",
        "\n",
        "        daily_y_dict[date] = labels.values.astype(np.float32)\n",
        "\n",
        "    return daily_y_dict, label_names\n",
        "\n",
        "\n",
        "# --- 3. 特征生成  ---\n",
        "def prepare_orderbook_data(data_df):\n",
        "    \"\"\"\n",
        "    【增强版】在原有特征基础上，增加了高阶动态、多时间尺度、微观结构比率等高级特征。\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    # --- 基础订单簿特征 (不变) ---\n",
        "    for i in range(1, 6): features.append(data_df[f'AskPrice{i}'].values)\n",
        "    for i in range(1, 6): features.append(data_df[f'AskVolume{i}'].values)\n",
        "    for i in range(1, 6): features.append(data_df[f'BidPrice{i}'].values)\n",
        "    for i in range(1, 6): features.append(data_df[f'BidVolume{i}'].values)\n",
        "\n",
        "    # --- 基础衍生特征 (不变) ---\n",
        "    for i in range(1, 5): features.append(data_df[f'AskPrice{i+1}'].values - data_df[f'AskPrice{i}'].values)\n",
        "    for i in range(1, 5): features.append(data_df[f'BidPrice{i}'].values - data_df[f'BidPrice{i+1}'].values)\n",
        "    for i in range(1, 6): features.append((data_df[f'AskPrice{i}'].values + data_df[f'BidPrice{i}'].values) / 2)\n",
        "    for i in range(1, 6): features.append(data_df[f'AskPrice{i}'].values - data_df[f'BidPrice{i}'].values)\n",
        "    ask_cum_vol = np.zeros(len(data_df)); bid_cum_vol = np.zeros(len(data_df))\n",
        "    for i in range(1, 6):\n",
        "        ask_cum_vol += data_df[f'AskVolume{i}'].values; bid_cum_vol += data_df[f'BidVolume{i}'].values\n",
        "        features.append(ask_cum_vol.copy()); features.append(bid_cum_vol.copy())\n",
        "    features.append(bid_cum_vol / (ask_cum_vol + bid_cum_vol + 1e-12))\n",
        "\n",
        "    mid_price_l1 = (data_df['BidPrice1'] + data_df['AskPrice1']) / 2\n",
        "    returns = mid_price_l1.pct_change().fillna(0)\n",
        "    price_change = mid_price_l1.diff().fillna(0)\n",
        "    for window in [5, 10, 20]:\n",
        "        features.append(price_change.rolling(window=window, min_periods=1).sum().fillna(0).values)\n",
        "    for window in [10, 30, 60]:\n",
        "        features.append(returns.rolling(window=window, min_periods=1).std().fillna(0).values)\n",
        "\n",
        "    prev_bid_price = data_df['BidPrice1'].shift(1).bfill()\n",
        "    prev_ask_price = data_df['AskPrice1'].shift(1).bfill()\n",
        "    delta_bid_vol = data_df['BidVolume1'].diff().fillna(0)\n",
        "    delta_ask_vol = data_df['AskVolume1'].diff().fillna(0)\n",
        "    ofi_raw = np.zeros_like(mid_price_l1.values, dtype=float)\n",
        "    ofi_raw[data_df['BidPrice1'] > prev_bid_price] = data_df.loc[data_df['BidPrice1'] > prev_bid_price, 'BidVolume1']\n",
        "    ofi_raw[data_df['AskPrice1'] < prev_ask_price] = -data_df.loc[data_df['AskPrice1'] < prev_ask_price, 'AskVolume1']\n",
        "    is_price_stable = (data_df['BidPrice1'] == prev_bid_price) & (data_df['AskPrice1'] == prev_ask_price)\n",
        "    ofi_raw[is_price_stable] = (delta_bid_vol - delta_ask_vol)[is_price_stable]\n",
        "    features.append(ofi_raw)\n",
        "    for window in [10, 30, 60]:\n",
        "        features.append(pd.Series(ofi_raw).rolling(window=window, min_periods=1).sum().fillna(0).values)\n",
        "\n",
        "    seconds_in_day = (data_df['Timestamp'] - data_df['Timestamp'].dt.normalize()).dt.total_seconds()\n",
        "    trading_day_seconds = 14400\n",
        "    features.append(np.sin(2 * np.pi * seconds_in_day / trading_day_seconds))\n",
        "    features.append(np.cos(2 * np.pi * seconds_in_day / trading_day_seconds))\n",
        "    features.append((data_df['Timestamp'].dt.hour < 12).astype(int).values)\n",
        "    features.append((data_df['Timestamp'].dt.hour >= 13).astype(int).values)\n",
        "\n",
        "    for i in range(1, 6): features.append(data_df[f'BidPrice{i}'].diff().fillna(0).values)\n",
        "    for i in range(1, 6): features.append(data_df[f'AskPrice{i}'].diff().fillna(0).values)\n",
        "    for i in range(1, 6): features.append(data_df[f'BidVolume{i}'].diff().fillna(0).values)\n",
        "    for i in range(1, 6): features.append(data_df[f'AskVolume{i}'].diff().fillna(0).values)\n",
        "\n",
        "    spread1 = data_df['AskPrice1'] - data_df['BidPrice1']\n",
        "    spread_change_pct = spread1.pct_change().fillna(0).replace([np.inf, -np.inf], 0)\n",
        "    features.append(spread_change_pct.values)\n",
        "\n",
        "    v_ask_price = (data_df['AskPrice1'] * data_df['AskVolume1'] + data_df['AskPrice2'] * data_df['AskVolume2']) / (data_df['AskVolume1'] + data_df['AskVolume2'] + 1e-12)\n",
        "    v_bid_price = (data_df['BidPrice1'] * data_df['BidVolume1'] + data_df['BidPrice2'] * data_df['BidVolume2']) / (data_df['BidVolume1'] + data_df['BidVolume2'] + 1e-12)\n",
        "    vwap = ((v_ask_price + v_bid_price) / 2).ffill().bfill()\n",
        "    vwap_change_pct = vwap.pct_change().fillna(0).replace([np.inf, -np.inf], 0)\n",
        "    features.append(vwap_change_pct.values)\n",
        "\n",
        "    total_bid_volume = data_df[[f'BidVolume{i}' for i in range(1, 6)]].sum(axis=1)\n",
        "    total_ask_volume = data_df[[f'AskVolume{i}' for i in range(1, 6)]].sum(axis=1)\n",
        "    obi = (total_bid_volume - total_ask_volume) / (total_bid_volume + total_ask_volume + 1e-12)\n",
        "    features.append(obi.values)\n",
        "    weighted_bid_price = np.sum([data_df[f'BidPrice{i}'] * data_df[f'BidVolume{i}'] for i in range(1, 6)], axis=0) / (total_bid_volume + 1e-12)\n",
        "    weighted_ask_price = np.sum([data_df[f'AskPrice{i}'] * data_df[f'AskVolume{i}'] for i in range(1, 6)], axis=0) / (total_ask_volume + 1e-12)\n",
        "    features.append(weighted_bid_price.values)\n",
        "    features.append(weighted_ask_price.values)\n",
        "\n",
        "    # --------------------------------------------------------------------------------------\n",
        "    # --- 【新增特征类别1: 高阶动态特征】 ---\n",
        "    # 价格加速度: 趋势是在加速还是减速？\n",
        "    price_acceleration = price_change.diff().fillna(0)\n",
        "    features.append(price_acceleration.values)\n",
        "    for window in [10, 30, 60]:\n",
        "        features.append(price_acceleration.rolling(window=window, min_periods=1).sum().fillna(0).values)\n",
        "\n",
        "    # 不平衡度的波动性: 市场情绪是稳定还是在剧烈摆动？\n",
        "    obi_volatility = obi.rolling(window=20, min_periods=1).std().fillna(0)\n",
        "    features.append(obi_volatility.values)\n",
        "    ofi_volatility = pd.Series(ofi_raw).rolling(window=20, min_periods=1).std().fillna(0)\n",
        "    features.append(ofi_volatility.values)\n",
        "\n",
        "    # --- 【新增特征类别2: 多时间尺度特征 (EMA)】 ---\n",
        "    # 使用EMA（指数移动平均）捕捉不同时间尺度下的趋势\n",
        "    # 短期 vs 中期\n",
        "    ema_mid_price_fast = mid_price_l1.ewm(span=10, adjust=False).mean()\n",
        "    ema_mid_price_slow = mid_price_l1.ewm(span=40, adjust=False).mean()\n",
        "    features.append((ema_mid_price_fast - ema_mid_price_slow).values) # MACD on mid-price\n",
        "\n",
        "    # 中期 vs 长期\n",
        "    ema_mid_price_very_slow = mid_price_l1.ewm(span=120, adjust=False).mean()\n",
        "    features.append((ema_mid_price_slow - ema_mid_price_very_slow).values)\n",
        "\n",
        "    # OFI 和 OBI 的多尺度信息\n",
        "    ema_obi_fast = obi.ewm(span=20, adjust=False).mean()\n",
        "    ema_obi_slow = obi.ewm(span=60, adjust=False).mean()\n",
        "    features.append((ema_obi_fast - ema_obi_slow).values) # MACD on OBI\n",
        "\n",
        "    ema_ofi_fast = pd.Series(ofi_raw).ewm(span=20, adjust=False).mean()\n",
        "    ema_ofi_slow = pd.Series(ofi_raw).ewm(span=60, adjust=False).mean()\n",
        "    features.append((ema_ofi_fast - ema_ofi_slow).values) # MACD on OFI\n",
        "\n",
        "    # --- 【新增特征类别3: 微观结构比率】 ---\n",
        "    # 描述订单簿“形状”的无量纲特征\n",
        "    # 挂单量比率: 盘口第一档的深度占前五档总深度的比例\n",
        "    features.append((data_df['BidVolume1'] / (total_bid_volume + 1e-12)).values)\n",
        "    features.append((data_df['AskVolume1'] / (total_ask_volume + 1e-12)).values)\n",
        "\n",
        "    # 盘口第一档和第二档的量价比\n",
        "    features.append((data_df['BidVolume1'] / (data_df['BidVolume2'] + 1e-12)).values)\n",
        "    features.append((data_df['AskVolume1'] / (data_df['AskVolume2'] + 1e-12)).values)\n",
        "\n",
        "    # 价差比率: 第二档价差相对于第一档价差的倍数\n",
        "    spread2 = data_df['AskPrice2'] - data_df['BidPrice2']\n",
        "    features.append((spread2 / (spread1 + 1e-12)).values)\n",
        "\n",
        "    # --- 【新增特征类别4: 流动性成本代理】 ---\n",
        "    # 市场冲击成本代理: 移动一个tick的成本有多高？\n",
        "    # 该值越小，说明流动性越好，冲击成本越低。\n",
        "    market_impact_cost = spread1 / (data_df['BidVolume1'] + data_df['AskVolume1'] + 1e-12)\n",
        "    features.append(market_impact_cost.values)\n",
        "\n",
        "    # 订单簿斜率代理: 每单位价格变动能吸收多少订单量？\n",
        "    price_diff_bid_1_2 = data_df['BidPrice1'] - data_df['BidPrice2']\n",
        "    price_diff_ask_1_2 = data_df['AskPrice2'] - data_df['AskPrice1']\n",
        "    # 避免除以0\n",
        "    slope_bid = data_df['BidVolume1'] / (price_diff_bid_1_2.where(price_diff_bid_1_2 > 0, 0.001))\n",
        "    slope_ask = data_df['AskVolume1'] / (price_diff_ask_1_2.where(price_diff_ask_1_2 > 0, 0.001))\n",
        "    features.append(slope_bid.values)\n",
        "    features.append(slope_ask.values)\n",
        "    # --------------------------------------------------------------------------------------\n",
        "\n",
        "    final_features = np.array(features, dtype=np.float32).T\n",
        "    if np.any(np.isnan(final_features)) or np.any(np.isinf(final_features)):\n",
        "        print(\"警告: 特征矩阵中检测到 NaN 或 Inf 值，将用0填充。\")\n",
        "        final_features = np.nan_to_num(final_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    print(f\"【增强版】总特征数量: {final_features.shape[1]}\")\n",
        "    return final_features\n",
        "\n",
        "# --- 4. 多日数据处理 ---\n",
        "def process_multi_day_data(start_date, end_date=None, symbol_types=SYMBOL_TYPES):\n",
        "    if end_date is None:\n",
        "        end_date = (pd.to_datetime(start_date) + pd.Timedelta(days=LOOKBACK_WINDOW + 7)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    result_by_symbol = {}\n",
        "    for symbol_type in symbol_types:\n",
        "        print(f\"\\n===== 处理品种: {symbol_type} =====\")\n",
        "        raw_data = get_processed_data(start_date, end_date, symbol_types=[symbol_type])\n",
        "        if raw_data.empty: continue\n",
        "\n",
        "        daily_raw_dict = {\n",
        "            pd.to_datetime(date).strftime(\"%Y-%m-%d\"): g.reset_index(drop=True)\n",
        "            for date, g in raw_data.groupby(\"Trade_Date\")\n",
        "        }\n",
        "        daily_orderbook_dict = {d: prepare_orderbook_data(df) for d, df in daily_raw_dict.items()}\n",
        "\n",
        "        normalization_mean_dict, normalization_stddev_dict, clipping_bounds_dict = {}, {}, {}\n",
        "        dates_list = sorted(list(daily_orderbook_dict.keys()))\n",
        "        if len(dates_list) <= LOOKBACK_WINDOW:\n",
        "            print(f\"警告: 可用交易日 {len(dates_list)} <= LOOKBACK_WINDOW {LOOKBACK_WINDOW}，将用所有历史估计标准化参数。\")\n",
        "\n",
        "        # --- START: 核心修改 - 只对衍生特征计算标准化参数 ---\n",
        "        for i in range(LOOKBACK_WINDOW, len(dates_list)):\n",
        "            date = dates_list[i]\n",
        "            look_back_days = dates_list[i-LOOKBACK_WINDOW:i]\n",
        "            stack_all_features = np.vstack([daily_orderbook_dict[dd] for dd in look_back_days])\n",
        "\n",
        "            # #注释更改2: 这里我们选特征只用20列以后，所以标准化参数也只针对这部分计算\n",
        "            stack_derived_features = stack_all_features[:, RAW_N:]\n",
        "\n",
        "            # 截断边界仍然在所有特征上计算，这有助于了解原始值的范围\n",
        "            lower_bounds_all = np.percentile(stack_all_features, 1, axis=0)\n",
        "            upper_bounds_all = np.percentile(stack_all_features, 99, axis=0)\n",
        "            clipping_bounds_dict[date] = (lower_bounds_all, upper_bounds_all)\n",
        "\n",
        "            # 仅截断衍生特征部分，用于计算均值和标准差\n",
        "            clipped_stack_derived = np.clip(stack_derived_features,\n",
        "                                            lower_bounds_all[RAW_N:],\n",
        "                                            upper_bounds_all[RAW_N:])\n",
        "\n",
        "            # 仅计算衍生特征的均值和标准差\n",
        "            means_derived = np.mean(clipped_stack_derived, axis=0)\n",
        "            stds_derived = np.std(clipped_stack_derived, axis=0)\n",
        "            stds_derived[stds_derived < 1e-8] = 1.0\n",
        "\n",
        "            # 为了方便后续使用，创建一个完整的参数数组\n",
        "            # 前20列的均值为0，标准差为1，这样即使误操作(x-0)/1也不会改变原始值\n",
        "            full_means = np.zeros(stack_all_features.shape[1], dtype=np.float32)\n",
        "            full_stds = np.ones(stack_all_features.shape[1], dtype=np.float32)\n",
        "\n",
        "            # 将计算好的衍生特征参数填入\n",
        "            full_means[RAW_N:] = means_derived.astype(np.float32)\n",
        "            full_stds[RAW_N:] = stds_derived.astype(np.float32)\n",
        "\n",
        "            normalization_mean_dict[date] = full_means\n",
        "            normalization_stddev_dict[date] = stds_derived.astype(np.float32) # 这里存stddev_dict只存衍生部分的即可，因为下面只用衍生的std\n",
        "            normalization_stddev_dict[date] = stds_derived # Re-assign for clarity, mean is full, std is partial. Let's fix this for consistency.\n",
        "\n",
        "            # Let's be consistent and store full arrays for both.\n",
        "            normalization_mean_dict[date] = full_means\n",
        "            normalization_stddev_dict[date] = full_stds\n",
        "        # --- END: 核心修改 ---\n",
        "\n",
        "        daily_norm_data_dict = {}\n",
        "        for i in range(LOOKBACK_WINDOW, len(dates_list)):\n",
        "            date = dates_list[i]\n",
        "\n",
        "            # --- START: 核心修改 - 构建混合矩阵 ---\n",
        "            # 1. 分离当天的原始特征和衍生特征\n",
        "            features_all = daily_orderbook_dict[date].copy()\n",
        "            features_base = features_all[:, :RAW_N]      # 前20列原始特征\n",
        "            features_derived = features_all[:, RAW_N:] # 20列之后的衍生特征\n",
        "\n",
        "            # 2. 获取用于标准化的参数\n",
        "            lower_bounds_all, upper_bounds_all = clipping_bounds_dict[date]\n",
        "            means_full, stds_full = normalization_mean_dict[date], normalization_stddev_dict[date]\n",
        "\n",
        "            # 提取衍生特征对应的参数\n",
        "            lower_bounds_derived = lower_bounds_all[RAW_N:]\n",
        "            upper_bounds_derived = upper_bounds_all[RAW_N:]\n",
        "            means_derived = means_full[RAW_N:]\n",
        "            stds_derived = stds_full[RAW_N:]\n",
        "\n",
        "            # 3. 只对衍生特征进行截断和标准化\n",
        "            clipped_features_derived = np.clip(features_derived, lower_bounds_derived, upper_bounds_derived)\n",
        "            norm_features_derived = (clipped_features_derived - means_derived) / stds_derived\n",
        "\n",
        "            # 4. #注释更改1: 将原始的前20列和标准化后的衍生特征拼接成最终的混合矩阵\n",
        "            final_features = np.hstack([features_base, norm_features_derived])\n",
        "\n",
        "            daily_norm_data_dict[date] = final_features\n",
        "\n",
        "            # 5. #注释更改3: 验证混合矩阵结构，方便检查\n",
        "            if i == LOOKBACK_WINDOW: # 只在处理第一天时打印一次\n",
        "                print(\"\\n--- 验证混合矩阵结构 ---\")\n",
        "                print(f\"前20列 (原始值) 的均值示例: {np.mean(final_features[:, :RAW_N], axis=0)[:5]}\")\n",
        "                print(f\"后半部分 (标准化衍生特征) 的均值示例: {np.mean(final_features[:, RAW_N:], axis=0)[:5]}\")\n",
        "                print(f\"后半部分 (标准化衍生特征) 的标准差示例: {np.std(final_features[:, RAW_N:], axis=0)[:5]}\")\n",
        "                print(\"\\n>>> 检查点: 前20列均值应为较大的原始价格/量级，后半部分均值接近0，标准差接近1。\")\n",
        "                print(\"--- 验证完毕 ---\\n\")\n",
        "            # --- END: 核心修改 ---\n",
        "\n",
        "        daily_label_dict, label_names = generate_multitask_labels(K, daily_raw_dict)\n",
        "        # --- 在这里内联删除脏样本（保持 X/Y 对齐） ---\n",
        "        PRUNE_CAP_BPS   = 10000    # 5秒持仓建议 10000 bps\n",
        "        DROP_TAIL_K     = True   # 每天最后 K 行由于 shift(-K) 无未来值，直接删\n",
        "        DROP_BAD_PRICE  = True   # Ask/Bid <= 0 或 非数 删\n",
        "        y_idx = label_names.index('y_reg')\n",
        "\n",
        "        kept_total, drop_total = 0, 0\n",
        "        for d in sorted(set(daily_label_dict) & set(daily_norm_data_dict)):\n",
        "            # 取并对齐长度\n",
        "            y = daily_label_dict[d]\n",
        "            if isinstance(y, pd.DataFrame):\n",
        "                y = y.to_numpy()\n",
        "            else:\n",
        "                y = np.asarray(y)\n",
        "            x = np.asarray(daily_norm_data_dict[d])\n",
        "            n = min(len(y), len(x))\n",
        "            if n == 0:\n",
        "                continue\n",
        "            y = y[:n]; x = x[:n]\n",
        "\n",
        "            # 初始保留\n",
        "            keep = np.ones(n, dtype=bool)\n",
        "\n",
        "            # 1) 丢最后 K 行（避免 bid_future(+K)=NaN）\n",
        "            tail_mask = np.zeros(n, dtype=bool)\n",
        "            if DROP_TAIL_K and n > K:\n",
        "                tail_mask[-K:] = True\n",
        "                keep &= ~tail_mask\n",
        "\n",
        "            # 2) y_reg 非数 或 绝对值超阈\n",
        "            yreg = y[:, y_idx]\n",
        "            nonfinite_mask = ~np.isfinite(yreg)\n",
        "            cap_mask = np.abs(yreg) > PRUNE_CAP_BPS\n",
        "            keep &= np.isfinite(yreg) & (np.abs(yreg) <= PRUNE_CAP_BPS)\n",
        "\n",
        "            # 3) 价格为 0/负/非数（用原始数据更可靠）\n",
        "            price_mask = np.zeros(n, dtype=bool)\n",
        "            if DROP_BAD_PRICE and d in daily_raw_dict:\n",
        "                ask = pd.to_numeric(daily_raw_dict[d]['AskPrice1'], errors='coerce').to_numpy()[:n]\n",
        "                bid = pd.to_numeric(daily_raw_dict[d]['BidPrice1'], errors='coerce').to_numpy()[:n]\n",
        "                bad_price = ~(np.isfinite(ask) & np.isfinite(bid) & (ask > 0) & (bid > 0))\n",
        "                price_mask = bad_price\n",
        "                keep &= ~bad_price\n",
        "\n",
        "            dropped = int((~keep).sum())\n",
        "            kept    = int(keep.sum())\n",
        "            if dropped:\n",
        "                # 说明：各原因计数可能重叠（总数按并集计算）\n",
        "                c_tail = int(tail_mask.sum())\n",
        "                c_nonf = int(nonfinite_mask.sum())\n",
        "                c_cap  = int(cap_mask.sum())\n",
        "                c_price= int(price_mask.sum())\n",
        "                print(f\"[prune] {d}: drop {dropped}/{n} rows \"\n",
        "                      f\"(tailK={c_tail}, |y_reg|>{PRUNE_CAP_BPS}bps={c_cap}, \"\n",
        "                      f\"nonfinite={c_nonf}, bad_price={c_price})\")\n",
        "                bad_idx = np.where(~keep)[0][:5]\n",
        "                for j in bad_idx:\n",
        "                    ts = daily_raw_dict[d]['Timestamp'].iloc[j] if 'Timestamp' in daily_raw_dict[d].columns else None\n",
        "                    yv = y[j, y_idx] if j < len(y) else None\n",
        "                    aj = ask[j] if 'ask' in locals() and j < len(ask) else None\n",
        "                    bj = bid[j] if 'bid' in locals() and j < len(bid) else None\n",
        "                    print(f\"    idx={j} y_reg={yv} ask={aj} bid={bj} ts={ts}\")\n",
        "\n",
        "            # 回写（保持 X/Y 对齐）\n",
        "            daily_label_dict[d]      = y[keep]\n",
        "            daily_norm_data_dict[d]  = x[keep]\n",
        "            kept_total += kept; drop_total += dropped\n",
        "\n",
        "        print(f\"[prune] total dropped: {drop_total}, kept: {kept_total}\")\n",
        "\n",
        "\n",
        "        result_by_symbol[symbol_type] = {\n",
        "            \"daily_norm_data_dict\": daily_norm_data_dict,\n",
        "            \"daily_label_dict\": daily_label_dict, \"label_names\": label_names,\n",
        "            \"norm_mean_dict\": normalization_mean_dict, \"norm_stddev_dict\": normalization_stddev_dict,\n",
        "            \"clipping_bounds_dict\": clipping_bounds_dict, \"daily_orderbook_dict\": daily_orderbook_dict,\n",
        "            \"daily_raw_dict\": daily_raw_dict,\n",
        "        }\n",
        "        print(f\"{symbol_type} 数据处理完成，可用日期: {sorted(list(daily_norm_data_dict.keys()))}\")\n",
        "\n",
        "    return result_by_symbol\n",
        "\n",
        "# --- 5. 生成训练/测试样本 ---\n",
        "def generate_X_y(daily_norm_data_dict, daily_label_dict):\n",
        "    all_x, all_y = [], []\n",
        "    valid_dates = sorted(list(set(daily_norm_data_dict.keys()) & set(daily_label_dict.keys())))\n",
        "\n",
        "    for date in valid_dates:\n",
        "        features, labels = daily_norm_data_dict[date], daily_label_dict[date]\n",
        "        num_labels = len(labels)\n",
        "        if num_labels > 0:\n",
        "            all_x.append(features[:num_labels, :]); all_y.append(labels)\n",
        "\n",
        "    if not all_x: return None, None\n",
        "    data_x, data_y_multi = np.vstack(all_x), np.vstack(all_y)\n",
        "\n",
        "    N, P_x = data_x.shape\n",
        "    if N < TIMESTAMP_PER_SAMPLE: return None, None\n",
        "\n",
        "    x = np.zeros([(N - TIMESTAMP_PER_SAMPLE + 1), TIMESTAMP_PER_SAMPLE, P_x])\n",
        "    for i in range(N - TIMESTAMP_PER_SAMPLE + 1):\n",
        "        x[i] = data_x[i:(i + TIMESTAMP_PER_SAMPLE), :]\n",
        "\n",
        "    x = x.reshape(x.shape + (1,))\n",
        "    y_multi = data_y_multi[(TIMESTAMP_PER_SAMPLE - 1):, :]\n",
        "    return x, y_multi\n",
        "\n",
        "\n",
        "# --- 6. 辅助及可视化函数 ---\n",
        "def save_processed_data(data, filename):\n",
        "    with open(os.path.join(DATA_CACHE_DIR, filename), 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "    print(f\"数据已保存到 {os.path.join(DATA_CACHE_DIR, filename)}\")\n",
        "\n",
        "def load_processed_data(filename):\n",
        "    file_path = os.path.join(DATA_CACHE_DIR, filename)\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "def visualize_regression_data(y_true, y_pred, symbol_type):\n",
        "    vis_output_dir = os.path.join(DATA_CACHE_DIR, \"visualizations\")\n",
        "    os.makedirs(vis_output_dir, exist_ok=True)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.3, label='预测 vs. 真实 (BPS)')\n",
        "    lims = [min(plt.xlim()[0], plt.ylim()[0]), max(plt.xlim()[1], plt.ylim()[1])]\n",
        "    plt.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='y=x (完美预测)')\n",
        "    plt.xlabel(\"真实收益率 (True Values, BPS)\", fontsize=12)\n",
        "    plt.ylabel(\"预测收益率 (Predictions, BPS)\", fontsize=12)\n",
        "    plt.title(f\"{symbol_type} - 主任务回归预测结果\", fontsize=14)\n",
        "    plt.legend(); plt.grid(True); plt.axis('equal'); plt.tight_layout()\n",
        "    save_path = os.path.join(vis_output_dir, f'{symbol_type}_regression_scatter.png')\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"回归散点图已保存至: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(history, symbol_type):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='训练损失 (Train Loss)')\n",
        "    plt.plot(history['val_loss'], label='验证损失 (Validation Loss)')\n",
        "    plt.title(f'{symbol_type} 模型总损失历史', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Combined Loss', fontsize=12)\n",
        "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.6); plt.tight_layout()\n",
        "    vis_output_dir = os.path.join(DATA_CACHE_DIR, \"visualizations\")\n",
        "    os.makedirs(vis_output_dir, exist_ok=True)\n",
        "    save_path = os.path.join(vis_output_dir, f'{symbol_type}_loss_history.png')\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"训练历史图已保存至: {save_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0229a141-fda2-429b-8a31-e2ee2422b337",
      "metadata": {
        "id": "0229a141-fda2-429b-8a31-e2ee2422b337"
      },
      "outputs": [],
      "source": [
        "# ==== 保留原有功能 + 新增 y_reg>0 机会数统计 ====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 机会口径可调\n",
        "THRESHOLD_BPS = 0.0   # 判定阈值：y_reg > THRESHOLD_BPS\n",
        "USE_GE        = False # 改成 True 则用 ≥ 阈值\n",
        "\n",
        "def _extract_ycol(v, label_names, col='y_reg'):\n",
        "    \"\"\"从 DataFrame/ndarray 中抽取指定列，返回一维 ndarray。\"\"\"\n",
        "    idx = label_names.index(col)\n",
        "    if isinstance(v, pd.DataFrame):\n",
        "        if col in v.columns:\n",
        "            return v[col].to_numpy()\n",
        "        arr = v.to_numpy()\n",
        "        return arr if arr.ndim == 1 else arr[:, idx]\n",
        "    arr = np.asarray(v)\n",
        "    return arr if arr.ndim == 1 else arr[:, idx]\n",
        "\n",
        "def count_yreg_by_day(daily_label_dict, label_names, thr=0.0, col='y_reg', ge=False):\n",
        "    \"\"\"兼容 DataFrame 或 ndarray 的按日机会数统计，返回 DataFrame 和合计。\"\"\"\n",
        "    rows, all_pos, all_total = [], 0, 0\n",
        "    cmp = (lambda a: a >= thr) if ge else (lambda a: a > thr)\n",
        "    for d in sorted(daily_label_dict):\n",
        "        v = daily_label_dict[d]\n",
        "        if v is None:\n",
        "            continue\n",
        "        arr = _extract_ycol(v, label_names, col)\n",
        "        arr = arr[np.isfinite(arr)]\n",
        "        n = int(arr.size)\n",
        "        k = int(np.sum(cmp(arr)))\n",
        "        rows.append([str(d), k, n, (k/n if n else np.nan)])\n",
        "        all_pos += k; all_total += n\n",
        "    return pd.DataFrame(rows, columns=['date','count_pos','total','share']), all_pos, all_total\n",
        "\n",
        "# --- 执行多日数据处理（原样保留） ---\n",
        "result_by_symbol = process_multi_day_data(START_DATE, END_DATE, symbol_types=SYMBOL_TYPES)\n",
        "\n",
        "if result_by_symbol:\n",
        "    print(\"\\n===== 数据处理完成，开始可视化标签分布（保留） + 统计 y_reg 机会数（新增） =====\")\n",
        "    for symbol_type, data_dict in result_by_symbol.items():\n",
        "        print(f\"\\n品种: {symbol_type}\")\n",
        "\n",
        "        if not data_dict.get('daily_norm_data_dict'):\n",
        "            print(f\"品种 {symbol_type} 没有足够的数据。\")\n",
        "            continue\n",
        "\n",
        "        # 生成并显示多任务训练数据形状（原样保留）\n",
        "        X, y_multi = generate_X_y(data_dict['daily_norm_data_dict'], data_dict['daily_label_dict'])\n",
        "        label_names = data_dict.get('label_names', [])\n",
        "\n",
        "        if X is not None and y_multi is not None and label_names:\n",
        "            print(f\"生成的训练数据形状: X: {X.shape}, y_multi: {y_multi.shape}\")\n",
        "            print(f\"标签名称: {label_names}\")\n",
        "            print(f\"y_multi 示例 (前5行):\\n{y_multi[:5, :]}\")\n",
        "\n",
        "            # --- 原功能：为每个任务单独绘制标签分布直方图 ---\n",
        "            num_tasks = y_multi.shape[1]\n",
        "            ncols = 3\n",
        "            nrows = (num_tasks + ncols - 1) // ncols\n",
        "\n",
        "            fig, axes = plt.subplots(nrows, ncols, figsize=(15, nrows * 4))\n",
        "            axes = np.array(axes).reshape(-1)  # 展平更稳健\n",
        "\n",
        "            for i in range(num_tasks):\n",
        "                task_name = label_names[i]\n",
        "                task_data = y_multi[:, i]\n",
        "\n",
        "                ax = axes[i]\n",
        "                ax.hist(task_data, bins=100, alpha=0.75,\n",
        "                        label=f'均值: {np.nanmean(task_data):.2f}\\n标准差: {np.nanstd(task_data):.2f}')\n",
        "                ax.set_title(f'{symbol_type} - 标签分布: {task_name}')\n",
        "                ax.set_xlabel('值 (BPS 或其他单位)')\n",
        "                ax.set_ylabel('频数')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                ax.legend()\n",
        "\n",
        "            # 隐藏多余的子图\n",
        "            for j in range(num_tasks, len(axes)):\n",
        "                fig.delaxes(axes[j])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # --- 新增：统计 y_reg 机会数（总体 + 按日 Top5），不影响以上绘图 ---\n",
        "            if 'y_reg' in label_names:\n",
        "                y_idx = label_names.index('y_reg')\n",
        "                y = np.asarray(y_multi[:, y_idx])\n",
        "                y = y[np.isfinite(y)]  # 过滤 NaN/inf\n",
        "\n",
        "                total = int(y.size)\n",
        "                pos   = int(np.sum(y >= THRESHOLD_BPS)) if USE_GE else int(np.sum(y > THRESHOLD_BPS))\n",
        "                if THRESHOLD_BPS == 0.0:\n",
        "                    eq0 = int(np.sum(y == 0.0))\n",
        "                    neg = total - pos - eq0\n",
        "                    print(f\"[总体] y_reg>{THRESHOLD_BPS}: {pos} / {total} = {pos/total:.2%} | =0: {eq0} ({eq0/total:.2%}) | <0: {neg} ({neg/total:.2%})\")\n",
        "                else:\n",
        "                    print(f\"[总体] y_reg>{THRESHOLD_BPS}: {pos} / {total} = {pos/total:.2%}\")\n",
        "\n",
        "                # 按日\n",
        "                if data_dict.get('daily_label_dict'):\n",
        "                    day_df, all_pos, all_total = count_yreg_by_day(\n",
        "                        data_dict['daily_label_dict'], label_names,\n",
        "                        thr=THRESHOLD_BPS, col='y_reg', ge=USE_GE\n",
        "                    )\n",
        "                    if not day_df.empty:\n",
        "                        print(\"\\n按日 Top5（share 最大）:\")\n",
        "                        print(day_df.sort_values('share', ascending=False).head(5)\n",
        "                              .to_string(index=False, formatters={'share': lambda x: f'{x:.2%}' if pd.notna(x) else 'NA'}))\n",
        "                        print(\"\\n按日 Top5（count_pos 最大）:\")\n",
        "                        print(day_df.sort_values('count_pos', ascending=False).head(5)\n",
        "                              .to_string(index=False, formatters={'share': lambda x: f'{x:.2%}' if pd.notna(x) else 'NA'}))\n",
        "                        print(f\"\\n[按日合计] y_reg>{THRESHOLD_BPS}: {all_pos} / {all_total} = {all_pos/all_total:.2%}\")\n",
        "            else:\n",
        "                print(\"未找到 y_reg 标签，跳过机会数统计。\")\n",
        "\n",
        "        # 原功能：保存处理后的数据\n",
        "        save_processed_data(data_dict, f'{symbol_type}_processed_data_regression.pkl')\n",
        "else:\n",
        "    print(\"数据处理失败，请检查日期范围或本地Parquet文件 'IM00_subset.parquet'。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692e4f2b-b4dd-4510-aa5d-2e31e92c14cc",
      "metadata": {
        "id": "692e4f2b-b4dd-4510-aa5d-2e31e92c14cc"
      },
      "outputs": [],
      "source": [
        "# ==== Feature Selection using ONLY the earliest 50% of days (no-leak) ====\n",
        "import os, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "# 从衍生特征中选出前 N 个最重要的；最终特征 = 前 RAW_N + TopK(衍生)\n",
        "NUM_TOP_DERIVED_FEATURES = 86\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "for symbol_type in SYMBOL_TYPES:\n",
        "    print(f\"\\n{'='*25} 为品种 {symbol_type} 进行特征选择（仅用最早 50% 交易日） {'='*25}\")\n",
        "\n",
        "    processed_data_path = os.path.join(DATA_CACHE_DIR, f'{symbol_type}_processed_data_regression.pkl')\n",
        "    data_dict = load_processed_data(f'{symbol_type}_processed_data_regression.pkl')\n",
        "    if data_dict is None:\n",
        "        print(f\"未找到 {symbol_type} 的缓存数据，跳过特征选择。\")\n",
        "        continue\n",
        "\n",
        "    if 'daily_norm_data_dict' not in data_dict or 'daily_label_dict' not in data_dict:\n",
        "        print(\"数据字典缺少 'daily_norm_data_dict' 或 'daily_label_dict'。跳过。\")\n",
        "        continue\n",
        "\n",
        "    daily_norm_data = data_dict['daily_norm_data_dict']\n",
        "    daily_label     = data_dict['daily_label_dict']\n",
        "\n",
        "    # 1) 以“交易日”为单位对齐，并按日期排序\n",
        "    valid_dates = sorted(set(daily_norm_data.keys()) & set(daily_label.keys()))\n",
        "    num_days = len(valid_dates)\n",
        "    if num_days < 2:\n",
        "        print(f\"{symbol_type}: 可用交易日过少（{num_days}），跳过。\")\n",
        "        continue\n",
        "\n",
        "    # 2) 只使用最早 50% 的交易日作为「特征选择全集」\n",
        "    half_idx = max(1, math.floor(num_days * 0.5))\n",
        "    fs_dates = valid_dates[:half_idx]  # Feature Selection dates (earliest half)\n",
        "    print(f\"{symbol_type}: 总交易日={num_days} | 用于特征选择的最早 50% 交易日数={len(fs_dates)}\")\n",
        "    if len(fs_dates) == 0:\n",
        "        print(f\"{symbol_type}: 50% 子集为空，跳过。\")\n",
        "        continue\n",
        "\n",
        "    # 3) 在这 50% 内扁平化样本（逐日拼接；不跨日滑窗）\n",
        "    X_list, y_list = [], []\n",
        "    for d in fs_dates:\n",
        "        F = daily_norm_data[d]\n",
        "        L = daily_label[d]\n",
        "        n = min(len(F), len(L))\n",
        "        if n <= 0:\n",
        "            continue\n",
        "        # 允许 F/L 为 DataFrame 或 ndarray，统一扁平化为 ndarray\n",
        "        X_list.append(np.asarray(F[:n, :]))\n",
        "        y_list.append(np.asarray(L[:n, :]))\n",
        "\n",
        "    if not X_list:\n",
        "        print(f\"{symbol_type}: 50% 子集中无样本，跳过。\")\n",
        "        continue\n",
        "\n",
        "    X_flat = np.vstack(X_list)               # [N, F]\n",
        "    y_multi_flat = np.vstack(y_list)         # [N, L]\n",
        "    y_reg_flat = y_multi_flat[:, 0].ravel()  # 仅用主任务 y_reg 做重要度\n",
        "\n",
        "    # 4) 仅以“衍生特征”作为 LGBM 的输入\n",
        "    if X_flat.shape[1] <= RAW_N:\n",
        "        print(f\"{symbol_type}: 没有衍生特征（F={X_flat.shape[1]} <= RAW_N={RAW_N}），跳过。\")\n",
        "        continue\n",
        "\n",
        "    X_derived = X_flat[:, RAW_N:]\n",
        "    num_derived = X_derived.shape[1]\n",
        "    print(f\"{symbol_type}: 用于 LGBM 的衍生特征形状: {X_derived.shape}；目标 y 形状: {y_reg_flat.shape}\")\n",
        "\n",
        "    # 5) 在「最早 50%」内部做顺序切分：前 90% 训练、后 10% 验证（不打乱、不使用后半段的任何数据）\n",
        "    split_idx = int(len(X_derived) * 0.9)\n",
        "    split_idx = min(max(split_idx, 1), len(X_derived) - 1)  # 保证两端均非空\n",
        "\n",
        "    X_tr, y_tr = X_derived[:split_idx], y_reg_flat[:split_idx]\n",
        "    X_va, y_va = X_derived[split_idx:], y_reg_flat[split_idx:]\n",
        "    print(f\"{symbol_type}: 50% 子集内切分 -> 训练样本={len(X_tr)}，验证样本={len(X_va)}\")\n",
        "\n",
        "    # 6) 训练 LGBM 拿重要度（仅在 50% 子集上）\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l1',\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=31,\n",
        "        n_jobs=-1,\n",
        "        seed=RANDOM_STATE\n",
        "    )\n",
        "    if len(X_va) >= 32:\n",
        "        lgbm.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            eval_metric='mae',\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=10, first_metric_only=True, verbose=False)]\n",
        "        )\n",
        "    else:\n",
        "        # 验证样本太少则不做早停\n",
        "        lgbm.fit(X_tr, y_tr)\n",
        "\n",
        "    # 7) 重要度统计 & 选 TopK（TopK 也只来自这 50% 子集训练得到的模型）\n",
        "    importances = lgbm.feature_importances_\n",
        "    if importances is None or len(importances) != num_derived:\n",
        "        importances = np.zeros(num_derived, dtype=np.float32)\n",
        "\n",
        "    feature_importance_df = (\n",
        "        pd.DataFrame({\"derived_feature_index\": np.arange(num_derived), \"importance\": importances})\n",
        "          .sort_values(\"importance\", ascending=False, kind=\"mergesort\")\n",
        "    )\n",
        "    print(\"\\n衍生特征重要性排名前10位（基于最早 50% 训练得到）:\")\n",
        "    print(feature_importance_df.head(10))\n",
        "\n",
        "    top_k = int(min(NUM_TOP_DERIVED_FEATURES, num_derived))\n",
        "    top_derived_indices_local = feature_importance_df['derived_feature_index'].head(top_k).to_numpy()\n",
        "    top_absolute_indices = (top_derived_indices_local + RAW_N).tolist()  # 映射回全量特征的绝对索引\n",
        "\n",
        "    # 8) 合并“前 RAW_N（标准化的原始量价副本）+ 衍生 TopK”\n",
        "    base_raw_indices = list(range(RAW_N))\n",
        "    final_selected_indices = sorted(set(base_raw_indices + top_absolute_indices))\n",
        "\n",
        "    print(f\"\\n最终特征数: {len(final_selected_indices)} \"\n",
        "          f\"(含 {len(base_raw_indices)} 个原始副本 + {len(top_absolute_indices)} 个衍生TopK；\"\n",
        "          f\"衍生总数={num_derived}; 仅用最早 50% 的日子训练 LGBM)\")\n",
        "\n",
        "    # 9) 保存\n",
        "    data_dict['selected_feature_indices'] = final_selected_indices\n",
        "    save_processed_data(data_dict, f'{symbol_type}_processed_data_regression.pkl')\n",
        "    print(f\"已将筛选后的特征索引保存到: '{processed_data_path}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f734f19d-0675-4c12-baaf-228cc6609174",
      "metadata": {
        "id": "f734f19d-0675-4c12-baaf-228cc6609174"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 模型参数 ---\n",
        "D_MODEL = 128          # Transformer内部的工作维度\n",
        "ATTENTION_NUM_HEADS = 8  # 多头注意力头数\n",
        "NUM_ENCODER_LAYERS = 3   # Transformer编码器层数\n",
        "DIM_FEEDFORWARD = 256    # 前馈网络维度\n",
        "MODEL_DROPOUT = 0.2      # Dropout比例\n",
        "\n",
        "# --- 训练参数 ---\n",
        "NUM_EPOCHS = 150          # 训练的Epoch数 (可以适当减少，因为没有预训练)\n",
        "LEARNING_RATE = 1e-4     # 学习率 (使用一个统一的学习率)\n",
        "BATCH_SIZE = 1024\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# --- 多任务损失权重 ---\n",
        "# 权重决定了每个任务对总损失的贡献\n",
        "LOSS_WEIGHTS = {\n",
        "    'y_reg': 1.0,         # 主任务\n",
        "    'y_rv': 0.5,          # 波动率\n",
        "    'y_fillprob': 0.8,    # 成交概率\n",
        "    'y_roll': 0.2,        # 交易成本\n",
        "    'y_vpin': 0.4,         # 流动性风险\n",
        "    'y_direction': 0.5,\n",
        "    'y_upper_bound': 0.5,\n",
        "    'y_lower_bound': 0.5\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72db062-4795-48be-88ab-52ef2eb5f5a8",
      "metadata": {
        "id": "c72db062-4795-48be-88ab-52ef2eb5f5a8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Dict, Tuple, List, Union\n",
        "\n",
        "# --- 1. 数据集类 ---\n",
        "\n",
        "class RegressionLOBDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.FloatTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"根据某个监控指标来提前停止训练。\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0.0, path='checkpoint.pt', mode='min', trace_func=print):\n",
        "        self.patience = int(patience)\n",
        "        self.verbose = bool(verbose)\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = float(delta)\n",
        "        self.path = path\n",
        "        self.mode = mode\n",
        "        self.trace_func = trace_func\n",
        "        # 用 -inf/inf 初始化阈值，兼容 numpy 2.0\n",
        "        self.val_metric_best = -float('inf') if mode == 'max' else float('inf')\n",
        "\n",
        "    def save_checkpoint(self, current_metric, model):\n",
        "        try:\n",
        "            import torch\n",
        "            torch.save(model.state_dict(), self.path)\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.val_metric_best = current_metric\n",
        "\n",
        "    def __call__(self, current_metric, model):\n",
        "        # 对于 'min'，更小更好；对于 'max'，更大更好\n",
        "        improve = (current_metric < self.val_metric_best - self.delta) if self.mode == 'min' \\\n",
        "                  else (current_metric > self.val_metric_best + self.delta)\n",
        "\n",
        "        if self.val_metric_best in (-float('inf'), float('inf')) or improve:\n",
        "            if self.verbose and self.val_metric_best not in (-float('inf'), float('inf')):\n",
        "                self.trace_func(f\"Metric improved: {self.val_metric_best:.6f} -> {current_metric:.6f}\")\n",
        "            self.save_checkpoint(current_metric, model)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                self.trace_func(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "\n",
        "    def save_checkpoint(self, current_metric, model):\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation metric improved ({self.val_metric_best:.6f} --> {current_metric:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_metric_best = current_metric\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.permute(1, 0, 2); x = x + self.pe[:x.size(0)]; return x.permute(1, 0, 2)\n",
        "\n",
        "\n",
        "# --- 2. Transformer ---\n",
        "class TransformerLOB(nn.Module):\n",
        "    def __init__(self, feature_num, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, task_names: List[str]):\n",
        "        super().__init__()\n",
        "        self.feature_num = feature_num\n",
        "\n",
        "        # --- 【修改】使用MLP替换单层Linear作为输入投射层 ---\n",
        "        # 增强模型对输入特征的非线性表征能力\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(feature_num, d_model * 2), # 先将特征投射到更高维度\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 2, d_model)      # 再投射回Transformer的工作维度\n",
        "        )\n",
        "\n",
        "        # --- Transformer主干网络 ---\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        # --- 任务头 (Heads) ---\n",
        "        self.output_heads = nn.ModuleDict()\n",
        "        for task_name in task_names:\n",
        "            if task_name == 'y_direction':\n",
        "                # 分类任务的输出头，输出维度为3 (下跌, 不变, 上涨)\n",
        "                self.output_heads[task_name] = nn.Sequential(\n",
        "                    nn.Linear(d_model * TIMESTAMP_PER_SAMPLE, d_model // 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.Linear(d_model // 2, 3) # 输出3个logits\n",
        "                )\n",
        "            else:\n",
        "                # 回归任务的输出头\n",
        "                self.output_heads[task_name] = nn.Sequential(\n",
        "                    nn.Linear(d_model * TIMESTAMP_PER_SAMPLE, d_model // 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.Linear(d_model // 2, 1)\n",
        "                )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.dim() > 1:\n",
        "                if 'linear' in name or 'proj' in name:\n",
        "                    nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        if x.dim() == 4:\n",
        "             x = x.squeeze(-1) # Shape: (Batch, SeqLen, Features)\n",
        "\n",
        "        # 1. 将输入特征通过MLP投射到d_model维度\n",
        "        x_projected = self.input_proj(x) # Shape: (Batch, SeqLen, d_model)\n",
        "\n",
        "        # 2. 后续流程不变\n",
        "        x_pos_encoded = self.pos_encoder(x_projected)\n",
        "        transformer_output = self.transformer_encoder(x_pos_encoded) # Shape: (Batch, SeqLen, d_model)\n",
        "\n",
        "        # 3. 池化并通过不同的任务头得到预测结果\n",
        "        pooled_output = transformer_output.flatten(start_dim=1)\n",
        "        task_predictions = {\n",
        "            task_name: head(pooled_output).squeeze(-1)\n",
        "            for task_name, head in self.output_heads.items()\n",
        "        }\n",
        "\n",
        "        return task_predictions\n",
        "\n",
        "def initiate_transformer_model(feature_num, task_names, **kwargs):\n",
        "    return TransformerLOB(\n",
        "        feature_num=feature_num,\n",
        "        task_names=task_names,\n",
        "        d_model=kwargs.get('d_model', 128),\n",
        "        nhead=kwargs.get('nhead', 8),\n",
        "        num_encoder_layers=kwargs.get('num_encoder_layers', 3),\n",
        "        dim_feedforward=kwargs.get('dim_feedforward', 256),\n",
        "        dropout=kwargs.get('dropout', 0.1)\n",
        "    )\n",
        "\n",
        "# --- 3. 自定义损失函数 ---\n",
        "class ICLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "        mean_pred = torch.mean(y_pred)\n",
        "        mean_true = torch.mean(y_true)\n",
        "        pred_centered = y_pred - mean_pred\n",
        "        true_centered = y_true - mean_true\n",
        "        cov = torch.sum(pred_centered * true_centered)\n",
        "        std_pred = torch.sqrt(torch.sum(pred_centered**2) + self.eps)\n",
        "        std_true = torch.sqrt(torch.sum(true_centered**2) + self.eps)\n",
        "        pearson_corr = cov / (std_pred * std_true)\n",
        "        return 1 - pearson_corr\n",
        "\n",
        "class AsymmetricMSELoss(nn.Module):\n",
        "    def __init__(self, asymmetry_param=3.0):\n",
        "        super().__init__()\n",
        "        self.asymmetry_param = asymmetry_param\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        errors = self.mse(y_pred, y_true)\n",
        "        loss_weights = torch.ones_like(y_true)\n",
        "        false_positive_mask = (y_pred > 0) & (y_true < 0)\n",
        "        loss_weights[false_positive_mask] = self.asymmetry_param\n",
        "        asymmetric_loss = errors * loss_weights\n",
        "        return torch.mean(asymmetric_loss)\n",
        "\n",
        "class PrecisionFocusedLoss(nn.Module):\n",
        "    def __init__(self, alpha=10.0, asymmetry_param=3.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.ic_loss = ICLoss()\n",
        "        self.asymmetric_mse = AsymmetricMSELoss(asymmetry_param=asymmetry_param)\n",
        "        print(f\"PrecisionFocusedLoss initialized with alpha={self.alpha} and asymmetry_param={asymmetry_param}\")\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        loss_ic = self.ic_loss(y_pred, y_true)\n",
        "        loss_asymm_mse = self.asymmetric_mse(y_pred, y_true)\n",
        "        combined_loss = loss_ic + self.alpha * loss_asymm_mse\n",
        "        return combined_loss\n",
        "\n",
        "class BalancedPrecisionRecallLoss(nn.Module):\n",
        "    def __init__(self, base_loss_weight=1.0, fp_asymmetry_param=4.0, fn_penalty_param=2.0,\n",
        "                 proxy_clf_weight=0.5, gamma=5.0, threshold=0.0):\n",
        "        super().__init__()\n",
        "        self.base_loss_weight = base_loss_weight\n",
        "        self.fp_asymmetry_param = fp_asymmetry_param\n",
        "        self.fn_penalty_param = fn_penalty_param\n",
        "        self.proxy_clf_weight = proxy_clf_weight\n",
        "        self.gamma = gamma\n",
        "        self.threshold = threshold\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        self.bce = nn.BCELoss(reduction='none')\n",
        "        print(f\"BalancedPrecisionRecallLoss initialized with:\\n\"\n",
        "              f\"  - base_loss_weight={base_loss_weight}\\n\"\n",
        "              f\"  - fp_asymmetry_param={fp_asymmetry_param} (for Precision)\\n\"\n",
        "              f\"  - fn_penalty_param={fn_penalty_param} (for Recall)\\n\"\n",
        "              f\"  - proxy_clf_weight={proxy_clf_weight}\\n\"\n",
        "              f\"  - gamma={gamma}\")\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        base_errors = self.mse(y_pred, y_true)\n",
        "        base_loss = torch.mean(base_errors) * self.base_loss_weight\n",
        "\n",
        "        loss_weights = torch.ones_like(y_true)\n",
        "        fp_mask = (y_pred > self.threshold) & (y_true < self.threshold)\n",
        "        loss_weights[fp_mask] = self.fp_asymmetry_param\n",
        "        fn_mask = (y_pred <= self.threshold) & (y_true > self.threshold)\n",
        "        loss_weights[fn_mask] = self.fn_penalty_param\n",
        "        asymmetric_loss = torch.mean(base_errors * loss_weights)\n",
        "\n",
        "        binary_labels = (y_true > self.threshold).float()\n",
        "        pred_probs = torch.sigmoid(self.gamma * y_pred)\n",
        "        proxy_clf_loss = torch.mean(self.bce(pred_probs, binary_labels)) * self.proxy_clf_weight\n",
        "\n",
        "        total_loss = base_loss + asymmetric_loss + proxy_clf_loss\n",
        "        return total_loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict\n",
        "\n",
        "class MultiTaskLoss(nn.Module):\n",
        "    \"\"\"组合多个任务的损失函数（y_reg 使用复合损失，其它头保持原口径）\"\"\"\n",
        "    def __init__(self,\n",
        "                 task_names: List[str],\n",
        "                 loss_weights: Dict[str, float],\n",
        "                 mu_yreg: float,\n",
        "                 sig_yreg: float,\n",
        "                 gamma: float = 3.0,      # 代理分类放大系数\n",
        "                 fp_asym: float = 4.0,    # 假阳性惩罚\n",
        "                 fn_pen: float = 2.0,     # 假阴性惩罚\n",
        "                 w_mse: float = 1.0,      # 回归(Huber)权重\n",
        "                 w_proxy: float = 0.5,    # 代理分类(BCE)权重\n",
        "                 w_center: float = 0.01,  # 零均值正则权重\n",
        "                 huber_delta: float = 2.0 # Huber delta，单位=标准化后的单位\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.task_names  = task_names\n",
        "        self.loss_weights= loss_weights\n",
        "\n",
        "        # 其它头的损失（排除 y_reg，y_direction 用 CE）\n",
        "        self.loss_fns = nn.ModuleDict({\n",
        "            'y_rv': nn.MSELoss(),\n",
        "            'y_fillprob': nn.BCEWithLogitsLoss(),\n",
        "            'y_roll': nn.MSELoss(),\n",
        "            'y_vpin': nn.MSELoss(),\n",
        "            'y_direction': nn.CrossEntropyLoss(),\n",
        "            'y_upper_bound': nn.MSELoss(),\n",
        "            'y_lower_bound': nn.MSELoss()\n",
        "        })\n",
        "\n",
        "        # y_reg 复合项参数\n",
        "        self.mu_yreg   = float(mu_yreg)\n",
        "        self.sig_yreg  = float(sig_yreg)\n",
        "        self.gamma     = float(gamma)\n",
        "        self.fp_asym   = float(fp_asym)\n",
        "        self.fn_pen    = float(fn_pen)\n",
        "        self.w_mse     = float(w_mse)\n",
        "        self.w_proxy   = float(w_proxy)\n",
        "        self.w_center  = float(w_center)\n",
        "        self.huber_delta = float(huber_delta)\n",
        "\n",
        "    @staticmethod\n",
        "    def _huber_mean(err: torch.Tensor, delta: float) -> torch.Tensor:\n",
        "        abs_e = err.abs()\n",
        "        quad  = 0.5 * (abs_e ** 2)\n",
        "        lin   = delta * (abs_e - 0.5 * delta)\n",
        "        return torch.where(abs_e <= delta, quad, lin).mean()\n",
        "\n",
        "    def _yreg_loss(self, y_true_raw_bps: torch.Tensor, y_pred_raw_bps: torch.Tensor) -> torch.Tensor:\n",
        "        # 1) 回归项：对 y_reg 做 z-score 再算 Huber（稳梯度）\n",
        "        y_t = (y_true_raw_bps - self.mu_yreg) / self.sig_yreg\n",
        "        y_p = (y_pred_raw_bps - self.mu_yreg) / self.sig_yreg\n",
        "        mse = self._huber_mean(y_p - y_t, self.huber_delta)\n",
        "\n",
        "        # 2) 代理分类项：仍在原始 bps 上（方向分界=0bps）\n",
        "        prob  = torch.sigmoid(self.gamma * y_pred_raw_bps)\n",
        "        label = (y_true_raw_bps > 0).float()\n",
        "        bce   = F.binary_cross_entropy(prob, label)\n",
        "\n",
        "        # 3) 非对称惩罚：原始 bps 上\n",
        "        fp = ((y_pred_raw_bps > 0) & (y_true_raw_bps <= 0)).float()\n",
        "        fn = ((y_pred_raw_bps <= 0) & (y_true_raw_bps > 0)).float()\n",
        "        asym = (fp * self.fp_asym + fn * self.fn_pen).mean()\n",
        "\n",
        "        # 4) 轻微零均值正则：压偏移，不改语义\n",
        "        center_reg = (y_pred_raw_bps.mean())**2\n",
        "\n",
        "        return self.w_mse*mse + self.w_proxy*bce + asym + self.w_center*center_reg\n",
        "\n",
        "    def forward(self, predictions: Dict[str, torch.Tensor], targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        predictions: dict(name -> tensor), 例如:\n",
        "            - y_reg: (B,) 或 (B,1)\n",
        "            - y_fillprob: (B,) logits\n",
        "            - y_direction: (B, C) logits\n",
        "        targets: (B, len(task_names)), 顺序与 task_names 一致\n",
        "        \"\"\"\n",
        "        total_loss = 0.0\n",
        "        for i, task_name in enumerate(self.task_names):\n",
        "            if task_name not in self.loss_weights:\n",
        "                continue\n",
        "\n",
        "            if task_name == 'y_reg':\n",
        "                # 兼容 (B,1) / (B,) 两种形状\n",
        "                y_pred = predictions['y_reg']\n",
        "                if y_pred.ndim > 1 and y_pred.shape[-1] == 1:\n",
        "                    y_pred = y_pred.squeeze(-1)\n",
        "                y_true = targets[:, i]\n",
        "                loss   = self._yreg_loss(y_true, y_pred)\n",
        "\n",
        "            else:\n",
        "                if task_name not in self.loss_fns:\n",
        "                    continue\n",
        "                y_pred = predictions[task_name]\n",
        "                # 标量头兼容 (B,1)\n",
        "                if task_name != 'y_direction' and y_pred.ndim > 1 and y_pred.shape[-1] == 1:\n",
        "                    y_pred = y_pred.squeeze(-1)\n",
        "                y_true = targets[:, i]\n",
        "                if task_name == 'y_direction':\n",
        "                    y_true = y_true.long()\n",
        "                loss_fn = self.loss_fns[task_name]\n",
        "                loss    = loss_fn(y_pred, y_true)\n",
        "\n",
        "            total_loss = total_loss + self.loss_weights[task_name] * loss\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd573b91-1342-46c5-b5b6-4bbedc5e5861",
      "metadata": {
        "id": "fd573b91-1342-46c5-b5b6-4bbedc5e5861"
      },
      "outputs": [],
      "source": [
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 评估指标计算函数 (保持不变) ---\n",
        "def calculate_classification_metrics(y_true, y_pred, threshold=0):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    pred_positive, true_positive = (y_pred > threshold), (y_true > 0)\n",
        "    true_positives = np.sum(pred_positive & true_positive)\n",
        "    false_positives = np.sum(pred_positive & ~true_positive)\n",
        "    false_negatives = np.sum(~pred_positive & true_positive)\n",
        "    precision = true_positives / (true_positives + false_positives + 1e-8)\n",
        "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
        "    return precision, recall\n",
        "\n",
        "def calculate_ic(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
        "    if len(y_true) < 2 or len(y_pred) < 2: return 0.0, 0.0\n",
        "    pearson_ic, _ = pearsonr(y_pred, y_true)\n",
        "    spearman_ic, _ = spearmanr(y_pred, y_true)\n",
        "    return pearson_ic, spearman_ic\n",
        "\n",
        "# --- 多任务训练和评估主流程  ---\n",
        "for symbol_type in SYMBOL_TYPES:\n",
        "    print(f\"\\n{'='*25} 开始训练品种: {symbol_type} {'='*25}\")\n",
        "\n",
        "    # 1. 加载数据\n",
        "    data = load_processed_data(f'{symbol_type}_processed_data_regression.pkl')\n",
        "    if data is None:\n",
        "        print(f\"未找到品种 {symbol_type} 的缓存数据，跳过。\"); continue\n",
        "\n",
        "    selected_indices = data.get('selected_feature_indices')\n",
        "    label_names = data.get('label_names', [])\n",
        "    if not label_names:\n",
        "        print(f\"错误: 未在缓存数据中找到 'label_names'。\"); continue\n",
        "\n",
        "    if selected_indices is None:\n",
        "        print(f\"警告: 未找到为 {symbol_type} 选择的特征索引。将使用所有特征。\")\n",
        "        num_features_total = data['daily_norm_data_dict'][next(iter(data['daily_norm_data_dict']))].shape[1]\n",
        "        selected_indices = list(range(num_features_total))\n",
        "    else:\n",
        "        print(f\"成功加载选择的 {len(selected_indices)} 个特征索引。\")\n",
        "\n",
        "    # 2. 按天划分数据集 (逻辑不变)\n",
        "    all_norm_data_dict = data['daily_norm_data_dict']; all_label_dict = data['daily_label_dict']\n",
        "    valid_dates = sorted(list(set(all_norm_data_dict.keys()) & set(all_label_dict.keys())))\n",
        "\n",
        "    if len(valid_dates) < 5:\n",
        "        print(f\"错误: 品种 {symbol_type} 的有效交易日不足5天({len(valid_dates)}天)，无法按6:2:2划分。\"); continue\n",
        "\n",
        "    num_days = len(valid_dates)\n",
        "    train_end_idx = math.floor(num_days * 0.8); val_end_idx = math.floor(num_days * 0.9)\n",
        "    train_dates, val_dates, test_dates = valid_dates[:train_end_idx], valid_dates[train_end_idx:val_end_idx], valid_dates[val_end_idx:]\n",
        "\n",
        "    if not train_dates or not val_dates or not test_dates:\n",
        "        print(f\"错误: 划分后数据集为空。\"); continue\n",
        "    print(f\"共 {num_days} 个交易日, 划分为: {len(train_dates)} 训练, {len(val_dates)} 验证, {len(test_dates)} 测试。\")\n",
        "\n",
        "    train_norm_dict, train_label_dict = {d: all_norm_data_dict[d] for d in train_dates}, {d: all_label_dict[d] for d in train_dates}\n",
        "    val_norm_dict, val_label_dict = {d: all_norm_data_dict[d] for d in val_dates}, {d: all_label_dict[d] for d in val_dates}\n",
        "    test_norm_dict, test_label_dict = {d: all_norm_data_dict[d] for d in test_dates}, {d: all_label_dict[d] for d in test_dates}\n",
        "\n",
        "    X_train_full, y_train = generate_X_y(train_norm_dict, train_label_dict)\n",
        "    X_val_full, y_val = generate_X_y(val_norm_dict, val_label_dict)\n",
        "    X_test_full, y_test = generate_X_y(test_norm_dict, test_label_dict)\n",
        "\n",
        "\n",
        "    #     # === A) 标签尺度核对（bps） ===\n",
        "    # def _describe_reg(name, arr):\n",
        "    #     arr = np.asarray(arr).ravel()\n",
        "    #     qs = np.percentile(arr, [0,1,5,25,50,75,95,99,100])\n",
        "    #     return f\"{name}: μ={arr.mean():.2f}, σ={arr.std():.2f}, pct[0,1,5,25,50,75,95,99,100]={np.array2string(qs, precision=2)}\"\n",
        "\n",
        "    # def _describe_cls(name, arr):\n",
        "    #     arr = np.asarray(arr).ravel()\n",
        "    #     if name == 'y_fillprob':\n",
        "    #         pos = (arr > 0.5).mean()\n",
        "    #         return f\"{name}: >0.5 占比={pos:.3f}\"\n",
        "    #     elif name == 'y_direction':\n",
        "    #         # 假设 3 类: 0/1/2\n",
        "    #         p = [(arr==k).mean() for k in [0,1,2]]\n",
        "    #         return f\"{name}: 类占比(0/1/2)={[f'{x:.3f}' for x in p]}\"\n",
        "    #     else:\n",
        "    #         return f\"{name}: (未知分类头)\"\n",
        "\n",
        "    # print(\"\\n[标签尺度核对 | 训练 vs 验证]\")\n",
        "    # for name in label_names:\n",
        "    #     j = label_names.index(name)\n",
        "    #     if name in ('y_fillprob','y_direction'):\n",
        "    #         s_tr = _describe_cls(name, y_train[:, j])\n",
        "    #         s_va = _describe_cls(name, y_val[:, j])\n",
        "    #     else:\n",
        "    #         s_tr = _describe_reg(name+\"(train)\", y_train[:, j])\n",
        "    #         s_va = _describe_reg(name+\"(val)\",   y_val[:, j])\n",
        "    #     print(\"  \", s_tr)\n",
        "    #     print(\"  \", s_va)\n",
        "\n",
        "    # if X_train_full is None or X_val_full is None or X_test_full is None:\n",
        "    #     print(\"错误：某个数据集在生成样本后为空，请检查数据。\"); continue\n",
        "    # ===================== 替换开始：打印前25列 + 剔除前20列 =====================\n",
        "    # RAW_N 兜底（如果没在全局定义过，就按20）\n",
        "    try:\n",
        "        RAW_N\n",
        "    except NameError:\n",
        "        RAW_N = 20\n",
        "\n",
        "    # 1) 训练前打印“前25列”的统计与切片（未剔除RAW20之前）\n",
        "    num_features_total = X_train_full.shape[2]\n",
        "    cols_to_show = min(25, num_features_total)\n",
        "    tags = np.array(['RAW' if i < RAW_N else 'DER' for i in range(cols_to_show)])\n",
        "\n",
        "    # 展平样本与时间维度，仅看 channel=0\n",
        "    flat_train = X_train_full[..., 0].reshape(-1, num_features_total)\n",
        "    col_means = flat_train[:, :cols_to_show].mean(axis=0)\n",
        "    col_stds  = flat_train[:, :cols_to_show].std(axis=0)\n",
        "\n",
        "    print(\"\\n[检查] 训练集前 25 列列统计（未剔除 RAW20 之前）:\")\n",
        "    for i in range(cols_to_show):\n",
        "        print(f\"  col {i:3d} [{tags[i]}] mean={col_means[i]:.6g}  std={col_stds[i]:.6g}\")\n",
        "\n",
        "    print(\"\\n[示例] 第一条样本，前 3 个时间步，前 25 列的原始数值切片（channel=0）：\")\n",
        "    print(np.array2string(X_train_full[0, :3, :cols_to_show, 0], precision=6, suppress_small=False))\n",
        "\n",
        "    # 2) 仅使用“非前20列”的特征进行训练\n",
        "    if selected_indices is None:\n",
        "        selected_indices = list(range(num_features_total))\n",
        "    else:\n",
        "        print(f\"\\n原始 selected_indices 数量: {len(selected_indices)}\")\n",
        "\n",
        "    # 过滤掉前20列（RAW）\n",
        "    selected_indices_filtered = sorted(i for i in selected_indices if i >= RAW_N)\n",
        "\n",
        "    # 如果筛完为空（极端情况），回退为“全部衍生列”\n",
        "    if len(selected_indices_filtered) == 0:\n",
        "        selected_indices_filtered = list(range(RAW_N, num_features_total))\n",
        "        print(\"[警告] 选中特征在剔除 RAW20 后为空，改为使用所有衍生列。\")\n",
        "\n",
        "    # 额外安全检查：确认没有RAW列混入\n",
        "    has_raw_leak = any(i < RAW_N for i in selected_indices_filtered)\n",
        "    print(f\"已剔除前 20 列（RAW）。用于训练的特征数: {len(selected_indices_filtered)} / {num_features_total}；含RAW泄漏? {has_raw_leak}\")\n",
        "\n",
        "    if len(selected_indices_filtered) > 0:\n",
        "        print(f\"索引范围: 最小={min(selected_indices_filtered)}, 最大={max(selected_indices_filtered)}\")\n",
        "        print(f\"预览前20个用于训练的衍生列索引: {selected_indices_filtered[:20]}\")\n",
        "\n",
        "    # 3) 应用过滤后的特征索引\n",
        "    X_train = X_train_full[:, :, selected_indices_filtered, :]\n",
        "    X_val   = X_val_full[:,   :, selected_indices_filtered, :]\n",
        "    X_test  = X_test_full[:,  :, selected_indices_filtered, :]\n",
        "\n",
        "    feature_num = X_train.shape[2]\n",
        "    print(f\"\\n{symbol_type} 训练集: X={X_train.shape}, y={y_train.shape}\")\n",
        "    print(f\"{symbol_type} 验证集: X={X_val.shape}, y={y_val.shape}\")\n",
        "    print(f\"{symbol_type} 测试集: X={X_test.shape}, y={y_test.shape}\")\n",
        "    # ===================== 替换结束 =====================\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"使用设备: {device}\")\n",
        "\n",
        "    model_params = {'d_model': D_MODEL, 'nhead': ATTENTION_NUM_HEADS, 'num_encoder_layers': NUM_ENCODER_LAYERS, 'dim_feedforward': DIM_FEEDFORWARD, 'dropout': MODEL_DROPOUT}\n",
        "    model = initiate_transformer_model(feature_num, task_names=label_names, **model_params).to(device)\n",
        "\n",
        "    print(model)\n",
        "    # 4. 准备数据加载器、损失函数和优化器\n",
        "    NUM_WORKERS, PIN_MEMORY = 0, False   # ★ 关闭多进程与 pin_memory，安静稳妥\n",
        "    print(f\"使用DataLoader Workers: {NUM_WORKERS}, Pin Memory: {PIN_MEMORY}\")\n",
        "\n",
        "    train_dataset = RegressionLOBDataset(X_train, y_train)\n",
        "    val_dataset   = RegressionLOBDataset(X_val,   y_val)\n",
        "    test_dataset  = RegressionLOBDataset(X_test,  y_test)\n",
        "\n",
        "    from torch.utils.data import DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    # === 计算 y_reg 在训练集上的 mu/sigma（单位：bps）用于损失内部 z-score ===\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    YREG_IDX = label_names.index('y_reg')\n",
        "\n",
        "    if 'y_train' in globals() and y_train is not None:\n",
        "        yreg_train = y_train[:, YREG_IDX]\n",
        "        if torch.is_tensor(yreg_train):\n",
        "            yreg_train = yreg_train.detach().cpu().numpy()\n",
        "    else:\n",
        "        # 若没有独立的 y_train，就从 train_dataset 取\n",
        "        yreg_train = train_dataset.labels[:, YREG_IDX]\n",
        "        if torch.is_tensor(yreg_train):\n",
        "            yreg_train = yreg_train.detach().cpu().numpy()\n",
        "\n",
        "    MU_YREG  = float(np.mean(yreg_train))\n",
        "    SIG_YREG = float(np.std(yreg_train) + 1e-8)\n",
        "    print(f\"[y_reg|Train] mu={MU_YREG:.6f} bps, sigma={SIG_YREG:.6f} bps\")\n",
        "    # === 辅助回归头：方差感知重加权（最小改动版）===\n",
        "    # 只调整这几个重尾的头；y_vpin 0~1 不动\n",
        "    AUX_HEADS = [h for h in ['y_rv', 'y_roll', 'y_upper_bound', 'y_lower_bound'] if h in label_names]\n",
        "\n",
        "    # 1) 计算训练集尺度\n",
        "    aux_sig = {}\n",
        "    for h in AUX_HEADS:\n",
        "        j = label_names.index(h)\n",
        "        arr = y_train[:, j]\n",
        "        if torch.is_tensor(arr):\n",
        "            arr = arr.detach().cpu().numpy()\n",
        "        sig = float(np.std(arr) + 1e-8)\n",
        "        # 也顺便看一下稳健范围（99.5%分位），用于可选裁剪\n",
        "        lo = float(np.percentile(arr, 0.5))\n",
        "        hi = float(np.percentile(arr, 99.5))\n",
        "        aux_sig[h] = {'sig': sig, 'lo': lo, 'hi': hi}\n",
        "        print(f\"[scale|{h}] σ_train={sig:.2f} bps, clip_range[{lo:.1f},{hi:.1f}]\")\n",
        "\n",
        "    # 2) 基于 y_reg 的 σ 做“方差反比”重加权（并做上/下限夹紧）\n",
        "    LOSS_WEIGHTS_TUNED = dict(LOSS_WEIGHTS)  # 复制一份，不动你的原全局\n",
        "    ref = SIG_YREG  # 以主任务的 σ 作为参考\n",
        "    for h in AUX_HEADS:\n",
        "        sig = aux_sig[h]['sig']\n",
        "        scale = (ref / sig) ** 2              # 方差反比\n",
        "        scale = float(np.clip(scale, 0.02, 1.0))  # 夹紧到 [0.02, 1.0]，防止过小\n",
        "        old_w = LOSS_WEIGHTS_TUNED.get(h, 0.0)\n",
        "        LOSS_WEIGHTS_TUNED[h] = old_w * scale\n",
        "        print(f\"[reweight] {h}: old_w={old_w:.4f} -> new_w={LOSS_WEIGHTS_TUNED[h]:.4f} (scale={scale:.4f})\")\n",
        "\n",
        "    # 3) （可选）对辅助头做“基于训练集分位”的双侧裁剪，抑制极端尾部对 MSE 的支配\n",
        "    ENABLE_WINSOR = True  # 若暂不想改标签，改为 False 即可\n",
        "    if ENABLE_WINSOR and AUX_HEADS:\n",
        "        for h in AUX_HEADS:\n",
        "            j = label_names.index(h)\n",
        "            lo, hi = aux_sig[h]['lo'], aux_sig[h]['hi']\n",
        "            for Y, tag in [(y_train,'train'), (y_val,'val'), (y_test,'test')]:\n",
        "                Y[:, j] = np.clip(Y[:, j], lo, hi)\n",
        "            print(f\"[winsor] {h}: clip to [{lo:.1f},{hi:.1f}] bps on train/val/test (based on train)\")\n",
        "\n",
        "    # === 实例化带 y_reg 复合损失的 MultiTaskLoss（其余头保持原口径） ===\n",
        "    # 需要你已在上文定义新版 MultiTaskLoss(task_names, loss_weights, mu_yreg, sig_yreg, ...)\n",
        "# 让代理BCE的γ随 y_reg 的 σ 自适应，避免过饱和\n",
        "    GAMMA_AUTO = float(np.log(9) / (SIG_YREG + 1e-8))\n",
        "\n",
        "    loss_fn = MultiTaskLoss(\n",
        "        task_names=label_names,\n",
        "        loss_weights=LOSS_WEIGHTS_TUNED,   # ← 用重加权后的权重\n",
        "        mu_yreg=MU_YREG, sig_yreg=SIG_YREG,\n",
        "        gamma=GAMMA_AUTO,                  # ← 自适应 γ\n",
        "        fp_asym=4.0, fn_pen=2.0,\n",
        "        w_mse=1.0, w_proxy=0.5, w_center=0.01,\n",
        "        huber_delta=2.0\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # === 优化器 / 学习率调度 / 早停 ===\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-7)\n",
        "\n",
        "    model_path = f'./{symbol_type}_transformer_model_best.pth'\n",
        "    early_stopping = EarlyStopping(patience=30, verbose=True, path=model_path, mode='min', delta=1e-5)\n",
        "\n",
        "    print(\"组件就绪：DataLoaders、MultiTaskLoss(含y_reg复合项)、AdamW、Cosine LR、EarlyStopping\")\n",
        "\n",
        "\n",
        "    # 5. 训练循环\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_precision': []}\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch}\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(inputs)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds_main, val_labels_main = [], []\n",
        "        main_task_idx = label_names.index('y_reg')\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                predictions = model(inputs)\n",
        "                loss = loss_fn(predictions, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_preds_main.extend(predictions['y_reg'].cpu().numpy())\n",
        "                val_labels_main.extend(labels[:, main_task_idx].cpu().numpy())\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        # 在训练日志中，我们仍然使用阈值0来监控一个基础表现\n",
        "        val_precision, val_recall = calculate_classification_metrics(val_labels_main, val_preds_main, threshold=0)\n",
        "        history['val_precision'].append(val_precision)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch}/{NUM_EPOCHS} - LR: {current_lr:.7f} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f} - Val P@0: {val_precision:.4f} - Val R@0: {val_recall:.4f}\")\n",
        "\n",
        "        early_stopping(val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"训练早停触发。\"); break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*25 + \" 在验证集上寻找最优决策阈值 \" + \"=\"*25)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    val_preds_main, val_labels_main = [], []\n",
        "    main_task_idx = label_names.index('y_reg')\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"在验证集上预测\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            val_preds_main.extend(predictions['y_reg'].cpu().numpy())\n",
        "            val_labels_main.extend(labels[:, main_task_idx].cpu().numpy())\n",
        "\n",
        "    search_quantiles = np.arange(0, 1.0, 0.001)\n",
        "    _pos = np.asarray(val_preds_main, dtype=float)\n",
        "    _pos = _pos[np.isfinite(_pos) & (_pos > 0)]\n",
        "    if _pos.size == 0:\n",
        "        thresholds_to_search = np.array([0.0], dtype=float)\n",
        "    else:\n",
        "        thresholds_to_search = np.unique(np.quantile(_pos, search_quantiles))\n",
        "        thresholds_to_search = thresholds_to_search[thresholds_to_search >= 0]\n",
        "    print(f\"将在 {len(thresholds_to_search)} 个候选阈值上进行搜索 (正域，min={thresholds_to_search.min():.4f} ~ max={thresholds_to_search.max():.4f})\")\n",
        "\n",
        "\n",
        "    results = []\n",
        "    for thresh in tqdm(thresholds_to_search, desc=\"搜索最优阈值\", leave=False):\n",
        "        precision, recall = calculate_classification_metrics(val_labels_main, val_preds_main, threshold=thresh)\n",
        "        num_signals = np.sum(np.array(val_preds_main) > thresh)\n",
        "        if precision > 0 or recall > 0:\n",
        "             results.append((thresh, precision, recall, num_signals))\n",
        "\n",
        "    results_df = pd.DataFrame(results, columns=['Threshold', 'Precision', 'Recall', 'NumSignals'])\n",
        "\n",
        "    TARGET_PRECISION = 0.6\n",
        "    optimal_threshold = 0.0\n",
        "    candidates_df = results_df[results_df['Precision'] >= TARGET_PRECISION].copy()\n",
        "    if not candidates_df.empty:\n",
        "        best_idx = candidates_df['Recall'].idxmax()\n",
        "        best_row = candidates_df.loc[best_idx]\n",
        "        optimal_threshold, best_precision, best_recall = best_row['Threshold'], best_row['Precision'], best_row['Recall']\n",
        "        print(f\"\\n 找到最优阈值: {optimal_threshold:.4f}\")\n",
        "        print(f\"   - 在该阈值下，验证集精确率: {best_precision:.4f}\")\n",
        "        print(f\"   - 在该阈值下，验证集召回率: {best_recall:.4f}\")\n",
        "        print(f\"   - 在该阈值下，产生的信号数量: {int(best_row['NumSignals'])} (占验证集 {best_row['NumSignals']/len(val_labels_main)*100:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"\\n 在验证集上未能找到任何满足 Precision >= {TARGET_PRECISION} 的阈值。\")\n",
        "        if not results_df.empty:\n",
        "            max_p_row = results_df.loc[results_df['Precision'].idxmax()]\n",
        "            optimal_threshold = max_p_row['Threshold']\n",
        "            print(f\"   - 验证集上能达到的最高精确率为: {max_p_row['Precision']:.4f} (在阈值 {optimal_threshold:.4f})\")\n",
        "            print(f\"   - 将使用此最高精确率对应的阈值进行后续测试。\")\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.plot(results_df['Threshold'], results_df['Precision'], 'b-', label='精确率 (Precision)', marker='.', markersize=4)\n",
        "    plt.plot(results_df['Threshold'], results_df['Recall'], 'g-', label='召回率 (Recall)', marker='.', markersize=4)\n",
        "    plt.axhline(y=TARGET_PRECISION, color='r', linestyle='--', label=f'目标精确率 ({TARGET_PRECISION})')\n",
        "    plt.axvline(x=optimal_threshold, color='k', linestyle='--', label=f'最优阈值 ({optimal_threshold:.4f})')\n",
        "    plt.title(f'{symbol_type} - 精确率-召回率 vs. 决策阈值 (验证集)', fontsize=14)\n",
        "    plt.xlabel('决策阈值 (y_pred)', fontsize=12)\n",
        "    plt.ylabel('指标值', fontsize=12)\n",
        "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.6); plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 6. 最终评估\n",
        "    print(\"\\n--- 模型最终评估 ---\")\n",
        "    model.load_state_dict(torch.load(model_path)) # 确保加载的是最佳模型\n",
        "    model.eval()\n",
        "\n",
        "    all_preds_dict, all_labels_list = {name: [] for name in label_names}, []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"在测试集上评估\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            for name in label_names:\n",
        "                all_preds_dict[name].extend(predictions[name].cpu().numpy())\n",
        "            all_labels_list.append(labels.cpu().numpy())\n",
        "\n",
        "    all_labels_np = np.vstack(all_labels_list)\n",
        "\n",
        "    main_task_idx = label_names.index('y_reg')\n",
        "    y_pred_main, y_true_main = np.array(all_preds_dict['y_reg']), all_labels_np[:, main_task_idx]\n",
        "\n",
        "    # --- 回归指标评估  ---\n",
        "    y_pred_main_scaled_back, y_true_main_scaled_back = y_pred_main / 10000.0, y_true_main / 10000.0\n",
        "    mse = mean_squared_error(y_true_main_scaled_back, y_pred_main_scaled_back)\n",
        "    mae = mean_absolute_error(y_true_main_scaled_back, y_pred_main_scaled_back)\n",
        "    r2 = r2_score(y_true_main, y_pred_main)\n",
        "    pearson_ic, spearman_ic = calculate_ic(y_true_main, y_pred_main)\n",
        "\n",
        "    print(\"\\n回归任务评估结果 (主任务 y_reg):\")\n",
        "    print(f\"  均方误差 (MSE): {mse:.8f}\")\n",
        "    print(f\"  平均绝对误差 (MAE): {mae:.8f}  (约 {mae*10000:.2f} bps)\")\n",
        "    print(f\"  R-squared (R2 Score): {r2:.4f}\")\n",
        "    print(f\"  皮尔逊 IC (Pearson IC): {pearson_ic:.4f}\")\n",
        "    print(f\"  斯皮尔曼 IC (Spearman IC): {spearman_ic:.4f}\")\n",
        "\n",
        "    # --- 决策导向评估 (使用优化后的阈值) ---\n",
        "    print(f\"\\n决策导向评估结果 (使用最优阈值={optimal_threshold:.4f}):\")\n",
        "    precision, recall = calculate_classification_metrics(y_true_main, y_pred_main, threshold=optimal_threshold)\n",
        "    print(f\"  Precision: {precision:.4f}  (当模型预测盈利时，{precision*100:.2f}% 的情况是正确的)\")\n",
        "    print(f\"  Recall: {recall:.4f}  (模型能捕捉到所有真实盈利机会中的 {recall*100:.2f}%)\")\n",
        "\n",
        "    # 7. 结果可视化\n",
        "    print(\"\\n--- 结果可视化 ---\")\n",
        "    plot_training_history(history, symbol_type)\n",
        "    visualize_regression_data(y_true_main, y_pred_main, symbol_type)\n",
        "\n",
        "print(\"\\n所有品种训练完成\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== 统一版：仅 Transformer | 阈值搜索 + 三类回测（中文说明） ==========================\n",
        "import os, numpy as np, pandas as pd, torch, matplotlib.pyplot as plt\n",
        "\n",
        "# -------- 基本参数（按需修改） --------\n",
        "RAW_N = 20\n",
        "ASK1_COL, BID1_COL = 0, 10            # RAW: 0=卖一价(Ask1), 10=买一价(Bid1)\n",
        "HOLD_STEPS = K                         # 与训练标签视野一致（依赖你外部定义 K）\n",
        "CAPITAL_PER_TRADE = 100000.0           # 每笔可用资金\n",
        "FEE_BPS = 1                          # 双边合计费率（单位：bp）\n",
        "PRED_IS_BPS = False                    # 若 y_reg 预测本身就是 bps，请设 True\n",
        "TARGET_PRECISION = 0.6                 # 验证集阈值搜索目标 Precision\n",
        "PLOT_EQUITY = True                     # 是否画累计曲线\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------- 品种名（可从外部传入；没有就用默认） --------\n",
        "try:\n",
        "    symbol_type\n",
        "except NameError:\n",
        "    symbol_type = \"IM\"\n",
        "\n",
        "# -------- 推理使用的 86 列（与训练一致：剔除前 20 列 RAW） --------\n",
        "try:\n",
        "    selected_indices_filtered\n",
        "except NameError:\n",
        "    num_features_total = X_test_full.shape[2]\n",
        "    if 'selected_indices' not in globals() or selected_indices is None:\n",
        "        selected_indices = list(range(num_features_total))\n",
        "    selected_indices_filtered = sorted(int(i) for i in selected_indices if int(i) >= RAW_N)\n",
        "inference_indices = selected_indices_filtered\n",
        "feature_num_86 = len(inference_indices)\n",
        "\n",
        "# -------- 兜底：precision/recall 计算（若外部已提供 calculate_classification_metrics 会自动复用） --------\n",
        "if 'calculate_classification_metrics' not in globals():\n",
        "    def calculate_classification_metrics(y_true, y_pred, threshold=0.0):\n",
        "        y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
        "        pred_pos, true_pos = (y_pred > threshold), (y_true > 0)\n",
        "        tp = np.sum(pred_pos & true_pos)\n",
        "        fp = np.sum(pred_pos & ~true_pos)\n",
        "        fn = np.sum(~pred_pos & true_pos)\n",
        "        precision = tp / (tp + fp + 1e-12)\n",
        "        recall    = tp / (tp + fn + 1e-12)\n",
        "        return precision, recall\n",
        "\n",
        "# -------- 加载 Transformer（106→86 自动裁剪输入层） --------\n",
        "def load_transformer_trim_to_86(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"警告：未找到模型权重 '{model_path}'\");\n",
        "        return None\n",
        "\n",
        "    sd = torch.load(model_path, map_location=device)\n",
        "    tmodel = initiate_transformer_model(\n",
        "        feature_num_86,\n",
        "        task_names=label_names,\n",
        "        d_model=D_MODEL,\n",
        "        nhead=ATTENTION_NUM_HEADS,\n",
        "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "        dim_feedforward=DIM_FEEDFORWARD,\n",
        "        dropout=MODEL_DROPOUT\n",
        "    ).to(device)\n",
        "\n",
        "    wkey, bkey = 'input_proj.0.weight', 'input_proj.0.bias'\n",
        "    if (wkey in sd) and (bkey in sd):\n",
        "        in_dim_ckpt = sd[wkey].shape[1]\n",
        "        if in_dim_ckpt == 86:\n",
        "            tmodel.load_state_dict(sd, strict=True)\n",
        "        elif in_dim_ckpt == 106:\n",
        "            W106, b = sd[wkey], sd[bkey]\n",
        "            W86 = W106[:, RAW_N:]  # 丢掉 0..19（RAW）\n",
        "            sd_trim = {k: v for k, v in sd.items() if k not in (wkey, bkey)}\n",
        "            tmodel.load_state_dict(sd_trim, strict=False)\n",
        "            with torch.no_grad():\n",
        "                tmodel.input_proj[0].weight.copy_(W86)\n",
        "                tmodel.input_proj[0].bias.copy_(b)\n",
        "        else:\n",
        "            raise RuntimeError(f\"Transformer权重输入维度={in_dim_ckpt}（仅支持86或106）\")\n",
        "    else:\n",
        "        # 直接加载（比如保存时没含 input_proj 权重名）\n",
        "        tmodel.load_state_dict(sd, strict=True)\n",
        "\n",
        "    tmodel.eval()\n",
        "    return tmodel\n",
        "\n",
        "# -------- 通用推理 --------\n",
        "def infer_on_full(mdl, X_array, indices, batch=2048):\n",
        "    mdl.eval()\n",
        "    outs, N = [], X_array.shape[0]\n",
        "    for s in range(0, N, batch):\n",
        "        batch_np = X_array[s:s+batch, :, indices, :].astype(np.float32, copy=False)\n",
        "        batch_ts = torch.as_tensor(batch_np, dtype=torch.float32, device=device)\n",
        "        with torch.no_grad():\n",
        "            y = mdl(batch_ts)['y_reg'].detach().cpu().numpy().reshape(-1)\n",
        "        outs.append(y)\n",
        "    return np.concatenate(outs, axis=0)\n",
        "# -------- 验证集阈值搜索（precision≥目标 → recall 最大；否则 precision 最大） --------\n",
        "def find_best_threshold_on_val_full(mdl, indices, target_precision=TARGET_PRECISION):\n",
        "    main_idx  = label_names.index('y_reg')\n",
        "\n",
        "    # 1) 预测 & 取主任务真值\n",
        "    val_preds = infer_on_full(mdl, X_val_full, indices)\n",
        "    val_labels = np.asarray(y_val[:, main_idx], dtype=float).reshape(-1)\n",
        "\n",
        "    val_preds = np.asarray(val_preds, dtype=float).reshape(-1)\n",
        "    assert val_preds.shape[0] == val_labels.shape[0], \"[阈值搜索] 预测与标签长度不一致\"\n",
        "\n",
        "    if val_preds.size == 0:\n",
        "        print(\"验证集为空，阈值返回 0\")\n",
        "        return 0.0\n",
        "\n",
        "    # 2) 候选阈值：仅用正域分位，并强制加入 0；去重、排序、去非有限值\n",
        "    pos = val_preds[np.isfinite(val_preds) & (val_preds > 0)]\n",
        "    if pos.size == 0:\n",
        "        ths = np.array([0.0], dtype=float)\n",
        "    else:\n",
        "        # 包含 1.0 端点；去 NaN/Inf；>=0；去重并排序\n",
        "        qs  = np.linspace(0.0, 1.0, 1001, endpoint=True)\n",
        "        ths = np.quantile(pos, qs)\n",
        "        ths = ths[np.isfinite(ths)]\n",
        "        ths = ths[ths >= 0]\n",
        "        ths = np.unique(np.r_[0.0, ths])\n",
        "        ths.sort()\n",
        "\n",
        "    # 3) 逐阈值评估（目标P优先 → R最大；否则P最大）\n",
        "    best_th, best_p, best_r, met_target = 0.0, -1.0, 0.0, False\n",
        "    for th in ths:\n",
        "        p, r = calculate_classification_metrics(val_labels, val_preds, threshold=float(th))\n",
        "        if p >= target_precision:\n",
        "            if not met_target or r > best_r:\n",
        "                best_th, best_p, best_r, met_target = float(th), float(p), float(r), True\n",
        "        elif not met_target and p > best_p:\n",
        "            best_th, best_p, best_r = float(th), float(p), float(r)\n",
        "\n",
        "    num_signals = int(np.sum(val_preds > best_th))\n",
        "    print(f\"[阈值搜索] 目标P={target_precision:.2f} → TH={best_th:.6f} | P={best_p:.4f} | R={best_r:.4f} | 信号数={num_signals} | 达标? {met_target}\")\n",
        "    return best_th\n",
        "\n",
        "\n",
        "# -------- 小工具 --------\n",
        "def _to_bps(preds, already_bps=PRED_IS_BPS):\n",
        "    p = np.asarray(preds).reshape(-1).astype(np.float64)\n",
        "    return p if already_bps else p * 1e4\n",
        "\n",
        "# ====== A1. 收益率口径回测（不计费用）：pred>TH → Ask1 买，持有 HOLD_STEPS → Bid1 卖 ======\n",
        "def backtest_return_only(preds, TH, title_name):\n",
        "    print(f\"\\n【回测A | 收益率口径】说明：当预测值 > 阈值TH={TH:.6f} 时，\"\n",
        "          f\"在该时刻以 卖一价(Ask1) 买入，持有 {HOLD_STEPS} 个步长后以 买一价(Bid1) 卖出；\"\n",
        "          f\"只统计单笔收益率 (exit/entry - 1)，不计手续费与资金约束。\")\n",
        "\n",
        "    N = X_test_full.shape[0]\n",
        "    preds = np.asarray(preds).reshape(-1)\n",
        "    idx_entry = np.nonzero(preds > TH)[0]\n",
        "    idx_exit  = idx_entry + HOLD_STEPS\n",
        "    m = idx_exit < N\n",
        "    idx_entry, idx_exit = idx_entry[m], idx_exit[m]\n",
        "    if idx_entry.size == 0:\n",
        "        print(f\"[{symbol_type} | 回测A] 无交易\")\n",
        "        return None\n",
        "\n",
        "    entry_ask1 = X_test_full[idx_entry, -1, ASK1_COL, 0].astype(np.float64)\n",
        "    exit_bid1  = X_test_full[idx_exit,  -1, BID1_COL,  0].astype(np.float64)\n",
        "    ret = (exit_bid1 / np.clip(entry_ask1, 1e-12, None)) - 1.0\n",
        "\n",
        "    winrate = (ret > 0).mean()\n",
        "    print(f\"[{symbol_type} | 回测A] 笔数={len(ret)} | 胜率={winrate:.2%} | 平均单笔={ret.mean():.6f} | 累计={ret.sum():.6f}\")\n",
        "\n",
        "    if PLOT_EQUITY:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(np.cumsum(ret), marker='.')\n",
        "        plt.title(f'{symbol_type} {title_name} 回测A：累计收益率（TH={TH:.4f}，持有{HOLD_STEPS}步）')\n",
        "        plt.xlabel('Trade #'); plt.ylabel('Cumulative Return'); plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "    return pd.DataFrame({'sample_idx': idx_entry, 'ret': ret})\n",
        "\n",
        "# ====== A2. 现金口径回测（取整持仓+手续费）：pred>TH → Ask1 买，持有 → Bid1 卖 ======\n",
        "def _compute_trades_from_entries(entry_idx, capital=CAPITAL_PER_TRADE, fee_bps=FEE_BPS):\n",
        "    N = X_test_full.shape[0]\n",
        "    entry_idx = np.asarray(entry_idx, dtype=np.int64)\n",
        "    if entry_idx.size == 0:\n",
        "        return None\n",
        "\n",
        "    idx_exit = entry_idx + HOLD_STEPS\n",
        "    m = idx_exit < N\n",
        "    entry_idx, idx_exit = entry_idx[m], idx_exit[m]\n",
        "    if entry_idx.size == 0:\n",
        "        return None\n",
        "\n",
        "    entry = X_test_full[entry_idx, -1, ASK1_COL, 0].astype(np.float64)\n",
        "    exit_ = X_test_full[idx_exit,  -1, BID1_COL,  0].astype(np.float64)\n",
        "\n",
        "    qty = np.floor(capital / np.clip(entry, 1e-12, None)).astype(np.int64)\n",
        "    tradable = qty > 0\n",
        "    if not np.any(tradable):\n",
        "        return None\n",
        "\n",
        "    entry_idx, entry, exit_, qty = entry_idx[tradable], entry[tradable], exit_[tradable], qty[tradable]\n",
        "    notional_in  = entry * qty\n",
        "    notional_out = exit_ * qty\n",
        "    pnl_gross    = notional_out - notional_in\n",
        "    fee_rate     = fee_bps / 10000.0\n",
        "    fees         = (notional_in + notional_out) * fee_rate\n",
        "    pnl_net      = pnl_gross - fees\n",
        "\n",
        "    ret_gross = (exit_ / entry) - 1.0\n",
        "    ret_net   = pnl_net / np.clip(notional_in, 1e-12, None)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'sample_idx':   entry_idx,\n",
        "        'entry_ask1':   entry,\n",
        "        'exit_bid1':    exit_,\n",
        "        'qty':          qty,\n",
        "        'notional_in':  notional_in,\n",
        "        'notional_out': notional_out,\n",
        "        'fee':          fees,\n",
        "        'pnl_gross':    pnl_gross,\n",
        "        'pnl_net':      pnl_net,\n",
        "        'ret_gross':    ret_gross,\n",
        "        'ret_net':      ret_net,\n",
        "    })\n",
        "\n",
        "def _summarize_trades(trades_df, tag):\n",
        "    if trades_df is None or len(trades_df) == 0:\n",
        "        print(f\"[{symbol_type} | {tag}] 无交易统计。\")\n",
        "        return None\n",
        "\n",
        "    wins = trades_df['pnl_net'] > 0\n",
        "    losses = trades_df['pnl_net'] < 0\n",
        "    avg_win      = float(trades_df.loc[wins,   'pnl_net'].mean()) if wins.any()   else 0.0\n",
        "    avg_loss     = float(-trades_df.loc[losses,'pnl_net'].mean()) if losses.any() else 0.0\n",
        "    avg_win_bps  = float((trades_df.loc[wins,   'ret_net'].mean() * 1e4)) if wins.any()   else 0.0\n",
        "    avg_loss_bps = float((-trades_df.loc[losses,'ret_net'].mean() * 1e4)) if losses.any() else 0.0\n",
        "    cover_ratio  = (avg_loss / (avg_win + 1e-12)) if avg_win > 0 else np.inf\n",
        "\n",
        "    summary = {\n",
        "        'Trades':   int(len(trades_df)),\n",
        "        'WinRate':  float((trades_df['pnl_net'] > 0).mean()),\n",
        "        'AvgQty':   float(trades_df['qty'].mean()),\n",
        "        'GrossPnL': float(trades_df['pnl_gross'].sum()),\n",
        "        'Fees':     float(trades_df['fee'].sum()),\n",
        "        'NetPnL':   float(trades_df['pnl_net'].sum()),\n",
        "        'AvgWin':   avg_win,\n",
        "        'AvgLoss':  avg_loss,\n",
        "        'AvgWin_bps':  avg_win_bps,\n",
        "        'AvgLoss_bps': avg_loss_bps,\n",
        "        'WinsToCoverOneLoss': float(cover_ratio)\n",
        "    }\n",
        "\n",
        "    print(f\"[{symbol_type} | {tag}] 笔数={summary['Trades']} | 胜率={summary['WinRate']:.2%} | \"\n",
        "          f\"净PNL={summary['NetPnL']:.2f} | 平均赢={avg_win:.2f}({avg_win_bps:.2f}bp) | \"\n",
        "          f\"平均输={avg_loss:.2f}({avg_loss_bps:.2f}bp) | 覆盖一笔平均亏损需 {cover_ratio:.2f} 笔平均盈利\")\n",
        "    return summary\n",
        "\n",
        "def backtest_cash_by_TH(preds, TH, title_name, capital=CAPITAL_PER_TRADE, fee_bps=FEE_BPS, plot=True):\n",
        "    print(f\"\\n【回测B | 现金口径（>TH）】说明：当预测值 > 阈值TH={TH:.6f} 时，\"\n",
        "          f\"以 卖一价(Ask1) 用资金 {capital:.0f} 元 取整买入数量，持有 {HOLD_STEPS} 步后以 买一价(Bid1) 卖出；\"\n",
        "          f\"考虑双边费率 {fee_bps} bp。统计净PNL、胜率、平均输/赢（含 bps）。\")\n",
        "\n",
        "    idx_entry = np.nonzero(np.asarray(preds).reshape(-1) > float(TH))[0]\n",
        "    trades_df = _compute_trades_from_entries(idx_entry, capital=capital, fee_bps=fee_bps)\n",
        "    summary = _summarize_trades(trades_df, f\"回测B | {title_name}\")\n",
        "\n",
        "    if plot and (trades_df is not None) and (len(trades_df)>0) and PLOT_EQUITY:\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(np.cumsum(trades_df['pnl_net'].values), marker='.')\n",
        "        plt.title(f'{symbol_type} {title_name} 回测B：累计净PNL（TH={TH:.4f}，持有{HOLD_STEPS}步，费率{fee_bps}bp）')\n",
        "        plt.xlabel('Trade #'); plt.ylabel('Cumulative Net PnL'); plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "    return trades_df, summary\n",
        "\n",
        "def backtest_cash_by_min_pred_bps(preds, min_pred_bps, title_name, capital=CAPITAL_PER_TRADE, fee_bps=FEE_BPS):\n",
        "    print(f\"\\n【回测C | 现金口径（预测>{min_pred_bps}bp）】说明：当预测的价差 > {min_pred_bps} bp 时入场，\"\n",
        "          f\"以 卖一价(Ask1) 用资金 {capital:.0f} 元 取整买入，持有 {HOLD_STEPS} 步后以 买一价(Bid1) 卖出；\"\n",
        "          f\"考虑双边费率 {fee_bps} bp。统计净PNL、胜率、平均输/赢（含 bps）。\")\n",
        "    pred_bps = _to_bps(preds, already_bps=PRED_IS_BPS)\n",
        "    idx_entry = np.nonzero(pred_bps > float(min_pred_bps))[0]\n",
        "    trades_df = _compute_trades_from_entries(idx_entry, capital=capital, fee_bps=fee_bps)\n",
        "    summary = _summarize_trades(trades_df, f\"回测C({min_pred_bps}bp) | {title_name}\")\n",
        "    return trades_df, summary\n",
        "\n",
        "# ========================== 主流程：只用 Transformer ==========================\n",
        "def run_all_backtests_transformer():\n",
        "    model_path = f'./{symbol_type}_transformer_model_best.pth'\n",
        "    model_transformer = load_transformer_trim_to_86(model_path)\n",
        "    if model_transformer is None:\n",
        "        print(\"未找到可用 Transformer 权重，流程结束。\");\n",
        "        return\n",
        "\n",
        "    # 1) 验证集搜索阈值\n",
        "    TH = find_best_threshold_on_val_full(model_transformer, inference_indices, target_precision=TARGET_PRECISION)\n",
        "\n",
        "    # 2) 测试集预测\n",
        "    preds_test = infer_on_full(model_transformer, X_test_full, inference_indices)\n",
        "\n",
        "    # 3) 回测A：收益率口径（不计费）\n",
        "    _ = backtest_return_only(preds_test, TH, title_name='Transformer')\n",
        "\n",
        "    # 4) 回测B：现金口径（>TH，含费）\n",
        "    df_B, sum_B = backtest_cash_by_TH(preds_test, TH, title_name='Transformer',\n",
        "                                      capital=CAPITAL_PER_TRADE, fee_bps=FEE_BPS)\n",
        "\n",
        "    # # 5) 回测C：预测>1/2/3bp（现金口径，含费）\n",
        "    # summary_rows = []\n",
        "    # for mbps in (1.0, 2.0, 3.0):\n",
        "    #     _, s = backtest_cash_by_min_pred_bps(preds_test, mbps, title_name='Transformer',\n",
        "    #                                          capital=CAPITAL_PER_TRADE, fee_bps=FEE_BPS)\n",
        "    #     if s is not None:\n",
        "    #         s_row = {'Strategy':'Transformer', 'PredMinBps': mbps, **s}\n",
        "    #         summary_rows.append(s_row)\n",
        "\n",
        "    # 6) 汇总打印（B 与 C）\n",
        "    print(\"\\n=== 策略汇总：回测B（>TH，现金口径） ===\")\n",
        "    if sum_B is not None:\n",
        "        df_th = pd.DataFrame([{**{'Strategy':'Transformer','PredMinBps':'>TH'}, **sum_B}])\n",
        "        print(df_th[['Strategy','PredMinBps','Trades','WinRate','AvgQty','GrossPnL','Fees','NetPnL',\n",
        "                     'AvgWin','AvgLoss','AvgWin_bps','AvgLoss_bps','WinsToCoverOneLoss']])\n",
        "    else:\n",
        "        print(\"无结果。\")\n",
        "\n",
        "    # print(\"\\n=== 策略汇总：回测C（预测>1/2/3bp，现金口径） ===\")\n",
        "    # if summary_rows:\n",
        "    #     df_bps = pd.DataFrame(summary_rows)\n",
        "    #     df_bps = df_bps[['Strategy','PredMinBps','Trades','WinRate','AvgQty','GrossPnL','Fees','NetPnL',\n",
        "    #                      'AvgWin','AvgLoss','AvgWin_bps','AvgLoss_bps','WinsToCoverOneLoss']].sort_values(by=['Strategy','PredMinBps'])\n",
        "    #     print(df_bps)\n",
        "    # else:\n",
        "    #     print(\"无结果。\")\n",
        "\n",
        "# === 执行 ===\n",
        "run_all_backtests_transformer()\n"
      ],
      "metadata": {
        "id": "XNWUZvBhqKWy"
      },
      "id": "XNWUZvBhqKWy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}